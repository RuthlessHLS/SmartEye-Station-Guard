# æ–‡ä»¶: ai_service/app.py

import os
import base64
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager
from typing import Dict, List, Optional

import cv2
import numpy as np
import requests
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

# å¯¼å…¥æˆ‘ä»¬è‡ªå®šä¹‰çš„æ‰€æœ‰æ ¸å¿ƒAIæ¨¡å—
from core.video_stream import VideoStream
from core.object_detection import GenericPredictor
from core.behavior_detection import BehaviorDetector
from core.face_recognition import FaceRecognizer
from core.acoustic_detection import AcousticDetector
from models.alert_models import AIAnalysisResult  # ç¡®ä¿è¿™ä¸ªæ–‡ä»¶å­˜åœ¨

# åœ¨åº”ç”¨å¯åŠ¨æ—¶ï¼Œä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# --- å…¨å±€å˜é‡ ---
video_streams: Dict[str, VideoStream] = {}
detectors: Dict[str, object] = {}
thread_pool = ThreadPoolExecutor(max_workers=os.cpu_count() or 4)


# --- FastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† ---

def init_detectors():
    """åˆå§‹åŒ–æ‰€æœ‰AIæ£€æµ‹å™¨æ¨¡å‹ã€‚è¿™ä¸ªå‡½æ•°åœ¨æœåŠ¡å¯åŠ¨æ—¶åªæ‰§è¡Œä¸€æ¬¡ã€‚"""
    try:
        print("--- æ­£åœ¨åˆå§‹åŒ–æ‰€æœ‰æ£€æµ‹å™¨ ---")

        # 1. åˆå§‹åŒ–é€šç”¨ç›®æ ‡æ£€æµ‹å™¨
        class_names_path = os.getenv("CLASS_NAMES_PATH", "models/coco.names")
        class_names = []
        try:
            with open(class_names_path, 'r') as f:
                class_names = [line.strip() for line in f.readlines()]
            print(f"æˆåŠŸåŠ è½½ {len(class_names)} ä¸ªç±»åˆ«åç§°ã€‚")
        except FileNotFoundError:
            print(f"è­¦å‘Š: æ‰¾ä¸åˆ°ç±»åˆ«åç§°æ–‡ä»¶ at '{class_names_path}'ã€‚")
            class_names = ["background", "person"]

        detectors["object"] = GenericPredictor(
            model_weights_path=os.getenv("OBJECT_DETECTION_MODEL_PATH", "models/fasterrcnn_resnet50_fpn.pt"),
            num_classes=len(class_names),
            class_names=class_names
        )

        # 2. åˆå§‹åŒ–è¡Œä¸ºæ£€æµ‹å™¨
        detectors["behavior"] = BehaviorDetector()

        # 3. åˆå§‹åŒ–äººè„¸è¯†åˆ«å™¨
        detectors["face"] = FaceRecognizer(
            known_faces_dir=os.getenv("KNOWN_FACES_DIR", "known_faces")
        )

        # 4. åˆå§‹åŒ–å£°å­¦äº‹ä»¶æ£€æµ‹å™¨ (å¦‚æœå¯ç”¨äº†)
        if os.getenv("ENABLE_SOUND_DETECTION", "false").lower() == "true":
            try:
                detectors["acoustic"] = AcousticDetector()
            except Exception as e:
                print(f"è­¦å‘Š: å£°å­¦æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨æ­¤åŠŸèƒ½ã€‚é”™è¯¯: {e}")

        print("--- æ‰€æœ‰æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ ---")

    except Exception as e:
        print(f"è‡´å‘½é”™è¯¯: æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        raise


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨ã€‚"""
    # å¯åŠ¨ä»»åŠ¡
    init_detectors()
    if "acoustic" in detectors:
        detectors["acoustic"].start_listening()
        asyncio.create_task(run_acoustic_analysis())

    yield  # æœåŠ¡åœ¨æ­¤è¿è¡Œæ—¶ï¼Œå¤„ç†APIè¯·æ±‚

    # å…³é—­ä»»åŠ¡
    print("æœåŠ¡æ­£åœ¨å…³é—­ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
    for stream in video_streams.values():
        stream.stop()
    if "acoustic" in detectors and detectors["acoustic"].is_running:
        detectors["acoustic"].stop_listening()
    thread_pool.shutdown(wait=True)
    print("èµ„æºæ¸…ç†å®Œæ¯•ã€‚")


# åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹
app = FastAPI(
    title="AI æ™ºèƒ½åˆ†ææœåŠ¡ (æœ€ç»ˆç‰ˆ)",
    description="æä¾›è§†é¢‘æµå¤„ç†ã€ç›®æ ‡æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«ã€äººè„¸è¯†åˆ«å’Œå£°å­¦äº‹ä»¶æ£€æµ‹èƒ½åŠ›",
    version="2.0.0",
    lifespan=lifespan
)


# --- æ•°æ®æ¨¡å‹ ---

class StreamConfig(BaseModel):
    camera_id: str
    stream_url: str
    enable_face_recognition: bool = True  # é»˜è®¤å¯ç”¨
    enable_behavior_detection: bool = True  # é»˜è®¤å¯ç”¨


class FaceData(BaseModel):
    person_name: str
    image_data: str  # Base64ç¼–ç çš„å›¾åƒæ•°æ®


# --- æ ¸å¿ƒå‡½æ•° ---

def send_result_to_backend(result: AIAnalysisResult):
    """å°†åˆ†æç»“æœå¼‚æ­¥å‘é€åˆ°åç«¯DjangoæœåŠ¡ã€‚"""

    def task():
        backend_url = os.getenv("DJANGO_BACKEND_URL", "http://127.0.0.1:8000/api/alerts/ai-results/")
        try:
            response = requests.post(backend_url, json=result.dict(), timeout=10)
            if 200 <= response.status_code < 300:
                print(f"âœ… [ç»“æœä¸ŠæŠ¥] æˆåŠŸå‘é€äº‹ä»¶ '{result.event_type}' åˆ°åç«¯ã€‚")
            else:
                # æ‰“å°åç«¯è¿”å›çš„æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                print(f"âŒ [ç»“æœä¸ŠæŠ¥] å‘é€å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, åç«¯å“åº”: {response.text}")
        except requests.exceptions.RequestException as e:
            print(f"âŒ [è¯·æ±‚å¼‚å¸¸] æ— æ³•è¿æ¥åˆ°åç«¯æœåŠ¡: {e}")

    thread_pool.submit(task)


async def run_acoustic_analysis():
    """åœ¨åå°æŒç»­è¿è¡Œçš„åç¨‹ï¼Œç”¨äºåˆ†æéŸ³é¢‘æ•°æ®ã€‚"""
    print("å£°å­¦åˆ†æåå°ä»»åŠ¡å·²å¯åŠ¨ã€‚")
    acoustic_detector = detectors.get("acoustic")
    if not acoustic_detector: return

    while acoustic_detector.is_running:
        result = acoustic_detector.analyze_audio_chunk(volume_threshold=0.1)
        if result and result["need_alert"]:
            print(f"ğŸš¨ [éŸ³é¢‘] æ£€æµ‹åˆ°å¼‚å¸¸å£°éŸ³! éŸ³é‡: {result['details']['volume']}")
            alert = AIAnalysisResult(
                camera_id="audio_sensor_01",
                event_type=result["event_type"],
                location={"source": "microphone", "details": result['details']},
                confidence=result["confidence"],
                timestamp=time.time(),
            )
            send_result_to_backend(alert)
        await asyncio.sleep(0.1)


def create_master_processor(camera_id: str, config: StreamConfig):
    """
    åˆ›å»ºä¸€ä¸ªä¸»AIå¤„ç†å™¨ï¼Œå®ƒè´Ÿè´£åè°ƒè°ƒç”¨æ‰€æœ‰è§†è§‰ç›¸å…³çš„AIæ£€æµ‹å™¨ã€‚
    """

    def master_processor(frame: np.ndarray):
        try:
            # 1. ç›®æ ‡æ£€æµ‹ (æ€»æ˜¯æ‰§è¡Œï¼Œå› ä¸ºæ˜¯å…¶ä»–åˆ†æçš„åŸºç¡€)
            detected_objects = detectors["object"].predict(frame, confidence_threshold=0.6)

            # 2. è¡Œä¸ºæ£€æµ‹
            if config.enable_behavior_detection:
                person_boxes = [obj["coordinates"] for obj in detected_objects if obj["class_name"] == "person"]
                if person_boxes:
                    behaviors = detectors["behavior"].detect_behavior(frame, person_boxes, time.time())
                    for behavior in behaviors:
                        if behavior["is_abnormal"] and behavior["need_alert"]:
                            print(f"ğŸš¨ [{camera_id}] æ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸º: {behavior['behavior']}!")
                            alert = AIAnalysisResult(
                                camera_id=camera_id,
                                event_type=f"abnormal_behavior_{behavior['behavior']}",
                                location={"box": behavior["box"]},
                                confidence=behavior["confidence"],
                                timestamp=time.time(),
                            )
                            send_result_to_backend(alert)

            # 3. äººè„¸è¯†åˆ«
            if config.enable_face_recognition:
                recognized_faces = detectors["face"].detect_and_recognize(frame)
                for face in recognized_faces:
                    if face["identity"] == "Unknown":
                        print(f"ğŸš¨ [{camera_id}] æ£€æµ‹åˆ°æœªçŸ¥äººå‘˜!")
                        alert = AIAnalysisResult(
                            camera_id=camera_id,
                            event_type="unknown_face_detected",
                            location={"box": face["box"]},
                            confidence=face.get("confidence", 0.9),  # ä½¿ç”¨è¯†åˆ«å‡ºçš„ç½®ä¿¡åº¦ï¼Œè‹¥æ— åˆ™é»˜è®¤
                            timestamp=time.time(),
                        )
                        send_result_to_backend(alert)

        except Exception as e:
            print(f"å¤„ç†å¸§æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯ [{camera_id}]: {e}")

    return master_processor


# --- API ç«¯ç‚¹ (Endpoints) ---

@app.post("/stream/start/", status_code=202)
async def start_stream(config: StreamConfig):
    """å¯åŠ¨ä¸€ä¸ªæ–°çš„è§†é¢‘æµå¤„ç†ä»»åŠ¡ã€‚"""
    if config.camera_id in video_streams:
        raise HTTPException(status_code=400, detail=f"æ‘„åƒå¤´ {config.camera_id} å·²åœ¨å¤„ç†ä¸­ã€‚")
    try:
        stream = VideoStream(config.stream_url)
        stream.add_processor(create_master_processor(config.camera_id, config))
        if not stream.start():
            raise HTTPException(status_code=500, detail="æ— æ³•å¯åŠ¨è§†é¢‘æµå¤„ç†çº¿ç¨‹ã€‚")
        video_streams[config.camera_id] = stream
        return {"status": "accepted", "message": f"å·²å¯åŠ¨æ‘„åƒå¤´ {config.camera_id} çš„AIåˆ†æä»»åŠ¡ã€‚"}
    except Exception as e:
        if config.camera_id in video_streams:
            video_streams[config.camera_id].stop()
            del video_streams[config.camera_id]
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨å¤„ç†å¤±è´¥: {str(e)}")


@app.post("/stream/stop/{camera_id}")
async def stop_stream(camera_id: str):
    """åœæ­¢æŒ‡å®šæ‘„åƒå¤´çš„è§†é¢‘æµå¤„ç†ã€‚"""
    if camera_id in video_streams:
        video_streams[camera_id].stop()
        del video_streams[camera_id]
        return {"status": "success", "message": f"å·²åœæ­¢æ‘„åƒå¤´ {camera_id} çš„å¤„ç†ã€‚"}
    raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°æ­£åœ¨å¤„ç†çš„æ‘„åƒå¤´ {camera_id}ã€‚")


@app.post("/face/register/")
async def register_face(face_data: FaceData):
    """æ³¨å†Œæ–°çš„äººè„¸åˆ°äººè„¸è¯†åˆ«å™¨ä¸­ã€‚"""
    try:
        image_bytes = base64.b64decode(face_data.image_data)
        image_array = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
        if image is None:
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„Base64å›¾åƒæ•°æ®ã€‚")

        # è°ƒç”¨äººè„¸è¯†åˆ«å™¨çš„æ·»åŠ æ–¹æ³• (éœ€åœ¨FaceRecognizerç±»ä¸­å®ç°æ­¤æ–¹æ³•)
        # if detectors["face"].add_face(image, face_data.person_name):
        #     return {"status": "success", "message": f"äººè„¸ '{face_data.person_name}' æ³¨å†ŒæˆåŠŸã€‚"}
        # else:
        #     raise HTTPException(status_code=400, detail="æ³¨å†Œå¤±è´¥ï¼Œå¯èƒ½æœªåœ¨å›¾åƒä¸­æ£€æµ‹åˆ°äººè„¸ã€‚")

        # æš‚æ—¶è¿”å›æˆåŠŸï¼Œå¾…FaceRecognizerå®ç°add_face
        print(f"æ”¶åˆ°äººè„¸æ³¨å†Œè¯·æ±‚: {face_data.person_name}")
        return {"status": "success", "message": "äººè„¸æ³¨å†Œè¯·æ±‚å·²æ”¶åˆ° (åŠŸèƒ½å¾…å®ç°)ã€‚"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/system/status/")
async def get_system_status():
    """è·å–æ•´ä¸ªAIæœåŠ¡çš„å½“å‰çŠ¶æ€ã€‚"""
    return {
        "active_streams_count": len(video_streams),
        "detectors_initialized": {name: det is not None for name, det in detectors.items()},
        "active_streams_details": {
            cam_id: stream.get_stream_info() for cam_id, stream in video_streams.items()
        }
    }


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)