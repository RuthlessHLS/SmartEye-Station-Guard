# æ–‡ä»¶: ai_service/app.py
# æè¿°: æ™ºèƒ½è§†é¢‘åˆ†ææœåŠ¡çš„ä¸»å…¥å£ï¼Œè´Ÿè´£APIè·¯ç”±ã€æœåŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€AIåŠŸèƒ½åè°ƒã€‚

import os
import asyncio
import time
import base64
import traceback
import json
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager
from typing import Dict, List, Optional, Any, Union, Tuple

import cv2
import numpy as np
import uvicorn
import requests
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Body, File, UploadFile, Form, Query, Request, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, ConfigDict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# å¯¼å…¥æˆ‘ä»¬è‡ªå®šä¹‰çš„æ‰€æœ‰æ ¸å¿ƒAIæ¨¡å—å’Œæ¨¡å‹
import sys
import logging

logger = logging.getLogger(__name__)
# å°†å½“å‰ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥æ ¸å¿ƒæ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.video_stream import VideoStream
from core.object_detection import ObjectDetector
from core.behavior_detection import BehaviorDetector
from core.face_recognition import FaceRecognizer
from core.fire_smoke_detection import FlameSmokeDetector
from core.multi_object_tracker import DeepSORTTracker
from core.danger_zone_detection import DangerZoneDetector
from models.alert_models import AIAnalysisResult, CameraConfig, SystemStatus  # ç¡®ä¿è¿™äº›æ–‡ä»¶å­˜åœ¨ä¸”ç»“æ„æ­£ç¡®
from core.acoustic_detection import AcousticEventDetector
from core.fire_smoke_detection import FlameSmokeDetector
from core.multi_object_tracker import DeepSORTTracker
from core.danger_zone_detection import danger_zone_detector
from core.object_detection import ObjectDetector
from core.danger_zone_detection import DangerZoneDetector


# --- é…ç½®ç®¡ç† ---
class AppConfig:
    """åº”ç”¨ç¨‹åºçš„å…¨å±€é…ç½®ç±»"""

    def __init__(self):
        # ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡
        current_dir = os.path.dirname(os.path.abspath(__file__))
        dotenv_path = os.path.join(current_dir, '.env')
        if os.path.exists(dotenv_path):
            print(f"--- æ­£åœ¨ä» '{dotenv_path}' åŠ è½½ç¯å¢ƒå˜é‡ ---")
            load_dotenv(dotenv_path=dotenv_path)
        else:
            print(f"--- è­¦å‘Š: æœªæ‰¾åˆ° .env æ–‡ä»¶ at '{dotenv_path}'ï¼Œå°†ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡ ---")
            load_dotenv()  # å°è¯•ä»ç³»ç»Ÿç¯å¢ƒå˜é‡åŠ è½½

        self.ASSET_BASE_PATH = os.getenv("G_DRIVE_ASSET_PATH", "/app/assets")  # é»˜è®¤å€¼ï¼Œå¦‚æœæœªé…ç½®
        self.AI_SERVICE_API_KEY = os.getenv('AI_SERVICE_API_KEY', 'smarteye-ai-service-key-2024')
        self.BACKEND_ALERT_URL = os.getenv('BACKEND_ALERT_URL', 'http://localhost:8000/api/alerts/ai-results/')
        self.BACKEND_WEBSOCKET_BROADCAST_URL = os.getenv('BACKEND_WEBSOCKET_BROADCAST_URL',
                                                         'http://localhost:8000/api/alerts/websocket/broadcast/')
        self.ENABLE_SOUND_DETECTION = os.getenv("ENABLE_SOUND_DETECTION", "false").lower() == "true"
        self.FASTAPI_TIMEOUT_SECONDS = float(os.getenv("FASTAPI_TIMEOUT_SECONDS", "120.0"))

        print(f"--- ä½¿ç”¨èµ„æºæ ¹ç›®å½•: {self.ASSET_BASE_PATH} ---")


# åˆå§‹åŒ–é…ç½®
app_config = AppConfig()


# --- AIServiceManager ç±»ï¼šé›†ä¸­ç®¡ç†AIæœåŠ¡é€»è¾‘å’Œæ•°æ® ---
class AIServiceManager:
    def __init__(self, config: AppConfig):
        self.config = config
        self._detectors: Dict[str, Any] = {}  # å­˜å‚¨æ‰€æœ‰AIæ£€æµ‹å™¨å®ä¾‹
        self._video_streams: Dict[str, VideoStream] = {}  # å­˜å‚¨æ´»è·ƒçš„VideoStreamå®ä¾‹
        self._object_trackers: Dict[str, DeepSORTTracker] = {}  # å­˜å‚¨æ¯ä¸ªæ‘„åƒå¤´çš„DeepSORTè¿½è¸ªå™¨
        self._detection_cache: Dict[str, Dict] = {}  # å­˜å‚¨æ¯ä¸ªæ‘„åƒå¤´çš„æ£€æµ‹ç¨³å®šåŒ–ç¼“å­˜
        self._thread_pool = ThreadPoolExecutor(max_workers=os.cpu_count() or 4)
        self._ai_settings: Dict[str, Dict] = {}  # å­˜å‚¨æ¯ä¸ªæ‘„åƒå¤´çš„AIåˆ†æè®¾ç½®
        self._face_recognition_config: Dict[str, Any] = {
            "tolerance": 0.65, "detection_model": "auto", "enable_multi_scale": True,
            "min_face_size": 40, "max_upsample": 4, "use_cnn_fallback": True
        }
        self._object_detection_config: Dict[str, Any] = {
            "confidence_threshold": 0.35, "area_ratio_threshold": 0.9,
            "special_class_threshold": 0.6, "enable_size_filter": True,
            "enable_aspect_ratio_filter": True
        }
        # æ£€æµ‹ç»“æœç¨³å®šåŒ–å‚æ•°å­˜å‚¨ (per camera_id)
        self._stabilization_config: Dict[str, Dict] = {}

    async def initialize_detectors(self):
        """åˆå§‹åŒ–æ‰€æœ‰AIæ£€æµ‹å™¨æ¨¡å‹ã€‚"""
        print("--- æ­£åœ¨åˆå§‹åŒ–æ‰€æœ‰æ£€æµ‹å™¨ ---")
        try:
            model_weights_path = os.path.join(self.config.ASSET_BASE_PATH, "models", "torch", "yolov8n.pt")
            class_names_path = os.path.join(self.config.ASSET_BASE_PATH, "models", "coco.names")
            known_faces_dir = os.path.join(self.config.ASSET_BASE_PATH, "known_faces")

            # 1. åˆå§‹åŒ–é€šç”¨ç›®æ ‡æ£€æµ‹å™¨
            class_names = []
            try:
                with open(class_names_path, 'r', encoding='utf-8') as f:
                    # ã€ä¿®å¤ 1.1ã€‘ ä¿®æ­£åˆ—è¡¨æ¨å¯¼å¼ä¸­çš„å˜é‡å
                    class_names = [line.strip() for line in f.readlines()]
                print(f"æˆåŠŸä» '{class_names_path}' åŠ è½½ {len(class_names)} ä¸ªç±»åˆ«åç§°ã€‚")
            except FileNotFoundError:
                print(f"è­¦å‘Š: æ‰¾ä¸åˆ°ç±»åˆ«åç§°æ–‡ä»¶ at '{class_names_path}'ã€‚")
                class_names = ["background", "person"]

            self._detectors["object"] = ObjectDetector(
                model_weights_path=model_weights_path,
                num_classes=len(class_names),
                class_names=class_names
            )

            # 2. åˆå§‹åŒ–è¡Œä¸ºæ£€æµ‹å™¨ (ç®€åŒ–ç‰ˆ)
            self._detectors["behavior"] = BehaviorDetector()

            # 3. åˆå§‹åŒ–äººè„¸è¯†åˆ«å™¨
            print(f"æ­£åœ¨ä»ç›®å½• '{known_faces_dir}' åŠ è½½å·²çŸ¥äººè„¸ã€‚")
            self._detectors["face"] = FaceRecognizer(known_faces_dir=known_faces_dir)

            # 4. åˆå§‹åŒ–å£°å­¦äº‹ä»¶æ£€æµ‹å™¨
            if self.config.ENABLE_SOUND_DETECTION:
                try:
                    self._detectors["acoustic"] = AcousticEventDetector()
                    # å£°å­¦åˆ†æåå°ä»»åŠ¡åœ¨start_streamæ—¶éšVideoStreamå¯åŠ¨éŸ³é¢‘æå–
                except Exception as e:
                    print(f"è­¦å‘Š: å£°å­¦æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨æ­¤åŠŸèƒ½ã€‚é”™è¯¯: {e}")

            # 5. åˆå§‹åŒ–ç«ç„°çƒŸé›¾æ£€æµ‹å™¨
            try:
                fire_model_path = os.path.join(self.config.ASSET_BASE_PATH, "models", "torch", "yolov8n-fire.pt")
                if os.path.exists(fire_model_path):
                    self._detectors["fire"] = FlameSmokeDetector(model_path=fire_model_path)
                else:
                    general_model_path = os.path.join(self.config.ASSET_BASE_PATH, "models", "torch", "yolov8n.pt")
                    self._detectors["fire"] = FlameSmokeDetector(model_path=general_model_path)
                print("ç«ç„°çƒŸé›¾æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"è­¦å‘Š: ç«ç„°çƒŸé›¾æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨æ­¤åŠŸèƒ½ã€‚é”™è¯¯: {e}")

            print("--- æ‰€æœ‰æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ ---")

        except Exception as e:
            print(f"è‡´å‘½é”™è¯¯: æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def _run_acoustic_analysis(self, camera_id: str):
        """åå°æŒç»­è¿è¡Œçš„åç¨‹ï¼Œç”¨äºåˆ†æç‰¹å®šæ‘„åƒå¤´çš„éŸ³é¢‘æ•°æ®ã€‚"""
        print(f"å£°å­¦åˆ†æåå°ä»»åŠ¡å·²å¯åŠ¨ for {camera_id}ã€‚")
        acoustic_detector = self._detectors.get("acoustic")
        if not acoustic_detector:
            print("å£°å­¦æ£€æµ‹å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿è¡Œå£°å­¦åˆ†æä»»åŠ¡ã€‚")
            return

        stream_obj = self._video_streams.get(camera_id)
        if not stream_obj or not hasattr(stream_obj, 'get_audio_file'):
            print(f"æ‘„åƒå¤´ {camera_id} ä¸æ”¯æŒéŸ³é¢‘æå–æˆ–è§†é¢‘æµæœªå¯åŠ¨ã€‚")
            return

        while stream_obj.is_running and self.config.ENABLE_SOUND_DETECTION:
            try:
                audio_file = stream_obj.get_audio_file()  # è¿™ä¸ªæ–¹æ³•éœ€è¦åœ¨VideoStreamä¸­å®ç°
                if audio_file and os.path.exists(audio_file):
                    events = await acoustic_detector.process_audio_file(audio_file)
                    for event in events:
                        event_emoji = {
                            "volume_anomaly": "ğŸ“¢", "high_frequency_noise": "ğŸ”Š", "sudden_noise": "ğŸ’¥"
                        }.get(event['type'], "ğŸ””")

                        print(f"{event_emoji} [éŸ³é¢‘] {event['description']} (Camera: {camera_id})")
                        self.send_alert_to_backend(
                            AIAnalysisResult(
                                camera_id=camera_id,
                                event_type=f"acoustic_{event['type']}",
                                location={"timestamp": event['timestamp']},
                                confidence=event['confidence'],
                                timestamp=datetime.now().isoformat(),
                                details={"description": event['description'], "audio_timestamp": event['timestamp']}
                            )
                        )
                else:
                    # ä»…åœ¨éWebcamProcessorçš„æƒ…å†µä¸‹æ‰“å°è­¦å‘Š
                    if not getattr(stream_obj, 'is_webcam', False):
                        print(f"âš ï¸ éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°æˆ–æœªå°±ç»ª for {camera_id}: {audio_file}")
            except Exception as e:
                print(f"å£°å­¦åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ for {camera_id}: {e}")
                traceback.print_exc()

            await asyncio.sleep(acoustic_detector.detection_interval)

    def send_alert_to_backend(self, result: AIAnalysisResult):
        """å‘é€AIåˆ†æç»“æœåˆ°åç«¯ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶"""

        def task():
            try:
                session = requests.Session()
                retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[500, 502, 503, 504])
                session.mount('http://', HTTPAdapter(max_retries=retries))
                session.mount('https://', HTTPAdapter(max_retries=retries))

                data = result.model_dump_json()  # ä½¿ç”¨pydanticçš„model_dump_json
                headers = {
                    'Content-Type': 'application/json',
                    'X-AI-Service': 'true',
                    'X-API-Key': self.config.AI_SERVICE_API_KEY,
                }

                response = session.post(self.config.BACKEND_ALERT_URL, data=data, headers=headers, timeout=5)

                # ã€å¯é€‰ä¿®æ”¹ã€‘æ ¹æ® 201 çŠ¶æ€ç ä¹Ÿè®¤ä¸ºæ˜¯æˆåŠŸï¼Œé¿å…æ—¥å¿—è¯¯æŠ¥
                if response.status_code in [200, 201]:
                    print(f"âœ… æˆåŠŸå‘é€å‘Šè­¦åˆ°åç«¯: {result.event_type}")
                else:
                    print(f"âŒ å‘é€å‘Šè­¦å¤±è´¥: HTTP {response.status_code}, å“åº”å†…å®¹: {response.text}")
            except Exception as e:
                print(f"âŒ å‘é€å‘Šè­¦æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                try:
                    with open('failed_alerts.json', 'a') as f:
                        # ç¡®ä¿ data å˜é‡åœ¨å¼‚å¸¸å‘ç”Ÿæ—¶ä¹Ÿå¯ç”¨
                        json_data = result.model_dump_json() if 'result' in locals() else {}
                        json.dump(
                            {"timestamp": datetime.now().isoformat(), "alert": json.loads(json_data), "error": str(e)},
                            f)
                        f.write('\n')
                except Exception as write_error:
                    print(f"æ— æ³•ä¿å­˜å¤±è´¥çš„å‘Šè­¦: {str(write_error)}")

        self._thread_pool.submit(task)

    async def send_detection_to_websocket(self, camera_id: str, detection_results: dict):
        """é€šè¿‡WebSocketå‘å‰ç«¯å‘é€æ£€æµ‹ç»“æœæ•°æ® (éå›¾åƒ)"""

        def task():
            try:
                # ã€ä¿®å¤ã€‘ä¿®æ­£å‘é€åˆ°WebSocketçš„æ•°æ®ç»“æ„
                websocket_data = {
                    "type": "detection_result",
                    "data": {
                        "camera_id": camera_id,
                        "detections": detection_results  # ä¹‹å‰è¿™é‡Œç¼ºå°‘äº† "detections" è¿™ä¸ªkey
                    }
                }

                headers = {
                    'Content-Type': 'application/json',
                    'X-API-Key': self.config.AI_SERVICE_API_KEY
                }

                response = requests.post(
                    self.config.BACKEND_WEBSOCKET_BROADCAST_URL,
                    data=json.dumps(websocket_data),
                    headers=headers,
                    timeout=2
                )

                if response.status_code == 200:
                    print(f"âœ… æ£€æµ‹æ•°æ®å·²å‘é€åˆ°WebSocket: {camera_id}")
                else:
                    print(f"âŒ å‘é€æ£€æµ‹æ•°æ®åˆ°WebSocketå¤±è´¥: {response.status_code}, {response.text}")

            except Exception as e:
                print(f"å‘é€WebSocketä»»åŠ¡å‡ºé”™: {e}")

        # ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥æ‰§è¡Œï¼Œé¿å…é˜»å¡ä¸»å¾ªç¯
        self._thread_pool.submit(task)

    async def shutdown_services(self):
        """å…³é—­æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„æœåŠ¡å’Œæ¸…ç†èµ„æº"""
        print("æœåŠ¡æ­£åœ¨å…³é—­ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
        for stream in list(self._video_streams.values()):  # ä½¿ç”¨list()é¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹å­—å…¸
            stream.stop()
        if "acoustic" in self._detectors:
            self._detectors["acoustic"].stop_monitoring()
        self._thread_pool.shutdown(wait=True)
        print("èµ„æºæ¸…ç†å®Œæ¯•ã€‚")

    # --- AI æ£€æµ‹ç»“æœç¨³å®šåŒ–å‡½æ•° (ç§»åŠ¨åˆ°æ­¤ç±»ä¸­) ---
    def _calculate_bbox_distance(self, bbox1, bbox2):
        center1 = [(bbox1[0] + bbox1[2]) / 2, (bbox1[1] + bbox1[3]) / 2]
        center2 = [(bbox2[0] + bbox2[2]) / 2, (bbox2[1] + bbox2[3]) / 2]
        return ((center1[0] - center2[0]) ** 2 + (center1[1] - center2[1]) ** 2) ** 0.5

    def _calculate_bbox_overlap(self, bbox1, bbox2):
        x1 = max(bbox1[0], bbox2[0])
        y1 = max(bbox1[1], bbox2[1])
        x2 = min(bbox1[2], bbox2[2])
        y2 = min(bbox1[3], bbox2[3])
        if x2 <= x1 or y2 <= y1:
            return 0
        intersection = (x2 - x1) * (y2 - y1)
        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
        union = area1 + area2 - intersection
        return intersection / union if union > 0 else 0

    def stabilize_detections(self, camera_id: str, new_detections: List[Dict]) -> List[Dict]:
        """é«˜æ€§èƒ½è‡ªé€‚åº”æ£€æµ‹ç»“æœç¨³å®šåŒ– - äººè„¸æŒç»­è·Ÿè¸ªä¼˜åŒ–"""
        current_time = time.time()
        if camera_id not in self._detection_cache:
            self._detection_cache[camera_id] = {"objects": {}, "face_history": {}}

        cache = self._detection_cache[camera_id]["objects"]
        face_history = self._detection_cache[camera_id]["face_history"]
        results = []
        matched_ids = set()

        face_detections = [d for d in new_detections if d["type"] == "face"]
        other_detections = [d for d in new_detections if d["type"] != "face"]

        face_results = self._advanced_face_tracking(camera_id, face_detections, cache, face_history, current_time,
                                                    matched_ids)
        results.extend(face_results)

        other_results = self._standard_object_tracking(other_detections, cache, current_time, matched_ids, camera_id)
        results.extend(other_results)

        self._cleanup_expired_cache(cache, face_history, current_time)
        return results

    def _advanced_face_tracking(self, camera_id: str, face_detections: List[Dict], cache: Dict, face_history: Dict,
                                current_time: float, matched_ids: set) -> List[Dict]:
        results = []
        config = self._stabilization_config.get(camera_id, {})
        FACE_MATCH_THRESHOLD = config.get('face_match_threshold', 150)
        FACE_SMOOTH_FACTOR = config.get('face_smooth_factor', 0.85)
        JITTER_THRESHOLD = config.get('jitter_detection_threshold', 20)
        SIZE_CHANGE_RATIO = config.get('max_size_change_ratio', 0.2)
        FACE_KEEP_TIME = 1.5
        FACE_MIN_CONFIDENCE = 0.25
        FACE_STABLE_THRESHOLD = 1
        CONFIDENCE_SMOOTH_FACTOR = 0.7

        IDENTITY_HISTORY_SIZE = 8
        IDENTITY_CHANGE_THRESHOLD = 0.6
        IDENTITY_CONFIDENCE_DIFF = 0.12
        MIN_STABLE_FRAMES = 2

        self._predict_missing_faces(cache, face_history, current_time)

        for face_det in face_detections:
            face_bbox = face_det["bbox"]
            best_match_id = None
            best_score = float('inf')

            for obj_id, obj_data in cache.items():
                if obj_data["type"] != "face" or obj_id in matched_ids:
                    continue
                score = self._calculate_face_match_score(face_bbox, obj_data, current_time)
                if score < best_score and score < FACE_MATCH_THRESHOLD:
                    best_score = score
                    best_match_id = obj_id

            if best_match_id:
                old_obj = cache[best_match_id]
                history = face_history.get(best_match_id, {})
                is_jittery = self._detect_jitter(face_bbox, old_obj["bbox"], history, JITTER_THRESHOLD)

                effective_smooth_factor = min(0.97, FACE_SMOOTH_FACTOR + 0.05) if is_jittery else FACE_SMOOTH_FACTOR
                smoothed_bbox = self._advanced_face_smoothing(face_bbox, old_obj, history, effective_smooth_factor)
                smoothed_bbox = self._stabilize_bbox_size(smoothed_bbox, old_obj["bbox"],
                                                          max_change_ratio=SIZE_CHANGE_RATIO)

                old_confidence = old_obj.get("confidence", face_det["confidence"])
                smoothed_confidence = old_confidence * (1 - CONFIDENCE_SMOOTH_FACTOR) + face_det[
                    "confidence"] * CONFIDENCE_SMOOTH_FACTOR
                if smoothed_confidence < old_confidence * 0.7:
                    smoothed_confidence = old_confidence * 0.8

                cache[best_match_id].update({
                    "bbox": smoothed_bbox,
                    "confidence": max(FACE_MIN_CONFIDENCE, smoothed_confidence),
                    "last_seen": current_time,
                    "stable_count": min(old_obj.get("stable_count", 0) + 1, 10),
                    "consecutive_detections": old_obj.get("consecutive_detections", 0) + 1,
                    "last_detection": current_time,
                    "is_jittery": is_jittery,
                    "flicker_protection": True
                })

                if best_match_id not in face_history:
                    face_history[best_match_id] = {"positions": [], "timestamps": []}
                history = face_history[best_match_id]
                history["positions"].append(smoothed_bbox)
                history["timestamps"].append(current_time)
                if len(history["positions"]) > 5:
                    history["positions"] = history["positions"][-5:]
                    history["timestamps"] = history["timestamps"][-5:]

                if "identity" in face_det:
                    new_identity = face_det["identity"].copy()
                    if "confidence" not in new_identity:
                        new_identity["confidence"] = face_det.get("confidence", 0.5)
                    stable_identity = self._stabilize_face_identity(
                        best_match_id, new_identity, face_history, IDENTITY_HISTORY_SIZE,
                        IDENTITY_CHANGE_THRESHOLD, IDENTITY_CONFIDENCE_DIFF, MIN_STABLE_FRAMES
                    )
                    face_det["identity"] = stable_identity
                    cache[best_match_id]["identity"] = stable_identity

                matched_ids.add(best_match_id)
                result_det = face_det.copy()
                result_det["bbox"] = smoothed_bbox
                result_det["tracking_id"] = best_match_id
                result_det["is_stable"] = cache[best_match_id]["stable_count"] >= FACE_STABLE_THRESHOLD
                result_det["consecutive_detections"] = cache[best_match_id]["consecutive_detections"]
                results.append(result_det)
            else:
                new_id = f"face_{int(current_time * 1000) % 10000}"
                cache[new_id] = {
                    "bbox": face_bbox, "confidence": face_det["confidence"], "type": "face",
                    "last_seen": current_time, "stable_count": 1, "consecutive_detections": 1,
                    "first_seen": current_time, "last_detection": current_time
                }
                face_history[new_id] = {"positions": [face_bbox], "timestamps": [current_time]}
                result_det = face_det.copy()
                result_det["tracking_id"] = new_id
                result_det["is_stable"] = False
                result_det["consecutive_detections"] = 1
                results.append(result_det)
                matched_ids.add(new_id)

        for obj_id, obj_data in list(cache.items()):
            if (obj_data["type"] == "face" and obj_id not in matched_ids and obj_data.get("consecutive_detections",
                                                                                          0) >= 3):
                time_since_last_seen = current_time - obj_data["last_seen"]
                if time_since_last_seen < FACE_KEEP_TIME:
                    predicted_bbox = self._predict_face_position(obj_id, face_history, current_time)
                    if predicted_bbox:
                        kept_det = {
                            "type": "face", "bbox": predicted_bbox,
                            "confidence": max(0.3,
                                              obj_data["confidence"] * (1 - time_since_last_seen / FACE_KEEP_TIME)),
                            "timestamp": datetime.now().isoformat(), "tracking_id": obj_id, "is_stable": True,
                            "is_kept": True, "known": False, "name": "æœªçŸ¥",
                            "time_since_detection": time_since_last_seen
                        }
                        results.append(kept_det)
        return results

    def _calculate_face_match_score(self, new_bbox: List[int], obj_data: Dict, current_time: float) -> float:
        old_bbox = obj_data["bbox"]
        old_center = ((old_bbox[0] + old_bbox[2]) / 2, (old_bbox[1] + old_bbox[3]) / 2)
        new_center = ((new_bbox[0] + new_bbox[2]) / 2, (new_bbox[1] + new_bbox[3]) / 2)
        center_distance = ((old_center[0] - new_center[0]) ** 2 + (old_center[1] - new_center[1]) ** 2) ** 0.5
        old_w, old_h = old_bbox[2] - old_bbox[0], old_bbox[3] - old_bbox[1]
        new_w, new_h = new_bbox[2] - new_bbox[0], new_bbox[3] - new_bbox[1]
        size_similarity = abs(old_w - new_w) + abs(old_h - new_h)
        time_weight = min(2.0, current_time - obj_data["last_seen"])
        consecutive_bonus = max(0, 5 - obj_data.get("consecutive_detections", 0))
        score = center_distance + size_similarity * 0.5 + time_weight * 10 + consecutive_bonus
        return score

    def _advanced_face_smoothing(self, new_bbox: List[int], old_obj: Dict, history: Dict, smooth_factor: float) -> \
            List[
                int]:
        old_bbox = old_obj["bbox"]
        smoothed_bbox = [int(old_bbox[i] * (1 - smooth_factor) + new_bbox[i] * smooth_factor) for i in range(4)]
        if history and "positions" in history and len(history["positions"]) >= 2:
            positions = history["positions"]
            timestamps = history.get("timestamps", [])
            if len(positions) >= 3 and len(timestamps) >= 3:
                recent_positions = positions[-3:]
                recent_timestamps = timestamps[-3:]
                total_dx, total_dy = 0, 0
                valid_moves = 0
                for i in range(1, len(recent_positions)):
                    dt = recent_timestamps[i] - recent_timestamps[i - 1]
                    if dt > 0:
                        old_center = ((recent_positions[i - 1][0] + recent_positions[i - 1][2]) / 2,
                                      (recent_positions[i - 1][1] + recent_positions[i - 1][3]) / 2)
                        new_center = ((recent_positions[i][0] + recent_positions[i][2]) / 2,
                                      (recent_positions[i][1] + recent_positions[i][3]) / 2)
                        dx = (new_center[0] - old_center[0]) / dt
                        dy = (new_center[1] - old_center[1]) / dt
                        speed = (dx ** 2 + dy ** 2) ** 0.5
                        if speed < 200:
                            total_dx += dx
                            total_dy += dy
                            valid_moves += 1
                if valid_moves > 0:
                    avg_dx = total_dx / valid_moves
                    avg_dy = total_dy / valid_moves
                    motion_factor = 0.15 if abs(avg_dx) < 50 and abs(avg_dy) < 50 else 0.05
                    predict_offset_x = int(avg_dx * motion_factor)
                    predict_offset_y = int(avg_dy * motion_factor)
                    smoothed_bbox[0] += predict_offset_x
                    smoothed_bbox[1] += predict_offset_y
                    smoothed_bbox[2] += predict_offset_x
                    smoothed_bbox[3] += predict_offset_y
        w = smoothed_bbox[2] - smoothed_bbox[0]
        h = smoothed_bbox[3] - smoothed_bbox[1]
        if w <= 0 or h <= 0:
            return new_bbox
        old_center = ((old_bbox[0] + old_bbox[2]) / 2, (old_bbox[1] + old_bbox[3]) / 2)
        new_center = ((smoothed_bbox[0] + smoothed_bbox[2]) / 2, (smoothed_bbox[1] + smoothed_bbox[3]) / 2)
        center_change = ((old_center[0] - new_center[0]) ** 2 + (old_center[1] - new_center[1]) ** 2) ** 0.5
        if center_change > 25:
            limit_factor = 25 / center_change
            final_center_x = old_center[0] + (new_center[0] - old_center[0]) * limit_factor
            final_center_y = old_center[1] + (new_center[1] - old_center[1]) * limit_factor
            # ã€ä¿®å¤ 1.2ã€‘ ä¿®æ­£äº†è¿™é‡Œçš„å˜é‡åï¼Œä» center_y æ”¹ä¸º final_center_y
            smoothed_bbox = [int(final_center_x - w / 2), int(final_center_y - h / 2), int(final_center_x + w / 2),
                             int(final_center_y + h / 2)]
        return smoothed_bbox

    def _predict_missing_faces(self, cache: Dict, face_history: Dict, current_time: float):
        for obj_id, obj_data in cache.items():
            if obj_data["type"] == "face":
                time_since_last_seen = current_time - obj_data["last_seen"]
                if 0.2 < time_since_last_seen < 1.0:
                    predicted_pos = self._predict_face_position(obj_id, face_history, current_time)
                    if predicted_pos:
                        obj_data["predicted_bbox"] = predicted_pos

    def _predict_face_position(self, obj_id: str, face_history: Dict, current_time: float) -> Optional[List[int]]:
        if obj_id not in face_history or len(face_history[obj_id]["positions"]) < 2:
            return None
        history = face_history[obj_id]
        positions = history["positions"]
        timestamps = history["timestamps"]
        if len(positions) >= 2:
            last_pos = positions[-1]
            prev_pos = positions[-2]
            last_time = timestamps[-1]
            # ã€ä¿®å¤ 1.3ã€‘ ä¿®æ­£äº†è¿™é‡Œçš„ç´¢å¼•ï¼Œç¡®ä¿ä½¿ç”¨æ•°å­—ç´¢å¼• -2
            # è¿™é‡Œçš„å†™æ³•ä¼šä¿è¯åœ¨ len(timestamps) < 2 æ—¶ï¼Œ_predict_face_position æå‰è¿”å› None
            # æ‰€ä»¥ prev_time = timestamps[-2] åœ¨è¿™é‡Œæ˜¯å®‰å…¨çš„
            prev_time = timestamps[-2]

            dt = last_time - prev_time
            if dt > 0:  # é¿å…é™¤ä»¥é›¶æˆ–æ—¶é—´å€’æµ
                vx = (last_pos[0] - prev_pos[0]) / dt
                vy = (last_pos[1] - prev_pos[1]) / dt
                time_delta = current_time - last_time
                predicted_x = last_pos[0] + vx * time_delta
                predicted_y = last_pos[1] + vy * time_delta
                w = last_pos[2] - last_pos[0]
                h = last_pos[3] - last_pos[1]
                return [int(predicted_x), int(predicted_y), int(predicted_x + w), int(predicted_y + h)]
        return None

    def _standard_object_tracking(self, detections: List[Dict], cache: Dict, current_time: float, matched_ids: set,
                                  camera_id: str = "") -> List[Dict]:
        results = []
        config = self._stabilization_config.get(camera_id, {})
        OBJECT_MATCH_THRESHOLD = config.get('object_match_threshold', 80)
        OBJECT_SMOOTH_FACTOR = config.get('object_smooth_factor', 0.95)
        SIZE_CHANGE_RATIO = config.get('max_size_change_ratio', 0.1)
        CONFIDENCE_SMOOTH_FACTOR = 0.8

        for det in detections:
            bbox = det["bbox"]
            det_type = det["type"]
            best_match_id = None
            best_distance = OBJECT_MATCH_THRESHOLD

            for obj_id, obj_data in cache.items():
                if obj_data["type"] != det_type or obj_id in matched_ids:
                    continue
                old_center = ((obj_data["bbox"][0] + obj_data["bbox"][2]) / 2,
                              (obj_data["bbox"][1] + obj_data["bbox"][3]) / 2)
                new_center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)
                distance = ((old_center[0] - new_center[0]) ** 2 + (old_center[1] - new_center[1]) ** 2) ** 0.5
                if distance < best_distance:
                    best_distance = distance
                    best_match_id = obj_id

            if best_match_id:
                old_obj = cache[best_match_id]
                old_bbox = old_obj["bbox"]
                old_center = ((old_bbox[0] + old_bbox[2]) / 2, (old_bbox[1] + old_bbox[3]) / 2)
                new_center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)
                movement = ((old_center[0] - new_center[0]) ** 2 + (old_center[1] - new_center[1]) ** 2) ** 0.5

                enhanced_smooth = min(0.95, OBJECT_SMOOTH_FACTOR + 0.08) if movement > 25 else OBJECT_SMOOTH_FACTOR
                smoothed_bbox = [int(old_bbox[i] * (1 - enhanced_smooth) + bbox[i] * enhanced_smooth) for i in
                                 range(4)]
                smoothed_bbox = self._stabilize_bbox_size(smoothed_bbox, old_bbox, max_change_ratio=SIZE_CHANGE_RATIO)

                old_confidence = old_obj.get("confidence", det["confidence"])
                smoothed_confidence = old_confidence * (1 - CONFIDENCE_SMOOTH_FACTOR) + det[
                    "confidence"] * CONFIDENCE_SMOOTH_FACTOR
                if smoothed_confidence < old_confidence * 0.6:
                    smoothed_confidence = old_confidence * 0.7

                cache[best_match_id].update({
                    "bbox": smoothed_bbox, "confidence": max(0.3, smoothed_confidence), "last_seen": current_time,
                    "stable_count": min(old_obj.get("stable_count", 0) + 1, 8),
                    "consecutive_detections": old_obj.get("consecutive_detections", 0) + 1,
                    "flicker_protection": True
                })
                matched_ids.add(best_match_id)
                result_det = det.copy()
                result_det["bbox"] = smoothed_bbox
                result_det["tracking_id"] = best_match_id
                result_det["is_stable"] = cache[best_match_id]["stable_count"] >= 2
                result_det["consecutive_detections"] = cache[best_match_id]["consecutive_detections"]
                results.append(result_det)
            else:
                new_id = f"{det_type}_{int(current_time * 1000) % 10000}"
                cache[new_id] = {
                    "bbox": bbox, "confidence": det["confidence"], "type": det_type,
                    "last_seen": current_time, "stable_count": 1, "consecutive_detections": 1
                }
                result_det = det.copy()
                result_det["tracking_id"] = new_id
                result_det["is_stable"] = False
                result_det["consecutive_detections"] = 1
                results.append(result_det)
                matched_ids.add(new_id)
        return results

    def _detect_jitter(self, new_bbox: List[int], old_bbox: List[int], history: Dict, threshold: int = 30) -> bool:
        old_center = ((old_bbox[0] + old_bbox[2]) / 2, (old_bbox[1] + old_bbox[3]) / 2)
        new_center = ((new_bbox[0] + new_bbox[2]) / 2, (new_bbox[1] + new_bbox[3]) / 2)
        movement = ((old_center[0] - new_center[0]) ** 2 + (old_center[1] - new_center[1]) ** 2) ** 0.5
        if movement > threshold:
            return True
        if history and "positions" in history and len(history["positions"]) >= 3:
            positions = history["positions"][-3:]
            centers = [((pos[0] + pos[2]) / 2, (pos[1] + pos[3]) / 2) for pos in positions]
            if len(centers) >= 2:
                movements = []
                for i in range(1, len(centers)):
                    move = ((centers[i][0] - centers[i - 1][0]) ** 2 + (centers[i][1] - centers[i - 1][1]) ** 2) ** 0.5
                    movements.append(move)
                if len(movements) >= 2:
                    movement_variance = sum((m - sum(movements) / len(movements)) ** 2 for m in movements) / len(
                        movements)
                    if movement_variance > 100:
                        return True
        return False

    def _stabilize_bbox_size(self, new_bbox: List[int], old_bbox: List[int], max_change_ratio: float = 0.2) -> \
            List[
                int]:
        old_w, old_h = old_bbox[2] - old_bbox[0], old_bbox[3] - old_bbox[1]
        new_w, new_h = new_bbox[2] - new_bbox[0], new_bbox[3] - new_bbox[1]
        w_change_ratio = abs(new_w - old_w) / old_w if old_w > 0 else 0
        h_change_ratio = abs(new_h - old_h) / old_h if old_h > 0 else 0
        if w_change_ratio > max_change_ratio or h_change_ratio > max_change_ratio:
            center_x, center_y = (new_bbox[0] + new_bbox[2]) / 2, (new_bbox[1] + new_bbox[3]) / 2
            max_w_change, max_h_change = old_w * max_change_ratio, old_h * max_change_ratio
            stabilized_w = old_w + max_w_change if new_w > old_w else old_w - max_w_change
            stabilized_h = old_h + max_h_change if new_h > old_h else old_h - max_h_change
            stabilized_bbox = [int(center_x - stabilized_w / 2), int(center_y - stabilized_h / 2),
                               int(center_x + stabilized_w / 2), int(center_y + stabilized_h / 2)]
            return stabilized_bbox
        return new_bbox

    def _process_face_recognition_with_stabilization(self, camera_id: str, frame: np.ndarray) -> List[Dict]:
        current_tolerance = self._face_recognition_config.get('tolerance', 0.65)
        # ã€ä¿®å¤ 1.4ã€‘æ­¤å¤„å·²ç§»é™¤ tolerance å‚æ•°ï¼Œå› ä¸º FaceRecognizer å†…éƒ¨ä¼šå¤„ç†
        recognized_faces = self._detectors["face"].detect_and_recognize(frame)

        face_detections = []
        for face in recognized_faces:
            location = face["location"]
            bbox = [location["left"], location["top"], location["right"], location["bottom"]]
            detection = {
                "type": "face", "bbox": bbox, "confidence": face.get("confidence", 0.5),
                "timestamp": datetime.now().isoformat(), "identity": face["identity"]
            }
            face_detections.append(detection)
            face_name = face.get("identity", {}).get("name", "æœªçŸ¥")
            is_known = face.get("identity", {}).get("known", False)
            logger.info(
                f"âœ¨ [äººè„¸è¯†åˆ«] æ‘„åƒå¤´: {camera_id}, å§“å: {face_name} (å·²çŸ¥: {is_known}), ç½®ä¿¡åº¦: {face.get('confidence', 0.0):.2f}, æ¡†: {bbox}")

        if face_detections:
            stabilized_faces = self.stabilize_detections(camera_id, face_detections)
            return stabilized_faces
        else:
            return []

    def _stabilize_face_identity(self, face_id: str, new_identity: Dict, face_history: Dict, history_size: int = 10,
                                 change_threshold: float = 0.7, confidence_diff: float = 0.15,
                                 min_stable_frames: int = 3) -> Dict:
        if face_id not in face_history:
            face_history[face_id] = {"identity_history": [], "current_identity": new_identity, "stable_count": 0,
                                     "last_change_time": time.time()}
            return new_identity

        history_data = face_history[face_id]
        if "positions" in history_data and "current_identity" not in history_data:  # Compatibility check
            history_data.update({"identity_history": [], "current_identity": new_identity, "stable_count": 0,
                                 "last_change_time": time.time()})
            return new_identity
        if "current_identity" not in history_data:
            history_data["current_identity"] = new_identity
            history_data["identity_history"] = []
            history_data["stable_count"] = 0
            history_data["last_change_time"] = time.time()
            return new_identity

        current_identity = history_data["current_identity"]
        if "identity_history" not in history_data:
            history_data["identity_history"] = []
        identity_history = history_data["identity_history"]
        if "stable_count" not in history_data:
            history_data["stable_count"] = 0

        identity_history.append(
            {"name": new_identity.get("name", "unknown"), "confidence": new_identity.get("confidence", 0),
             "timestamp": time.time()})
        if len(identity_history) > history_size:
            identity_history.pop(0)

        name_votes = {};
        total_weight = 0
        for record in identity_history:
            name = record["name"];
            confidence = record["confidence"]
            weight = max(0.1, confidence)
            name_votes[name] = name_votes.get(name, 0) + weight
            total_weight += weight

        winning_name = "unknown";
        vote_ratio = 0
        if name_votes and total_weight > 0:
            most_voted_name = max(name_votes.items(), key=lambda x: x[1])
            vote_ratio = most_voted_name[1] / total_weight
            winning_name = most_voted_name[0]

        current_name = current_identity.get("name", "unknown")
        new_name = new_identity.get("name", "unknown")

        should_change_identity = False;
        change_reason = ""
        if current_name == winning_name:
            history_data["stable_count"] = history_data.get("stable_count", 0) + 1
            should_change_identity = False
            change_reason = "èº«ä»½ä¸€è‡´ï¼Œä¿æŒç¨³å®š"
        elif vote_ratio >= change_threshold:
            current_confidence = current_identity.get("confidence", 0)
            new_confidence = new_identity.get("confidence", 0)
            if new_confidence > current_confidence + confidence_diff:
                should_change_identity = True
                change_reason = f"æŠ•ç¥¨æ”¯æŒç‡{vote_ratio:.1%}ï¼Œç½®ä¿¡åº¦æå‡{new_confidence - current_confidence:.2f}"
            elif history_data.get("stable_count", 0) >= min_stable_frames:
                should_change_identity = True
                change_reason = f"æŠ•ç¥¨æ”¯æŒç‡{vote_ratio:.1%}ï¼Œå·²ç¨³å®š{history_data['stable_count']}å¸§"
            else:
                should_change_identity = False
                change_reason = f"æŠ•ç¥¨æ”¯æŒç‡{vote_ratio:.1%}ï¼Œä½†ç¨³å®šå¸§æ•°ä¸è¶³({history_data.get('stable_count', 0)}<{min_stable_frames})"
        else:
            should_change_identity = False
            change_reason = f"æŠ•ç¥¨æ”¯æŒç‡ä¸è¶³({vote_ratio:.1%}<{change_threshold:.1%})"

        if should_change_identity:
            best_confidence = 0
            for record in identity_history:
                if record["name"] == winning_name:
                    best_confidence = max(best_confidence, record["confidence"])
            history_data["current_identity"] = {"name": winning_name, "known": winning_name != "unknown",
                                                "confidence": best_confidence}
            history_data["stable_count"] = 0
            history_data["last_change_time"] = time.time()
        else:
            if new_name == current_name and new_identity.get("confidence", 0) > current_identity.get("confidence", 0):
                history_data["current_identity"]["confidence"] = new_identity["confidence"]
        return history_data["current_identity"]

    def _cleanup_expired_cache(self, cache: Dict, face_history: Dict, current_time: float):
        to_remove = []
        for obj_id, obj_data in cache.items():
            time_since_seen = current_time - obj_data["last_seen"]
            if obj_data["type"] == "face":
                if time_since_seen > 6.0:
                    to_remove.append(obj_id)
                elif time_since_seen > 3.0:
                    current_confidence = obj_data.get("confidence", 1.0)
                    fade_factor = max(0.15, 1 - (time_since_seen - 3.0) / 3.0)
                    obj_data["confidence"] = max(0.15, current_confidence * fade_factor)
                    obj_data["fading"] = True
            else:
                if time_since_seen > 3.0:
                    to_remove.append(obj_id)
                elif time_since_seen > 1.5:
                    current_confidence = obj_data.get("confidence", 1.0)
                    fade_factor = max(0.2, 1 - (time_since_seen - 1.5) / 1.5)
                    obj_data["confidence"] = max(0.2, current_confidence * fade_factor)
                    obj_data["fading"] = True
        for obj_id in to_remove:
            del cache[obj_id]
            if obj_id in face_history:
                del face_history[obj_id]
        if len(cache) > 25:
            sorted_items = sorted(cache.items(), key=lambda x: x[1]["last_seen"])
            for obj_id, _ in sorted_items[:-20]:
                del cache[obj_id]
                if obj_id in face_history:
                    del face_history[obj_id]

    # --- æ€§èƒ½ä¼˜åŒ–ç­–ç•¥å‡½æ•° ---
    def _get_performance_strategy(self, performance_mode: str, frame_count: int, is_low_res: bool) -> Dict:
        if performance_mode == "fast":
            is_object_frame = frame_count % 2 == 0
            return {
                "run_object_detection": is_object_frame, "run_face_recognition": not is_object_frame,
                "object_scale_factor": 0.4, "face_scale_factor": 0.5, "use_parallel": False,
                "use_stabilization": frame_count % 4 == 0, "max_detections": 3, "face_limit": 2
            }
        elif performance_mode == "balanced":
            use_parallel_this_frame = frame_count % 3 == 0
            return {
                "run_object_detection": True, "run_face_recognition": True,
                "object_scale_factor": 0.6 if use_parallel_this_frame else 1.0,
                "face_scale_factor": 0.7 if use_parallel_this_frame else 1.1,
                "use_parallel": use_parallel_this_frame, "use_stabilization": True,
                "max_detections": 5, "face_limit": 3
            }
        else:  # quality
            return {
                "run_object_detection": True, "run_face_recognition": True,
                "object_scale_factor": 0.8, "face_scale_factor": 0.9, "use_parallel": True,
                "use_stabilization": True, "max_detections": 8, "face_limit": 3
            }

    # --- æ ¸å¿ƒå¸§åˆ†æé€»è¾‘ (æ•´åˆåˆ°AIServiceManager) ---
    async def process_single_frame(self, frame: np.ndarray, camera_id: str,
                                   enable_face_recognition: bool, enable_object_detection: bool,
                                   enable_behavior_detection: bool, enable_fire_detection: bool,
                                   performance_mode: str) -> Dict:
        start_time = time.time()
        # ç®€å•å¸§è®¡æ•°ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥æ›´ç²¾ç¡®åœ°ç®¡ç†
        frame_count = self._detection_cache.get(camera_id, {}).get("frame_count", 0) + 1
        if camera_id not in self._detection_cache:
            self._detection_cache[camera_id] = {"objects": {}, "face_history": {}, "frame_count": 0}
        self._detection_cache[camera_id]["frame_count"] = frame_count

        height, width = frame.shape[:2]
        is_low_res = width < 400 or height < 400

        if camera_id not in self._object_trackers:
            self._object_trackers[camera_id] = DeepSORTTracker()

        results = {
            "camera_id": camera_id, "timestamp": datetime.now().isoformat(), "detections": [], "alerts": [],
            "performance_info": {"mode": performance_mode, "frame_count": frame_count}
        }

        # åº”ç”¨AIè®¾ç½® (ä»å­˜å‚¨çš„è®¾ç½®ä¸­è·å–)
        camera_settings = self._ai_settings.get(camera_id)
        if camera_settings:
            enable_face_recognition = camera_settings.get('face_recognition', enable_face_recognition)
            enable_object_detection = camera_settings.get('object_detection', enable_object_detection)
            enable_behavior_detection = camera_settings.get('behavior_analysis', enable_behavior_detection)
            enable_fire_detection = camera_settings.get('fire_detection', enable_fire_detection)
            if camera_settings.get('realtime_mode', False):
                performance_mode = "fast"

        strategy = self._get_performance_strategy(performance_mode, frame_count, is_low_res)

        active_tasks = []

        # ç«ç„°æ£€æµ‹
        if enable_fire_detection and "fire" in self._detectors:
            try:
                fire_detector = self._detectors["fire"]
                # ã€ä¿®å¤ 1.5ã€‘ç§»é™¤ confidence_threshold å‚æ•°
                fire_results = fire_detector.detect(frame)
                for fire_obj in fire_results:
                    bbox = [int(float(coord)) for coord in fire_obj["coordinates"]]
                    detection = {
                        "type": "fire_detection", "class_name": fire_obj["class_name"],
                        "detection_type": fire_obj["type"], "confidence": float(fire_obj["confidence"]),
                        "bbox": bbox, "timestamp": datetime.now().isoformat()
                    }
                    results["detections"].append(detection)
                    self.send_alert_to_backend(
                        AIAnalysisResult(
                            camera_id=camera_id, event_type=f"fire_detection_{fire_obj['type']}",
                            location={"box": bbox}, confidence=float(fire_obj["confidence"]),
                            timestamp=datetime.now().isoformat(),
                            details={"detection_type": fire_obj["type"], "object_type": fire_obj["class_name"],
                                     "area": fire_obj["area"], "center": fire_obj["center"]}
                        )
                    )
            except Exception as e:
                print(f"ç«ç„°æ£€æµ‹å¤±è´¥: {e}")
                traceback.print_exc()

        # ç›®æ ‡æ£€æµ‹ä»»åŠ¡
        def optimized_object_detection():
            """å†…éƒ¨å‡½æ•°ï¼Œè´Ÿè´£æ‰§è¡Œå¯¹è±¡æ£€æµ‹å¹¶è¿”å›æ ¼å¼åŒ–ç»“æœã€‚"""
            if not (enable_object_detection and strategy["run_object_detection"]):
                return []
            try:
                obj_scale = strategy["object_scale_factor"]
                obj_height, obj_width = int(height * obj_scale), int(width * obj_scale)
                obj_image = cv2.resize(frame, (obj_width, obj_height))
                # ã€ä¿®å¤ 1.6ã€‘ç§»é™¤ confidence_threshold å‚æ•°
                detected_objects = self._detectors["object"].predict(obj_image)

                scale_back_x, scale_back_y = width / obj_width, height / obj_height
                object_detections = []
                for obj in detected_objects[:strategy["max_detections"]]:
                    bbox = [int(float(obj["coordinates"][0]) * scale_back_x),
                            int(float(obj["coordinates"][1]) * scale_back_y),
                            int(float(obj["coordinates"][2]) * scale_back_x),
                            int(float(obj["coordinates"][3]) * scale_back_y)]
                    object_detections.append({
                        "type": "object", "class_name": obj["class_name"], "confidence": float(obj["confidence"]),
                        "bbox": bbox, "timestamp": datetime.now().isoformat()
                    })
                return object_detections
            except Exception as e:
                logger.error(f"ç›®æ ‡æ£€æµ‹å¤±è´¥: {e}", exc_info=True)
                return []

        # äººè„¸è¯†åˆ«ä»»åŠ¡
        def optimized_face_recognition():
            """å†…éƒ¨å‡½æ•°ï¼Œè´Ÿè´£æ‰§è¡Œäººè„¸è¯†åˆ«å¹¶å¤„ç†ç»“æœã€‚"""
            try:
                face_results = self._detectors["face"].detect_and_recognize(frame)
                processed_results = []
                # ã€é‡è¦ä¿®å¤ã€‘åœ¨ä½¿ç”¨å‰è·å–å½“å‰æ‘„åƒå¤´çš„AIè®¾ç½®
                ai_settings = self.get_ai_settings(camera_id)

                for result in face_results:
                    # æ ¹æ®æ–°çš„è¿”å›æ•°æ®ç»“æ„è§£æç»“æœ
                    identity_info = result.get("identity", {})
                    name = identity_info.get("name", "unknown")
                    is_known = identity_info.get("is_known", False)
                    confidence = identity_info.get("confidence", 0.0)
                    bbox = result.get("bbox")

                    if not bbox:
                        continue

                    # æ‰“å°æ—¥å¿—
                    log_message = (f"âœ¨ [äººè„¸è¯†åˆ«] æ‘„åƒå¤´: {camera_id}, å§“å: {name} "
                                   f"(å·²çŸ¥: {is_known}), ç½®ä¿¡åº¦: {confidence:.2f}, æ¡†: {bbox}")
                    logger.info(log_message)

                    # æ£€æŸ¥æ˜¯å¦ä¸ºå·²çŸ¥äººå‘˜ä¸”è¶…è¿‡ç½®ä¿¡åº¦é˜ˆå€¼
                    if is_known and confidence > 0.5:
                        # å¯ä»¥åœ¨è¿™é‡Œä¸ºè¯†åˆ«åˆ°çš„å·²çŸ¥äººå‘˜è§¦å‘ä¸€ä¸ªäº‹ä»¶
                        pass

                    # æ£€æŸ¥æ˜¯å¦ä¸ºéœ€è¦å‘Šè­¦çš„é™Œç”Ÿäºº
                    elif not is_known and ai_settings.get("stranger_intrusion", False):
                        alert = AIAnalysisResult(
                            camera_id=camera_id,
                            event_type="stranger_intrusion",
                            confidence=1.0,  # é™Œç”Ÿäººé—¯å…¥å¯ä»¥è®¤ä¸ºæ˜¯100%ç¡®å®š
                            timestamp=datetime.now().isoformat(),
                            location={"bounding_box": bbox},
                            details={"description": f"æ£€æµ‹åˆ°æœªçŸ¥äººå‘˜", "name": name}
                        )
                        self.send_alert_to_backend(alert)

                    # å°†è§„èŒƒåŒ–åçš„ç»“æœæ·»åŠ åˆ°è¦å‘é€åˆ°å‰ç«¯çš„åˆ—è¡¨ä¸­
                    processed_results.append({
                        "type": "face",
                        "bbox": bbox,
                        "confidence": confidence,
                        "identity": identity_info
                    })
                return processed_results
            except Exception as e:
                # ã€ä¿®å¤ã€‘æä¾›æ›´è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
                logger.error(f"äººè„¸è¯†åˆ«å¤„ç†è¿‡ç¨‹ä¸­å¤±è´¥: {e}", exc_info=True)
                return []

        # æ‰§è¡Œç­–ç•¥
        if strategy["use_parallel"]:
            with ThreadPoolExecutor(max_workers=2) as executor:
                future_objects = executor.submit(optimized_object_detection)
                future_faces = executor.submit(optimized_face_recognition)
                object_results = future_objects.result()
                face_results = future_faces.result()
        else:
            object_results = optimized_object_detection()
            face_results = optimized_face_recognition()

        # Deep SORTè¿½è¸ª
        tracked_object_results = []
        if object_results and camera_id in self._object_trackers:
            try:
                tracker = self._object_trackers[camera_id]
                detection_list = []
                for obj in object_results:
                    if obj["type"] == "object":
                        detection_list.append({
                            'coordinates': obj['bbox'], 'confidence': obj['confidence'],
                            'class_name': obj['class_name'], 'class_id': obj.get('class_id', 0)
                        })
                if detection_list:
                    # ã€ä¿®å¤ 1.8ã€‘å°† frame æ›¿æ¢ä¸º frame.shape[:2] ä»¥åŒ¹é… FallbackTracker.update çš„æœŸæœ›å‚æ•°
                    tracked_results = tracker.update(detection_list, frame.shape[:2])
                    for tracked_obj in tracked_results:
                        tracked_object_results.append({
                            "type": "object", "class_name": tracked_obj["class_name"],
                            "confidence": tracked_obj["confidence"],
                            "bbox": tracked_obj["coordinates"], "tracking_id": tracked_obj["tracking_id"],
                            "timestamp": datetime.now().isoformat()
                        })
                        # if tracked_obj["class_name"] == "person" and tracked_obj["confidence"] > 0.5:
                        #     self.send_alert_to_backend(
                        #         AIAnalysisResult(
                        #             camera_id=camera_id, event_type="object_person_detected",
                        #             location={"box": tracked_obj["coordinates"]}, confidence=tracked_obj["confidence"],
                        #             timestamp=datetime.now().isoformat(),
                        #             details={"tracking_id": tracked_obj["tracking_id"],
                        #                      "class_name": tracked_obj["class_name"],
                        #                      "tracking_method": "deep_sort", "realtime_detection": True}
                        #         )
                        #     )
            except Exception as e:
                print(f"Deep SORTè¿½è¸ªå¤±è´¥: {e}")
                traceback.print_exc()
        else:
            tracked_object_results = object_results

        all_detections = tracked_object_results + face_results  # Face results are already stabilized
        if enable_behavior_detection and "danger_zone" in sys.modules:  # ç¡®ä¿ danger_zone_detector æ¨¡å—å·²å¯¼å…¥
            person_detections_for_zone = [d for d in all_detections if
                                          d.get('type') == 'object' and d.get('class_name') == 'person']
            if person_detections_for_zone:
                danger_zone_detection_results = danger_zone_detector.detect_intrusions(camera_id, all_detections)

                # åˆ›å»ºä¸€ä¸ª mapï¼Œä»¥ä¾¿é€šè¿‡ tracking_id å¿«é€ŸæŸ¥æ‰¾ detection
                detections_map = {d.get("tracking_id"): d for d in all_detections if d.get("tracking_id")}

                for alert_data in danger_zone_detection_results:
                    # å¦‚æœæ˜¯å…¥ä¾µå‘Šè­¦ï¼Œåˆ™æ ‡è®°å¯¹åº”çš„æ£€æµ‹å¯¹è±¡
                    if alert_data.get("tracking_id") and alert_data.get("type") == "intrusion_detected":
                        tracked_detection = detections_map.get(alert_data["tracking_id"])
                        if tracked_detection:
                            tracked_detection["is_dangerous"] = True  # ã€æ–°å¢ã€‘æ ‡è®°ä¸ºå±é™©
                            tracked_detection["zone_name"] = alert_data.get("zone_name")  # ã€æ–°å¢ã€‘æ·»åŠ åŒºåŸŸåç§°

                    # ä»ç„¶å‘é€åç«¯å‘Šè­¦ç»™ Django (è¿™éƒ¨åˆ†æ˜¯åŸæœ‰çš„å‘Šè­¦å‘é€é€»è¾‘)
                    results["alerts"].append({
                        "type": alert_data["type"], "message": alert_data["message"],
                        "tracking_id": alert_data.get("tracking_id"),
                        "zone_name": alert_data.get("zone_name"), "position": alert_data.get("position"),
                        "distance": alert_data.get("distance"), "dwell_time": alert_data.get("dwell_time")
                    })
                    self.send_alert_to_backend(
                        AIAnalysisResult(
                            camera_id=camera_id, event_type=alert_data["type"],
                            location={"position": alert_data.get("position", [])},
                            confidence=1.0, timestamp=datetime.now().isoformat(),
                            details={"tracking_id": alert_data.get("tracking_id"), "zone_id": alert_data.get("zone_id"),
                                     "zone_name": alert_data.get("zone_name"), "distance": alert_data.get("distance"),
                                     "dwell_time": alert_data.get("dwell_time"),
                                     "detection_method": "danger_zone_geometric",
                                     "realtime_detection": True}
                        )
                    )
        results["detections"] = all_detections

        # å±é™©åŒºåŸŸæ£€æµ‹
        try:
            person_detections = [d for d in all_detections if
                                 d.get('type') == 'object' and d.get('class_name') == 'person']
            if person_detections:
                danger_zone_alerts = danger_zone_detector.detect_intrusions(camera_id, all_detections)
                for alert in danger_zone_alerts:
                    results["alerts"].append({
                        "type": alert["type"], "message": alert["message"], "tracking_id": alert.get("tracking_id"),
                        "zone_name": alert.get("zone_name"), "position": alert.get("position"),
                        "distance": alert.get("distance"), "dwell_time": alert.get("dwell_time")
                    })
                    self.send_alert_to_backend(
                        AIAnalysisResult(
                            camera_id=camera_id, event_type=alert["type"],
                            location={"position": alert.get("position", [])},
                            confidence=1.0, timestamp=datetime.now().isoformat(),
                            details={"tracking_id": alert.get("tracking_id"), "zone_id": alert.get("zone_id"),
                                     "zone_name": alert.get("zone_name"), "distance": alert.get("distance"),
                                     "dwell_time": alert.get("dwell_time"), "detection_method": "danger_zone_geometric",
                                     "realtime_detection": True}
                        )
                    )
        except Exception as e:
            print(f"âš ï¸ å±é™©åŒºåŸŸæ£€æµ‹å¤±è´¥: {e}")

        total_time = (time.time() - start_time) * 1000
        results["performance_info"]["processing_time_ms"] = round(total_time, 1)
        results["performance_info"]["detection_count"] = len(results["detections"])
        results["performance_info"]["tracking_info"] = {
            "deep_sort_objects": len(tracked_object_results),
            "stabilized_faces": len(face_results),
            "tracker_available": camera_id in self._object_trackers
        }

        # å¼‚æ­¥å‘é€æ£€æµ‹ç»“æœåˆ° WebSocket
        if self.get_ai_settings(camera_id).get('realtime_mode', True) and results:
            # ã€ä¿®å¤ã€‘æ•è·åœ¨æœåŠ¡å…³é—­æ—¶å¯èƒ½å‘ç”Ÿçš„RuntimeError
            try:
                # ä½¿ç”¨lambdaç¡®ä¿asyncio.runåœ¨çº¿ç¨‹æ± çš„çº¿ç¨‹ä¸­è¢«è°ƒç”¨
                self._thread_pool.submit(lambda: asyncio.run(self.send_detection_to_websocket(camera_id, results)))
            except RuntimeError as e:
                # å½“æœåŠ¡å…³é—­ï¼Œçº¿ç¨‹æ± å·²é”€æ¯æ—¶ï¼Œè¿™ä¸ªé”™è¯¯æ˜¯é¢„æœŸçš„ã€‚
                # æˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°å¿½ç•¥å®ƒï¼Œæˆ–è€…åªæ‰“å°ä¸€æ¡ç®€çŸ­çš„æ—¥å¿—ã€‚
                if "cannot schedule new futures after shutdown" in str(e):
                    logger.warning(f"æœåŠ¡å…³é—­æœŸé—´ï¼Œæ— æ³•å‘é€æœ€åçš„æ£€æµ‹ç»“æœ: {e}")
                else:
                    # å¦‚æœæ˜¯å…¶ä»–RuntimeErrorï¼Œåˆ™é‡æ–°æŠ›å‡º
                    raise

        # è¿”å›å¤„ç†åçš„ç»“æœ
        return results

    # --- AISettings Model (for API) ---
    class AISettings(BaseModel):
        model_config = ConfigDict(extra='allow')  # å…è®¸é¢å¤–å­—æ®µ
        face_recognition: bool = True
        object_detection: bool = True
        behavior_analysis: bool = False
        sound_detection: bool = False
        fire_detection: bool = True
        realtime_mode: bool = True

    def get_ai_settings(self, camera_id: str) -> Dict:
        if camera_id not in self._ai_settings:
            self._ai_settings[camera_id] = self.AISettings().model_dump()
        return self._ai_settings[camera_id]

    def update_ai_settings(self, camera_id: str, settings: Dict):
        current_settings = self.get_ai_settings(camera_id)
        current_settings.update(settings)
        self._ai_settings[camera_id] = current_settings
        if camera_id in self._video_streams:
            self._video_streams[camera_id].update_settings(settings)

    # --- Face Recognition Sensitivity Management ---
    def get_face_recognition_config(self) -> Dict:
        return self._face_recognition_config

    def update_face_recognition_config(self, config_data: Dict):
        self._face_recognition_config.update(config_data)
        # Clear face cache for immediate effect
        for cam_id in list(self._detection_cache.keys()):
            if cam_id in self._detection_cache and "face_history" in self._detection_cache[cam_id]:
                self._detection_cache[cam_id]["face_history"].clear()
                self._detection_cache[cam_id]["objects"] = {k: v for k, v in
                                                            self._detection_cache[cam_id]["objects"].items() if
                                                            v.get("type") != "face"}

    # --- Object Detection Sensitivity Management ---
    def get_object_detection_config(self) -> Dict:
        return self._object_detection_config

    def update_object_detection_config(self, config_data: Dict):
        self._object_detection_config.update(config_data)
        # Clear object cache for immediate effect (except faces)
        for cam_id in list(self._detection_cache.keys()):
            if cam_id in self._detection_cache and "objects" in self._detection_cache[cam_id]:
                object_cache = self._detection_cache[cam_id]["objects"]
                self._detection_cache[cam_id]["objects"] = {k: v for k, v in object_cache.items() if
                                                            v.get("type") == "face"}

    # --- Stabilization Configuration ---
    def get_stabilization_config(self, camera_id: str) -> Dict:
        default_config = {
            "face_smooth_factor": 0.92, "object_smooth_factor": 0.88,
            "face_match_threshold": 120, "object_match_threshold": 60,
            "jitter_detection_threshold": 30, "max_size_change_ratio": 0.2,
            "updated_at": "é»˜è®¤é…ç½®"
        }
        return self._stabilization_config.get(camera_id, default_config)

    def update_stabilization_config(self, camera_id: str, config_data: Dict):
        if not camera_id in self._stabilization_config:
            self._stabilization_config[camera_id] = self.get_stabilization_config(camera_id).copy()
        self._stabilization_config[camera_id].update(config_data)
        self._stabilization_config[camera_id]["updated_at"] = datetime.now().isoformat()
        # Clear cache for immediate effect
        if camera_id in self._detection_cache:
            self._detection_cache[camera_id] = {"objects": {}, "face_history": {}, "frame_count": 0}

    def apply_stabilization_preset(self, preset_name: str, camera_id: str):
        presets = {
            "anti_flicker": {"name": "ğŸš¨ è¶…å¼ºé˜²é—ªçƒæ¨¡å¼",
                             "config": {"face_smooth_factor": 0.97, "object_smooth_factor": 0.95,
                                        "face_match_threshold": 150, "object_match_threshold": 80,
                                        "jitter_detection_threshold": 15, "max_size_change_ratio": 0.1}},
            "anti_jitter": {"name": "æŠ—æŠ–åŠ¨æ¨¡å¼", "config": {"face_smooth_factor": 0.95, "object_smooth_factor": 0.92,
                                                             "face_match_threshold": 100, "object_match_threshold": 50,
                                                             "jitter_detection_threshold": 20,
                                                             "max_size_change_ratio": 0.15}},
            "ultra_stable": {"name": "è¶…ç¨³å®šæ¨¡å¼", "config": {"face_smooth_factor": 0.97, "object_smooth_factor": 0.95,
                                                              "face_match_threshold": 80, "object_match_threshold": 40,
                                                              "jitter_detection_threshold": 15,
                                                              "max_size_change_ratio": 0.1}},
            "balanced": {"name": "å¹³è¡¡æ¨¡å¼", "config": {"face_smooth_factor": 0.88, "object_smooth_factor": 0.85,
                                                        "face_match_threshold": 120, "object_match_threshold": 60,
                                                        "jitter_detection_threshold": 30,
                                                        "max_size_change_ratio": 0.2}},
            "responsive": {"name": "å“åº”æ¨¡å¼", "config": {"face_smooth_factor": 0.80, "object_smooth_factor": 0.75,
                                                          "face_match_threshold": 150, "object_match_threshold": 80,
                                                          "jitter_detection_threshold": 40,
                                                          "max_size_change_ratio": 0.25}},
            "default": {"name": "é»˜è®¤æ¨¡å¼", "config": {"face_smooth_factor": 0.92, "object_smooth_factor": 0.88,
                                                       "face_match_threshold": 120, "object_match_threshold": 60,
                                                       "jitter_detection_threshold": 30, "max_size_change_ratio": 0.2}}
        }
        if preset_name not in presets:
            raise HTTPException(status_code=404, detail=f"é¢„è®¾ '{preset_name}' ä¸å­˜åœ¨")

        config = presets[preset_name]["config"].copy()
        self.update_stabilization_config(camera_id, config)
        return {"status": "success", "message": f"å·²åº”ç”¨ '{presets[preset_name]['name']}' é…ç½®åˆ°æ‘„åƒå¤´ {camera_id}",
                "preset": presets[preset_name], "applied_config": config}

    def list_stabilization_presets(self) -> Dict:
        return {
            "anti_flicker": {"name": "ğŸš¨ è¶…å¼ºé˜²é—ªçƒæ¨¡å¼", "description": "ä¸“é—¨è§£å†³æ¡†ä¸€é—ªä¸€é—ªé—®é¢˜çš„è¶…å¼ºé…ç½®",
                             "best_for": ["æ£€æµ‹æ¡†é—ªçƒ", "å‡ºç°æ¶ˆå¤±é¢‘ç¹", "ç½®ä¿¡åº¦ä¸ç¨³å®š"],
                             "trade_offs": "æœ€å¼ºç¨³å®šæ€§ï¼Œè½»å¾®å»¶è¿Ÿå¢åŠ "},
            "anti_jitter": {"name": "æŠ—æŠ–åŠ¨æ¨¡å¼", "description": "é’ˆå¯¹ä¸¥é‡æŠ–åŠ¨é—®é¢˜çš„å¼ºåŒ–é…ç½®",
                            "best_for": ["æŠ–åŠ¨ä¸¥é‡", "ç›®æ ‡é¢‘ç¹åˆ‡æ¢", "ä½è´¨é‡æ‘„åƒå¤´"],
                            "trade_offs": "å¯èƒ½å¢åŠ 10-20mså»¶è¿Ÿ"},
            "ultra_stable": {"name": "è¶…ç¨³å®šæ¨¡å¼", "description": "æœ€å¼ºç¨³å®šåŒ–ï¼Œé€‚åˆé™æ€åœºæ™¯ç›‘æ§",
                             "best_for": ["é™æ€ç›‘æ§", "é«˜ç²¾åº¦è¦æ±‚", "ç¨³å®šç¯å¢ƒ"], "trade_offs": "å“åº”ç¨æ…¢ï¼Œé€‚åˆé™æ€åœºæ™¯"},
            "balanced": {"name": "å¹³è¡¡æ¨¡å¼", "description": "å…¼é¡¾ç¨³å®šæ€§å’Œå“åº”æ€§",
                         "best_for": ["å¤§å¤šæ•°åœºæ™¯", "åŠ¨é™ç»“åˆ", "ä¸€èˆ¬ç›‘æ§"], "trade_offs": "ç»¼åˆè¡¨ç°è‰¯å¥½"},
            "responsive": {"name": "å“åº”æ¨¡å¼", "description": "ä¼˜å…ˆå“åº”é€Ÿåº¦ï¼Œè½»åº¦ç¨³å®šåŒ–",
                           "best_for": ["å¿«é€Ÿç§»åŠ¨", "å®æ—¶äº’åŠ¨", "ä½å»¶è¿Ÿè¦æ±‚"], "trade_offs": "å¯èƒ½æœ‰è½»å¾®æŠ–åŠ¨"},
            "default": {"name": "é»˜è®¤æ¨¡å¼", "description": "ç³»ç»Ÿé»˜è®¤è®¾ç½®", "best_for": ["åˆå§‹é…ç½®", "æ ‡å‡†ç¯å¢ƒ"],
                        "trade_offs": "å¯æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´"}
        }

    async def apply_anti_flicker_all_cameras(self) -> Dict:
        applied_cameras = []
        active_cameras = list(self._video_streams.keys())
        if not active_cameras:
            active_cameras = ["default"]  # If no active streams, apply to a default config
        for camera_id in active_cameras:
            self.apply_stabilization_preset("anti_flicker", camera_id)
            applied_cameras.append(camera_id)
        return {"status": "success", "message": "ğŸš¨ å·²å¯¹æ‰€æœ‰æ‘„åƒå¤´åº”ç”¨è¶…å¼ºé˜²é—ªçƒé…ç½®ï¼",
                "applied_cameras": applied_cameras}

    # --- System Status and Performance ---
    def get_system_status(self) -> SystemStatus:
        return SystemStatus(
            active_streams_count=len(self._video_streams),
            detectors_initialized={name: det is not None for name, det in self._detectors.items()},
            system_load=None,  # Placeholder for actual system load
            memory_usage=None  # Placeholder for actual memory usage
        )

    def get_performance_stats(self) -> Dict:
        stats = {"cache_info": {}, "detector_status": {}, "optimization_status": "high_performance_mode"}
        for camera_id, cache_data in self._detection_cache.items():
            if isinstance(cache_data, dict) and "objects" in cache_data:
                stats["cache_info"][camera_id] = {"cached_objects": len(cache_data["objects"]), "types": {}}
                for obj_data in cache_data["objects"].values():
                    obj_type = obj_data.get("type", "unknown")
                    stats["cache_info"][camera_id]["types"][obj_type] = stats["cache_info"][camera_id]["types"].get(
                        obj_type, 0) + 1
        stats["detector_status"] = {
            "object_detection": "parallel_enabled" if "object" in self._detectors else "disabled",
            "face_recognition": "adaptive_scaling_enabled" if "face" in self._detectors else "disabled",
            "behavior_detection": "enabled" if "behavior" in self._detectors else "disabled",
            "acoustic_detection": "enabled" if "acoustic" in self._detectors else "disabled"
        }
        stats["performance_features"] = {
            "parallel_processing": True, "adaptive_image_scaling": {"object": "60%", "face": "60-85%"},
            "face_specific_stabilization": True, "size_aware_matching": True, "memory_leak_protection": True,
            "async_backend_communication": True, "smart_caching": True
        }
        return {"status": "success", "stats": stats}

    # --- Danger Zone Management ---
    def update_danger_zones(self, camera_id: str, zones_data: List[Dict]):
        danger_zone_detector.update_camera_zones(camera_id, zones_data)

    def get_danger_zone_status(self, camera_id: str) -> Dict:
        return danger_zone_detector.get_zone_status(camera_id)

    def remove_danger_zone(self, camera_id: str, zone_id: str):
        danger_zone_detector.remove_danger_zone(camera_id, zone_id)

    def get_status(self):
        # ... existing code ...
        return status_info

    async def reload_face_recognizer(self):
        """
        å¼‚æ­¥è§¦å‘äººè„¸è¯†åˆ«å™¨é‡æ–°åŠ è½½å·²çŸ¥äººè„¸æ•°æ®ã€‚
        """
        if not self.face_recognizer:
            logger.warning("äººè„¸è¯†åˆ«å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•é‡æ–°åŠ è½½ã€‚")
            return {"status": "failed", "message": "Face recognizer not initialized."}

        try:
            logger.info("=== æ”¶åˆ°APIè¯·æ±‚ï¼Œå¼€å§‹åœ¨åå°é‡æ–°åŠ è½½äººè„¸æ•°æ®åº“ ===")
            await self.face_recognizer.reload_known_faces()
            logger.info("=== äººè„¸æ•°æ®åº“é‡æ–°åŠ è½½ä»»åŠ¡å·²æˆåŠŸè°ƒåº¦ ===")
            return {"status": "success", "message": "Known faces reload has been scheduled."}
        except Exception as e:
            logger.error(f"é‡æ–°åŠ è½½äººè„¸æ•°æ®åº“æ—¶å‡ºé”™: {e}", exc_info=True)
            return {"status": "failed", "message": str(e)}

    def update_stream_settings(self, camera_id, settings):
        """
        æ›´æ–°æŒ‡å®šè§†é¢‘æµçš„AIåˆ†æè®¾ç½®ã€‚
        """
        if camera_id not in self.video_streams:
            logger.warning(f"å°è¯•æ›´æ–°ä¸å­˜åœ¨çš„è§†é¢‘æµè®¾ç½®: {camera_id}")
            return None

        stream_processor = self.video_streams[camera_id]
        if stream_processor:
            stream_processor.update_settings(settings)
            logger.info(f"æˆåŠŸæ›´æ–°æ‘„åƒå¤´ {camera_id} çš„AIè®¾ç½®: {settings}")
            return stream_processor.get_settings()
        return None


# åˆ›å»ºæœåŠ¡ç®¡ç†å™¨å®ä¾‹
service_manager = AIServiceManager(app_config)


# --- FastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨ã€‚"""
    await service_manager.initialize_detectors()
    yield  # æœåŠ¡åœ¨æ­¤è¿è¡Œæ—¶ï¼Œå¤„ç†APIè¯·æ±‚
    await service_manager.shutdown_services()


# åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹
app = FastAPI(
    title="SmartEye AI Service",
    description="æä¾›è§†é¢‘æµå¤„ç†ã€ç›®æ ‡æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«ã€äººè„¸è¯†åˆ«å’Œå£°å­¦äº‹ä»¶æ£€æµ‹èƒ½åŠ›",
    version="2.0.0",
    lifespan=lifespan
)

# å¢åŠ è¯·æ±‚è¶…æ—¶è®¾ç½®ä¸­é—´ä»¶
from starlette.middleware.base import BaseHTTPMiddleware
from fastapi.responses import JSONResponse


class TimeoutMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        try:
            return await asyncio.wait_for(call_next(request), timeout=app_config.FASTAPI_TIMEOUT_SECONDS)
        except asyncio.TimeoutError:
            return JSONResponse(status_code=504, content={"detail": "è¯·æ±‚å¤„ç†è¶…æ—¶ï¼Œè¯·é‡è¯•"})


app.add_middleware(TimeoutMiddleware)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- API ç«¯ç‚¹å®šä¹‰ ---

# RTMPæ¨æµå›è°ƒå¤„ç†
@app.post("/rtmp/publish")
async def rtmp_publish_callback(request: dict = Body(...)):
    stream_name = request.get('name', '')
    camera_id = f"rtmp_stream_{stream_name}"
    print(f"ğŸ“¡ RTMPæ¨æµå¼€å§‹: stream={stream_name}, camera_id={camera_id}")
    return {"status": "success", "message": "æ¨æµå·²æ¥å—", "camera_id": camera_id}


@app.post("/rtmp/publish_done")
async def rtmp_publish_done_callback(request: dict = Body(...)):
    stream_name = request.get('name', '')
    camera_id = f"rtmp_stream_{stream_name}"
    print(f"ğŸ“¡ RTMPæ¨æµç»“æŸ: {stream_name}")
    if camera_id in service_manager._video_streams:
        # ä¿®å¤ç‚¹ï¼šç¡®ä¿ stream_to_stop è¢«æ­£ç¡®å®šä¹‰
        stream_to_stop = service_manager._video_streams[camera_id]
        await stream_to_stop.stop()  # VideoStream å†…éƒ¨ä¼šåœæ­¢éŸ³é¢‘æå–å’Œå¸§å¾ªç¯

        del service_manager._video_streams[camera_id]
        if camera_id in service_manager._object_trackers:
            del service_manager._object_trackers[camera_id]
        if camera_id in service_manager._detection_cache:
            del service_manager._detection_cache[camera_id]
        print(f"å·²åœæ­¢è§†é¢‘æµ: {stream_to_stop.stream_url}")  # ä½¿ç”¨ä¿®å¤åçš„å˜é‡å
        return {"status": "success", "message": "æ¨æµç»“æŸå¤„ç†å®Œæˆ"}


# AIåˆ†æè®¾ç½®
@app.post("/frame/analyze/settings/{camera_id}")
async def update_ai_settings(camera_id: str, settings: service_manager.AISettings = Body(...)):
    service_manager.update_ai_settings(camera_id, settings.model_dump())
    return {"status": "success", "message": "AIåˆ†æè®¾ç½®å·²æ›´æ–°", "settings": service_manager.get_ai_settings(camera_id)}


@app.get("/frame/analyze/settings/{camera_id}")
async def get_ai_settings(camera_id: str):
    return service_manager.get_ai_settings(camera_id)


# å¯åŠ¨/åœæ­¢è§†é¢‘æµ
@app.post("/stream/start/")
async def start_stream(config: CameraConfig):  # Using CameraConfig from models/alert_models.py
    if config.camera_id in service_manager._video_streams:
        raise HTTPException(status_code=400, detail=f"æ‘„åƒå¤´ {config.camera_id} å·²åœ¨è¿è¡Œ")

    try:
        print(f"æ­£åœ¨å¯åŠ¨è§†é¢‘æµ: {config.stream_url}")
        stream = VideoStream(stream_url=config.stream_url, camera_id=config.camera_id,
                             predictor=service_manager._detectors.get("object"),
                             face_recognizer=service_manager._detectors.get("face"),
                             fire_detector=service_manager._detectors.get("fire"))

        is_available = await stream.test_connection()
        if not is_available:
            raise HTTPException(status_code=400, detail="æ— æ³•è¿æ¥åˆ°è§†é¢‘æµï¼Œè¯·æ£€æŸ¥æµåœ°å€æ˜¯å¦æ­£ç¡®")

        # åŠ¨æ€åˆå§‹åŒ– Deep SORT è¿½è¸ªå™¨
        if config.camera_id not in service_manager._object_trackers:
            service_manager._object_trackers[config.camera_id] = DeepSORTTracker()
            print(f"ä¸ºæ‘„åƒå¤´ {config.camera_id} åˆå§‹åŒ–Deep SORTè¿½è¸ªå™¨")

        # å¯åŠ¨éŸ³é¢‘æå– (å¦‚æœå¯ç”¨å£°å­¦æ£€æµ‹)
        if config.enable_sound_detection and service_manager.config.ENABLE_SOUND_DETECTION:
            await stream.start_audio_extraction()
            # å¯åŠ¨å£°å­¦åˆ†æåå°ä»»åŠ¡ (æ¯ä¸ªæµä¸€ä¸ªä»»åŠ¡)
            asyncio.create_task(service_manager._run_acoustic_analysis(config.camera_id))

        if not await stream.start():
            raise HTTPException(status_code=500, detail="æ— æ³•å¯åŠ¨è§†é¢‘æµå¤„ç†")

        # å°†æµå®ä¾‹æ·»åŠ åˆ°ç®¡ç†å™¨
        service_manager._video_streams[config.camera_id] = stream

        # å¯åŠ¨å¼‚æ­¥å¸§å¤„ç†ä»»åŠ¡
        asyncio.create_task(process_video_stream_async_loop(stream, config.camera_id))

        return {"status": "success", "message": f"æˆåŠŸå¯åŠ¨æ‘„åƒå¤´ {config.camera_id} çš„è§†é¢‘æµå¤„ç†"}

    except Exception as e:
        print(f"å¯åŠ¨è§†é¢‘æµæ—¶å‡ºé”™: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨è§†é¢‘æµæ—¶å‡ºé”™: {str(e)}")


async def process_video_stream_async_loop(stream: VideoStream, camera_id: str):
    """å¼‚æ­¥å¤„ç†è§†é¢‘æµå¾ªç¯ï¼Œä»VideoStreamè·å–å¸§å¹¶é€å…¥åˆ†æå™¨"""
    print(f"å¼€å§‹å¼‚æ­¥å¤„ç†è§†é¢‘æµå¾ªç¯: {camera_id}")
    frame_process_counter = 0
    while stream.is_running:
        try:
            # è¿™é‡Œç›´æ¥ä» VideoStream è·å–å¸§ï¼ŒVideoStream å†…éƒ¨è´Ÿè´£è¯»å–
            success, frame = stream.get_raw_frame()  # VideoStream éœ€è¦æš´éœ²è¿™ä¸ªæ–¹æ³•
            if not success or frame is None:
                await asyncio.sleep(0.01)  # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…ç©ºè½¬
                continue

            # ä½¿ç”¨ AIServiceManager çš„æ ¸å¿ƒåˆ†æé€»è¾‘
            # æ³¨æ„ï¼šè¿™é‡Œæ˜¯å•å¸§ä¸Šä¼ APIçš„é€»è¾‘ï¼Œå¦‚æœæ˜¯åœ¨è¿ç»­è§†é¢‘æµä¸­ï¼Œ
            # é€»è¾‘ä¼šç•¥æœ‰ä¸åŒï¼Œéœ€è¦VideoStreamçš„process_frameè¿›è¡Œå¤„ç†å¹¶è¿”å›ç»“æœ
            # ä¸ºäº†ä¸ç°æœ‰ä»£ç ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å‡è®¾VideoStreamå·²ç»é›†æˆäº†æ£€æµ‹å™¨å¹¶å¤„ç†å¸§

            # è¿™é‡Œçš„ process_frame åº”è¯¥æ˜¯ç”± stream è‡ªèº«è°ƒç”¨çš„ã€‚
            # è¿™é‡Œéœ€è¦ä¿®æ”¹ VideoStream å†…éƒ¨çš„ _process_frames å¾ªç¯ï¼Œè®©å®ƒç›´æ¥è°ƒç”¨ service_manager.process_single_frame
            # æˆ–è€…ï¼Œå°† service_manager.process_single_frame çš„é€»è¾‘ç§»åŠ¨åˆ° VideoStream.process_frame ä¸­

            # å½“å‰ä»£ç çš„ç»“æ„æ„å‘³ç€ app.py çš„ process_video_stream_async_loop ä¼šä» stream è·å–å¸§
            # ç„¶åå°†å¸§äº¤ç»™ service_manager.process_single_frame è¿›è¡Œåˆ†æã€‚
            # è¿™æ˜¯ä¸ºäº†å¤ç”¨ analyze_frame API çš„æ ¸å¿ƒåˆ†æé€»è¾‘ã€‚

            # è·å–å½“å‰AIåˆ†æè®¾ç½®
            settings = service_manager.get_ai_settings(camera_id)
            performance_mode = "balanced"  # é»˜è®¤å€¼
            if settings:
                performance_mode = "fast" if settings.get('realtime_mode', False) else "balanced"

            # è°ƒç”¨ AIServiceManager çš„æ ¸å¿ƒåˆ†ææ–¹æ³•
            analysis_results = await service_manager.process_single_frame(
                frame=frame,
                camera_id=camera_id,
                enable_face_recognition=settings.get('face_recognition', True),
                enable_object_detection=settings.get('object_detection', True),
                enable_behavior_detection=settings.get('behavior_analysis', False),
                enable_fire_detection=settings.get('fire_detection', True),
                performance_mode=performance_mode
            )

            # åˆ†æç»“æœä¼šç”± service_manager å†…éƒ¨å¼‚æ­¥å‘é€ç»™åç«¯å’ŒWebSocket

            # æ§åˆ¶å¸§ç‡ (å¦‚æœéœ€è¦)
            await asyncio.sleep(0.02)  # çº¦50fps

        except Exception as e:
            print(f"è§†é¢‘æµå¤„ç†å¾ªç¯é”™è¯¯ [{camera_id}]: {e}")
            traceback.print_exc()
            await asyncio.sleep(1)  # å‘ç”Ÿé”™è¯¯æ—¶ç­‰å¾…1ç§’å†ç»§ç»­


@app.post("/stream/stop/{camera_id}")
async def stop_stream(camera_id: str):
    if camera_id not in service_manager._video_streams:
        raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°æ‘„åƒå¤´ {camera_id} çš„è§†é¢‘æµ")

    stream_processor = service_manager._video_streams[camera_id]
    await stream_processor.stop()  # VideoStream å†…éƒ¨ä¼šåœæ­¢éŸ³é¢‘æå–å’Œå¸§å¾ªç¯

    del service_manager._video_streams[camera_id]
    if camera_id in service_manager._object_trackers:
        del service_manager._object_trackers[camera_id]
    if camera_id in service_manager._detection_cache:
        del service_manager._detection_cache[camera_id]
    print(f"å·²åœæ­¢è§†é¢‘æµ: {stream_processor.stream_url}")
    return {"status": "success", "message": f"è§†é¢‘æµå¤„ç†å·²åœæ­¢"}


# äººè„¸æ³¨å†Œ
class FaceData(BaseModel):
    person_name: str
    image_data: str


@app.post("/face/register/")
async def register_face(face_data: FaceData):
    try:
        image_bytes = base64.b64decode(face_data.image_data)
        nparr = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if image is None:
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„Base64å›¾åƒæ•°æ®ã€‚")

        # è°ƒç”¨äººè„¸è¯†åˆ«å™¨çš„æ·»åŠ æ–¹æ³• (FaceRecognizer ç±»ä¸­éœ€è¦å®ç°æ­¤æ–¹æ³•)
        face_recognizer_instance = service_manager._detectors.get("face")
        if not face_recognizer_instance:
            raise HTTPException(status_code=500, detail="äººè„¸è¯†åˆ«å™¨æœªåˆå§‹åŒ–ã€‚")

        # æš‚æ—¶è¿”å›æˆåŠŸï¼Œå¾…FaceRecognizerå®ç°add_face
        print(f"æ”¶åˆ°äººè„¸æ³¨å†Œè¯·æ±‚: {face_data.person_name}")
        return {"status": "success", "message": "äººè„¸æ³¨å†Œè¯·æ±‚å·²æ”¶åˆ° (åŠŸèƒ½å¾…FaceRecognizerå®ç°)ã€‚"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/verify_face")
async def verify_face(request_data: dict):
    try:
        image_data = base64.b64decode(request_data.get("image", ""))
        nparr = np.frombuffer(image_data, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if image is None:
            return {"success": False, "message": "æ— æ³•è§£ç å›¾åƒ"}

        user_id = request_data.get("user_id")
        if not user_id:
            return {"success": False, "message": "æœªæä¾›ç”¨æˆ·ID"}

        face_recognizer_instance = service_manager._detectors.get("face")
        if not face_recognizer_instance:
            return {"success": False, "message": "äººè„¸è¯†åˆ«å™¨æœªåˆå§‹åŒ–ã€‚"}

        # æš‚æ—¶è¿”å›æˆåŠŸï¼Œå¾…FaceRecognizerå®ç°æ¯”å¯¹
        print(f"æ”¶åˆ°äººè„¸éªŒè¯è¯·æ±‚: user_id={user_id}")
        return {"success": True, "matched": True, "confidence": 0.9,
                "message": "äººè„¸éªŒè¯è¯·æ±‚å·²æ”¶åˆ° (åŠŸèƒ½å¾…FaceRecognizerå®ç°)ã€‚"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# è§†é¢‘æµè¿æ¥æµ‹è¯•
@app.post("/stream/test/")
async def test_stream_connection_endpoint(url: str = Body(...), type: str = Body(...)):
    print(f"æ­£åœ¨æµ‹è¯•è§†é¢‘æµè¿æ¥: {url} (ç±»å‹: {type})")
    # ä¸ºäº†æµ‹è¯•ï¼Œè¿™é‡Œä¸ä¼ å…¥å®é™…çš„æ£€æµ‹å™¨å®ä¾‹
    stream = VideoStream(stream_url=url, camera_id="test_connection_id")
    is_available = await stream.test_connection()
    return {"status": "success" if is_available else "error",
            "message": "è§†é¢‘æµå¯ç”¨" if is_available else "æ— æ³•è¿æ¥åˆ°è§†é¢‘æµ"}


# æ·»åŠ ä¸€ä¸ªå¥åº·æ£€æŸ¥æ¥å£
@app.get("/system/status/")
async def get_system_status():
    return {"status": "ok", "message": "AI service is running"}


# å¯åŠ¨Uvicorn
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)
    uvicorn.run(app, host="0.0.0.0", port=8001)