import datetime
import os
import base64
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager
from typing import Dict, List, Optional

import torch
import uvicorn
import cv2
import numpy as np
import requests
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Body
from pydantic import BaseModel

# å¯¼å…¥æˆ‘ä»¬è‡ªå®šä¹‰çš„æ‰€æœ‰æ ¸å¿ƒAIæ¨¡å—
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.video_stream import VideoStream
from core.object_detection import GenericPredictor
from core.behavior_detection import BehaviorDetector
from core.face_recognition import FaceRecognizer
from core.acoustic_detection import AcousticDetector
from models.alert_models import AIAnalysisResult  # ç¡®ä¿è¿™ä¸ªæ–‡ä»¶å­˜åœ¨

# åœ¨åº”ç”¨å¯åŠ¨æ—¶ï¼Œä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡
current_dir = os.path.dirname(os.path.abspath(__file__))
# æ„å»º .env æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
dotenv_path = os.path.join(current_dir, '.env')
# ä»æŒ‡å®šè·¯å¾„åŠ è½½ç¯å¢ƒå˜é‡
if os.path.exists(dotenv_path):
    print(f"--- æ­£åœ¨ä» '{dotenv_path}' åŠ è½½ç¯å¢ƒå˜é‡ ---")
    load_dotenv(dotenv_path=dotenv_path)
else:
    print(f"--- è­¦å‘Š: æœªæ‰¾åˆ° .env æ–‡ä»¶ at '{dotenv_path}'ï¼Œå°†ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡ ---")
load_dotenv()

# --- å…¨å±€å˜é‡ ---
video_streams: Dict[str, VideoStream] = {}
detectors: Dict[str, object] = {}
thread_pool = ThreadPoolExecutor(max_workers=os.cpu_count() or 4)


# --- FastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† ---

def init_detectors():
    """åˆå§‹åŒ–æ‰€æœ‰AIæ£€æµ‹å™¨æ¨¡å‹ã€‚è¿™ä¸ªå‡½æ•°åœ¨æœåŠ¡å¯åŠ¨æ—¶åªæ‰§è¡Œä¸€æ¬¡ã€‚"""
    try:
        print("--- æ­£åœ¨åˆå§‹åŒ–æ‰€æœ‰æ£€æµ‹å™¨ ---")


        # æˆ‘ä»¬ä¸å†ç¡¬ç¼–ç ä»»ä½•è·¯å¾„ï¼Œè€Œæ˜¯ä» .env æ–‡ä»¶ä¸­è¯»å–ç»Ÿä¸€çš„æ ¹è·¯å¾„ã€‚
        ASSET_BASE_PATH = os.getenv("G_DRIVE_ASSET_PATH")
        if not ASSET_BASE_PATH or not os.path.isdir(ASSET_BASE_PATH):
            raise FileNotFoundError(
                f"è‡´å‘½é”™è¯¯: ç¯å¢ƒå˜é‡ 'G_DRIVE_ASSET_PATH' æŒ‡å‘çš„èµ„æºç›®å½• '{ASSET_BASE_PATH}' ä¸å­˜åœ¨æˆ–æœªé…ç½®ã€‚è¯·æ£€æŸ¥ .env æ–‡ä»¶å’ŒGç›˜ç›®å½•ã€‚")

        print(f"--- ä½¿ç”¨èµ„æºæ ¹ç›®å½•: {ASSET_BASE_PATH} ---")

        # æ‰€æœ‰èµ„æºçš„è·¯å¾„éƒ½å°†åŸºäº ASSET_BASE_PATH æ„å»ºï¼Œå®ç°é›†ä¸­ç®¡ç†ã€‚
        model_weights_path = os.path.join(ASSET_BASE_PATH, "models", "torch",
                                          "fasterrcnn_resnet50_fpn_coco-258fb6c6.pth")
        class_names_path = os.path.join(ASSET_BASE_PATH, "models", "coco.names")
        known_faces_dir = os.path.join(ASSET_BASE_PATH, "known_faces")

        # 1. åˆå§‹åŒ–é€šç”¨ç›®æ ‡æ£€æµ‹å™¨
        class_names = []
        try:
            with open(class_names_path, 'r', encoding='utf-8') as f:
                class_names = [line.strip() for line in f.readlines()]
            print(f"æˆåŠŸä» '{class_names_path}' åŠ è½½ {len(class_names)} ä¸ªç±»åˆ«åç§°ã€‚")
        except FileNotFoundError:
            print(f"è­¦å‘Š: æ‰¾ä¸åˆ°ç±»åˆ«åç§°æ–‡ä»¶ at '{class_names_path}'ã€‚")
            # å³ä½¿æ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œä¹Ÿæä¾›ä¸€ä¸ªåŸºç¡€çš„é»˜è®¤å€¼ä»¥ç»´æŒè¿è¡Œ
            class_names = ["background", "person"]

            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_weights_path):
            print(f"è‡´å‘½é”™è¯¯: æ¨¡å‹æ–‡ä»¶åœ¨æŒ‡å®šè·¯å¾„æœªæ‰¾åˆ°: {model_weights_path}")
            print("è¯·ç¡®è®¤æ¨¡å‹å·²ä¸‹è½½å¹¶æ”¾ç½®åœ¨æ­£ç¡®çš„Gç›˜ç›®å½•ä¸‹ï¼Œç›®å½•ç»“æ„è¯·å‚è€ƒæ–‡æ¡£è¯´æ˜ã€‚")
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_weights_path}")

        print(f"æ­£åœ¨åŠ è½½ç›®æ ‡æ£€æµ‹æ¨¡å‹æƒé‡: {model_weights_path}")

        detectors["object"] = GenericPredictor(
            model_weights_path=model_weights_path,  # ä½¿ç”¨æˆ‘ä»¬æ–°æ„å»ºçš„è·¯å¾„
            num_classes=len(class_names),
            class_names=class_names
        )

        # 2. åˆå§‹åŒ–è¡Œä¸ºæ£€æµ‹å™¨
        detectors["behavior"] = BehaviorDetector()

        # 3. åˆå§‹åŒ–äººè„¸è¯†åˆ«å™¨
        print(f"æ­£åœ¨ä»ç›®å½• '{known_faces_dir}' åŠ è½½å·²çŸ¥äººè„¸ã€‚")
        detectors["face"] = FaceRecognizer(
            known_faces_dir=known_faces_dir  # ä½¿ç”¨æˆ‘ä»¬æ–°æ„å»ºçš„è·¯å¾„
        )

        # 4. åˆå§‹åŒ–å£°å­¦äº‹ä»¶æ£€æµ‹å™¨ (å¦‚æœå¯ç”¨äº†)
        if os.getenv("ENABLE_SOUND_DETECTION", "false").lower() == "true":
            try:
                detectors["acoustic"] = AcousticDetector()
            except Exception as e:
                print(f"è­¦å‘Š: å£°å­¦æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨æ­¤åŠŸèƒ½ã€‚é”™è¯¯: {e}")

        print("--- æ‰€æœ‰æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ ---")

    except Exception as e:
        print(f"è‡´å‘½é”™è¯¯: æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        raise


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨ã€‚"""
    # å¯åŠ¨ä»»åŠ¡
    init_detectors()
    if "acoustic" in detectors:
        detectors["acoustic"].start_listening()
        asyncio.create_task(run_acoustic_analysis())

    yield  # æœåŠ¡åœ¨æ­¤è¿è¡Œæ—¶ï¼Œå¤„ç†APIè¯·æ±‚

    # å…³é—­ä»»åŠ¡
    print("æœåŠ¡æ­£åœ¨å…³é—­ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
    for stream in video_streams.values():
        stream.stop()
    if "acoustic" in detectors and detectors["acoustic"].is_running:
        detectors["acoustic"].stop_listening()
    thread_pool.shutdown(wait=True)
    print("èµ„æºæ¸…ç†å®Œæ¯•ã€‚")


# åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹
app = FastAPI(
    title="AI æ™ºèƒ½åˆ†ææœåŠ¡ (æœ€ç»ˆç‰ˆ)",
    description="æä¾›è§†é¢‘æµå¤„ç†ã€ç›®æ ‡æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«ã€äººè„¸è¯†åˆ«å’Œå£°å­¦äº‹ä»¶æ£€æµ‹èƒ½åŠ›",
    version="2.0.0",
    lifespan=lifespan
)


# --- æ•°æ®æ¨¡å‹ (ä¿æŒä¸å˜) ---

class StreamConfig(BaseModel):
    camera_id: str
    stream_url: str
    enable_face_recognition: bool = True
    enable_behavior_detection: bool = True


class FaceData(BaseModel):
    person_name: str
    image_data: str


# --- æ ¸å¿ƒå‡½æ•° (ä¿æŒä¸å˜) ---

def send_result_to_backend(result: AIAnalysisResult):
    """å°†åˆ†æç»“æœå¼‚æ­¥å‘é€åˆ°åç«¯DjangoæœåŠ¡ã€‚"""

    def task():
        backend_url = os.getenv("DJANGO_BACKEND_URL", "http://127.0.0.1:8000/api/alerts/ai-results/")
        try:
            response = requests.post(backend_url, json=result.model_dump(), timeout=10)
            if 200 <= response.status_code < 300:
                print(f"âœ… [ç»“æœä¸ŠæŠ¥] æˆåŠŸå‘é€äº‹ä»¶ '{result.event_type}' åˆ°åç«¯ã€‚")
            else:
                print(f"âŒ [ç»“æœä¸ŠæŠ¥] å‘é€å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, åç«¯å“åº”: {response.text}")
        except requests.exceptions.RequestException as e:
            print(f"âŒ [è¯·æ±‚å¼‚å¸¸] æ— æ³•è¿æ¥åˆ°åç«¯æœåŠ¡: {e}")

    thread_pool.submit(task)


async def run_acoustic_analysis():
    """åœ¨åå°æŒç»­è¿è¡Œçš„åç¨‹ï¼Œç”¨äºåˆ†æéŸ³é¢‘æ•°æ®ã€‚"""
    print("å£°å­¦åˆ†æåå°ä»»åŠ¡å·²å¯åŠ¨ã€‚")
    acoustic_detector = detectors.get("acoustic")
    if not acoustic_detector: return

    while acoustic_detector.is_running:
        result = acoustic_detector.analyze_audio_chunk(volume_threshold=0.1)
        if result and result["need_alert"]:
            print(f"ğŸš¨ [éŸ³é¢‘] æ£€æµ‹åˆ°å¼‚å¸¸å£°éŸ³! éŸ³é‡: {result['details']['volume']}")
            alert = AIAnalysisResult(
                camera_id="audio_sensor_01",
                event_type=result["event_type"],
                location={"source": "microphone", "details": result['details']},
                confidence=result["confidence"],
                timestamp=datetime.datetime.now().isoformat(),
            )
            send_result_to_backend(alert)
        await asyncio.sleep(0.1)


def create_master_processor(camera_id: str, config: StreamConfig):
    """
    åˆ›å»ºä¸€ä¸ªä¸»AIå¤„ç†å™¨ï¼Œå®ƒè´Ÿè´£åè°ƒè°ƒç”¨æ‰€æœ‰è§†è§‰ç›¸å…³çš„AIæ£€æµ‹å™¨ã€‚
    """

    def master_processor(frame: np.ndarray):
        try:
            # ç›®æ ‡æ£€æµ‹
            detected_objects = detectors["object"].predict(frame, confidence_threshold=0.6)
            
            # æ‰“å°æ£€æµ‹åˆ°çš„ç›®æ ‡
            for obj in detected_objects:
                print(f"ğŸ¯ æ£€æµ‹åˆ° {obj['class_name']}: ç½®ä¿¡åº¦={obj['confidence']:.2f}, ä½ç½®={obj['coordinates']}")

            # è¡Œä¸ºæ£€æµ‹
            if config.enable_behavior_detection:
                person_boxes = [obj["coordinates"] for obj in detected_objects if obj["class_name"] == "person"]
                if person_boxes:
                    behaviors = detectors["behavior"].detect_behavior(frame, person_boxes, time.time())
                    for behavior in behaviors:
                        if behavior["is_abnormal"] and behavior["need_alert"]:
                            print(f"ğŸš¨ [{camera_id}] æ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸º: {behavior['behavior']}!")
                            alert = AIAnalysisResult(
                                camera_id=camera_id,
                                event_type=f"abnormal_behavior_{behavior['behavior']}",
                                location={"box": behavior["box"]},
                                confidence=behavior["confidence"],
                                timestamp=datetime.datetime.now().isoformat(),
                            )
                            send_result_to_backend(alert)

            if config.enable_face_recognition:
                recognized_faces = detectors["face"].detect_and_recognize(frame)
                for face in recognized_faces:
                    if not face["identity"]["known"]:  # ä¿®æ”¹è¿™é‡Œï¼Œä½¿ç”¨æ–°çš„åˆ¤æ–­é€»è¾‘
                        print(f"ğŸš¨ [{camera_id}] æ£€æµ‹åˆ°æœªçŸ¥äººå‘˜!")
                        alert = AIAnalysisResult(
                            camera_id=camera_id,
                            event_type="unknown_face_detected",
                            location={
                                "box": [
                                    face["location"]["left"],
                                    face["location"]["top"],
                                    face["location"]["right"],
                                    face["location"]["bottom"]
                                ]
                            },
                            confidence=face.get("confidence", 0.9),
                            timestamp=datetime.datetime.now().isoformat(),
                            details={
                                "face_location": face["location"],
                                "best_match": face.get("best_match")
                            }
                        )
                        send_result_to_backend(alert)

        except Exception as e:
            print(f"å¤„ç†å¸§æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯ [{camera_id}]: {e}")

    return master_processor


# --- API ç«¯ç‚¹ (ä¿æŒä¸å˜) ---

@app.post("/stream/start/")
async def start_stream(
    camera_id: str = Body(...),
    stream_url: str = Body(...),
    enable_face_recognition: bool = Body(default=True),
    enable_behavior_detection: bool = Body(default=True),
    enable_sound_detection: bool = Body(default=True)  # é»˜è®¤å¯ç”¨å£°éŸ³æ£€æµ‹
):
    """
    å¯åŠ¨è§†é¢‘æµå¤„ç†ã€‚
    """
    if camera_id in video_streams:
        return {"status": "error", "message": f"æ‘„åƒå¤´ {camera_id} å·²åœ¨è¿è¡Œ"}

    def master_processor(frame: np.ndarray):
        try:
            # ç›®æ ‡æ£€æµ‹
            detected_objects = detectors["object"].predict(frame, confidence_threshold=0.6)
            
            # æ‰“å°æ£€æµ‹åˆ°çš„ç›®æ ‡
            for obj in detected_objects:
                print(f"ğŸ¯ æ£€æµ‹åˆ° {obj['class_name']}: ç½®ä¿¡åº¦={obj['confidence']:.2f}, ä½ç½®={obj['coordinates']}")

            # è¡Œä¸ºæ£€æµ‹
            if enable_behavior_detection:
                person_boxes = [obj["coordinates"] for obj in detected_objects if obj["class_name"] == "person"]
                if person_boxes:
                    behaviors = detectors["behavior"].detect_behavior(frame, person_boxes, time.time())
                    for behavior in behaviors:
                        if behavior["is_abnormal"] and behavior["need_alert"]:
                            print(f"ğŸš¨ [{camera_id}] æ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸º: {behavior['behavior']}!")
                            alert = AIAnalysisResult(
                                camera_id=camera_id,
                                event_type=f"abnormal_behavior_{behavior['behavior']}",
                                location={"box": behavior["box"]},
                                confidence=behavior["confidence"],
                                timestamp=datetime.datetime.now().isoformat(),
                            )
                            send_result_to_backend(alert)

            # äººè„¸è¯†åˆ«
            if enable_face_recognition:
                recognized_faces = detectors["face"].detect_and_recognize(frame)
                for face in recognized_faces:
                    if not face["identity"]["known"]:
                        print(f"ğŸš¨ [{camera_id}] æ£€æµ‹åˆ°æœªçŸ¥äººè„¸!")
                        alert = AIAnalysisResult(
                            camera_id=camera_id,
                            event_type="unknown_face_detected",
                            location=face["location"],
                            confidence=face["identity"]["confidence"],
                            timestamp=datetime.datetime.now().isoformat(),
                        )
                        send_result_to_backend(alert)

        except Exception as e:
            print(f"å¤„ç†å™¨æ‰§è¡Œé”™è¯¯: {e}")
            traceback.print_exc()

    try:
        # åˆ›å»ºè§†é¢‘æµå®ä¾‹ï¼Œå¦‚æœå¯ç”¨äº†å£°éŸ³æ£€æµ‹ï¼Œä¼ å…¥å£°éŸ³æ£€æµ‹å™¨
        acoustic_detector = detectors.get("acoustic") if enable_sound_detection else None
        stream = VideoStream(stream_url, acoustic_detector=acoustic_detector)
        
        if not stream.start():
            return {"status": "error", "message": "æ— æ³•å¯åŠ¨è§†é¢‘æµ"}

        # æ·»åŠ ä¸»å¤„ç†å™¨
        stream.add_processor(master_processor)
        
        # ä¿å­˜æµå®ä¾‹
        video_streams[camera_id] = stream
        
        return {
            "status": "success",
            "message": f"æˆåŠŸå¯åŠ¨æ‘„åƒå¤´ {camera_id}",
            "stream_info": stream.get_stream_info()
        }
        
    except Exception as e:
        print(f"å¯åŠ¨è§†é¢‘æµæ—¶å‡ºé”™: {e}")
        return {"status": "error", "message": str(e)}

@app.post("/stream/stop/{camera_id}")
async def stop_stream(camera_id: str):
    """åœæ­¢è§†é¢‘æµå¤„ç†ã€‚"""
    try:
        if camera_id in video_streams:
            # åœæ­¢è§†é¢‘æµå¤„ç†
            stream_processor = video_streams[camera_id]
            stream_processor.stop()
            del video_streams[camera_id]
            
            # å¦‚æœæ²¡æœ‰æ´»è·ƒçš„è§†é¢‘æµäº†ï¼Œåœæ­¢å£°éŸ³æ£€æµ‹
            if not video_streams and detectors.get("acoustic") and detectors["acoustic"].is_running:
                detectors["acoustic"].stop_listening()
                print("å£°éŸ³æ£€æµ‹å·²åœæ­¢")

            print(f"å·²åœæ­¢è§†é¢‘æµ: {stream_processor.stream_url}")
            return {"status": "success", "message": "è§†é¢‘æµå¤„ç†å·²åœæ­¢"}
        else:
            return {"status": "error", "message": f"æœªæ‰¾åˆ°æ‘„åƒå¤´ {camera_id} çš„è§†é¢‘æµ"}

    except Exception as e:
        print(f"åœæ­¢è§†é¢‘æµæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return {"status": "error", "message": str(e)}


@app.post("/face/register/")
async def register_face(face_data: FaceData):
    """æ³¨å†Œæ–°çš„äººè„¸åˆ°äººè„¸è¯†åˆ«å™¨ä¸­ã€‚"""
    try:
        image_bytes = base64.b64decode(face_data.image_data)
        image_array = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
        if image is None:
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„Base64å›¾åƒæ•°æ®ã€‚")

        # è°ƒç”¨äººè„¸è¯†åˆ«å™¨çš„æ·»åŠ æ–¹æ³• (éœ€åœ¨FaceRecognizerç±»ä¸­å®ç°æ­¤æ–¹æ³•)
        # if detectors["face"].add_face(image, face_data.person_name):
        #     return {"status": "success", "message": f"äººè„¸ '{face_data.person_name}' æ³¨å†ŒæˆåŠŸã€‚"}
        # else:
        #     raise HTTPException(status_code=400, detail="æ³¨å†Œå¤±è´¥ï¼Œå¯èƒ½æœªåœ¨å›¾åƒä¸­æ£€æµ‹åˆ°äººè„¸ã€‚")

        # æš‚æ—¶è¿”å›æˆåŠŸï¼Œå¾…FaceRecognizerå®ç°add_face
        print(f"æ”¶åˆ°äººè„¸æ³¨å†Œè¯·æ±‚: {face_data.person_name}")
        return {"status": "success", "message": "äººè„¸æ³¨å†Œè¯·æ±‚å·²æ”¶åˆ° (åŠŸèƒ½å¾…å®ç°)ã€‚"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/system/status/")
async def get_system_status():
    """è·å–æ•´ä¸ªAIæœåŠ¡çš„å½“å‰çŠ¶æ€ã€‚"""
    return {
        "active_streams_count": len(video_streams),
        "detectors_initialized": {name: det is not None for name, det in detectors.items()},
        "active_streams_details": {
            cam_id: stream.get_stream_info() for cam_id, stream in video_streams.items()
        }
    }


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)