# æ–‡ä»¶: ai_service/app.py
# æè¿°: æ™ºèƒ½è§†é¢‘åˆ†ææœåŠ¡çš„ä¸»å…¥å£ï¼Œè´Ÿè´£APIè·¯ç”±ã€æœåŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€AIåŠŸèƒ½åè°ƒã€‚

import os
import asyncio
import time
import base64
import traceback
import json
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager
from typing import Dict, List, Optional, Any, Union, Tuple

import cv2
import numpy as np
import uvicorn
import requests
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Body, File, UploadFile, Form, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, ConfigDict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# å¯¼å…¥æˆ‘ä»¬è‡ªå®šä¹‰çš„æ‰€æœ‰æ ¸å¿ƒAIæ¨¡å—å’Œæ¨¡å‹
import sys

# å°†å½“å‰ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥æ ¸å¿ƒæ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.video_stream import VideoStream
from core.object_detection import GenericPredictor
from core.behavior_detection import BehaviorDetector
from core.face_recognition import FaceRecognizer
from core.acoustic_detection import AcousticEventDetector
from core.fire_smoke_detection import FlameSmokeDetector
from core.multi_object_tracker import DeepSORTTracker
from core.danger_zone_detection import danger_zone_detector
from models.alert_models import AIAnalysisResult, CameraConfig, SystemStatus  # ç¡®ä¿è¿™äº›æ–‡ä»¶å­˜åœ¨ä¸”ç»“æ„æ­£ç¡®


# --- é…ç½®ç®¡ç† ---
class AppConfig:
    """åº”ç”¨ç¨‹åºçš„å…¨å±€é…ç½®ç±»"""

    def __init__(self):
        # ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡
        current_dir = os.path.dirname(os.path.abspath(__file__))
        dotenv_path = os.path.join(current_dir, '.env')
        if os.path.exists(dotenv_path):
            print(f"--- æ­£åœ¨ä» '{dotenv_path}' åŠ è½½ç¯å¢ƒå˜é‡ ---")
            load_dotenv(dotenv_path=dotenv_path)
        else:
            print(f"--- è­¦å‘Š: æœªæ‰¾åˆ° .env æ–‡ä»¶ at '{dotenv_path}'ï¼Œå°†ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡ ---")
            load_dotenv()  # å°è¯•ä»ç³»ç»Ÿç¯å¢ƒå˜é‡åŠ è½½

        self.ASSET_BASE_PATH = os.getenv("G_DRIVE_ASSET_PATH", "/app/assets")  # é»˜è®¤å€¼ï¼Œå¦‚æœæœªé…ç½®
        self.AI_SERVICE_API_KEY = os.getenv('AI_SERVICE_API_KEY', 'smarteye-ai-service-key-2024')
        self.BACKEND_ALERT_URL = os.getenv('BACKEND_ALERT_URL', 'http://localhost:8000/api/alerts/ai-results/')
        self.BACKEND_WEBSOCKET_BROADCAST_URL = os.getenv('BACKEND_WEBSOCKET_BROADCAST_URL',
                                                         'http://localhost:8000/api/alerts/websocket/broadcast/')
        self.ENABLE_SOUND_DETECTION = os.getenv("ENABLE_SOUND_DETECTION", "false").lower() == "true"
        self.FASTAPI_TIMEOUT_SECONDS = float(os.getenv("FASTAPI_TIMEOUT_SECONDS", "120.0"))

        print(f"--- ä½¿ç”¨èµ„æºæ ¹ç›®å½•: {self.ASSET_BASE_PATH} ---")


# åˆå§‹åŒ–é…ç½®
app_config = AppConfig()


# --- AIServiceManager ç±»ï¼šé›†ä¸­ç®¡ç†AIæœåŠ¡é€»è¾‘å’Œæ•°æ® ---
class AIServiceManager:
    def __init__(self, config: AppConfig):
        self.config = config
        self._detectors: Dict[str, Any] = {}  # å­˜å‚¨æ‰€æœ‰AIæ£€æµ‹å™¨å®ä¾‹
        self._video_streams: Dict[str, VideoStream] = {}  # å­˜å‚¨æ´»è·ƒçš„VideoStreamå®ä¾‹
        self._object_trackers: Dict[str, DeepSORTTracker] = {}  # å­˜å‚¨æ¯ä¸ªæ‘„åƒå¤´çš„DeepSORTè¿½è¸ªå™¨
        self._detection_cache: Dict[str, Dict] = {}  # å­˜å‚¨æ¯ä¸ªæ‘„åƒå¤´çš„æ£€æµ‹ç¨³å®šåŒ–ç¼“å­˜
        self._thread_pool = ThreadPoolExecutor(max_workers=os.cpu_count() or 4)
        self._ai_settings: Dict[str, Dict] = {}  # å­˜å‚¨æ¯ä¸ªæ‘„åƒå¤´çš„AIåˆ†æè®¾ç½®
        self._face_recognition_config: Dict[str, Any] = {
            "tolerance": 0.65, "detection_model": "auto", "enable_multi_scale": True,
            "min_face_size": 40, "max_upsample": 4, "use_cnn_fallback": True
        }
        self._object_detection_config: Dict[str, Any] = {
            "confidence_threshold": 0.35, "area_ratio_threshold": 0.9,
            "special_class_threshold": 0.6, "enable_size_filter": True,
            "enable_aspect_ratio_filter": True
        }
        # æ£€æµ‹ç»“æœç¨³å®šåŒ–å‚æ•°å­˜å‚¨ (per camera_id)
        self._stabilization_config: Dict[str, Dict] = {}

    async def initialize_detectors(self):
        """åˆå§‹åŒ–æ‰€æœ‰AIæ£€æµ‹å™¨æ¨¡å‹ã€‚"""
        print("--- æ­£åœ¨åˆå§‹åŒ–æ‰€æœ‰æ£€æµ‹å™¨ ---")
        try:
            model_weights_path = os.path.join(self.config.ASSET_BASE_PATH, "models", "torch", "yolov8n.pt")
            class_names_path = os.path.join(self.config.ASSET_BASE_PATH, "models", "coco.names")
            known_faces_dir = os.path.join(self.config.ASSET_BASE_PATH, "known_faces")

            # 1. åˆå§‹åŒ–é€šç”¨ç›®æ ‡æ£€æµ‹å™¨
            class_names = []
            try:
                with open(class_names_path, 'r', encoding='utf-8') as f:
                    class_names = [line.strip() for line in f.readlines()]
                print(f"æˆåŠŸä» '{class_names_path}' åŠ è½½ {len(class_names)} ä¸ªç±»åˆ«åç§°ã€‚")
            except FileNotFoundError:
                print(f"è­¦å‘Š: æ‰¾ä¸åˆ°ç±»åˆ«åç§°æ–‡ä»¶ at '{class_names_path}'ã€‚")
                class_names = ["background", "person"]

            self._detectors["object"] = GenericPredictor(
                model_weights_path=model_weights_path,
                num_classes=len(class_names),
                class_names=class_names
            )

            # 2. åˆå§‹åŒ–è¡Œä¸ºæ£€æµ‹å™¨ (ç®€åŒ–ç‰ˆ)
            self._detectors["behavior"] = BehaviorDetector()

            # 3. åˆå§‹åŒ–äººè„¸è¯†åˆ«å™¨
            print(f"æ­£åœ¨ä»ç›®å½• '{known_faces_dir}' åŠ è½½å·²çŸ¥äººè„¸ã€‚")
            self._detectors["face"] = FaceRecognizer(known_faces_dir=known_faces_dir)

            # 4. åˆå§‹åŒ–å£°å­¦äº‹ä»¶æ£€æµ‹å™¨
            if self.config.ENABLE_SOUND_DETECTION:
                try:
                    self._detectors["acoustic"] = AcousticEventDetector()
                    # å£°å­¦åˆ†æåå°ä»»åŠ¡åœ¨start_streamæ—¶éšVideoStreamå¯åŠ¨éŸ³é¢‘æå–
                except Exception as e:
                    print(f"è­¦å‘Š: å£°å­¦æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨æ­¤åŠŸèƒ½ã€‚é”™è¯¯: {e}")

            # 5. åˆå§‹åŒ–ç«ç„°çƒŸé›¾æ£€æµ‹å™¨
            try:
                fire_model_path = os.path.join(self.config.ASSET_BASE_PATH, "models", "torch", "yolov8n-fire.pt")
                if os.path.exists(fire_model_path):
                    self._detectors["fire"] = FlameSmokeDetector(model_path=fire_model_path)
                else:
                    general_model_path = os.path.join(self.config.ASSET_BASE_PATH, "models", "torch", "yolov8n.pt")
                    self._detectors["fire"] = FlameSmokeDetector(model_path=general_model_path)
                print("ç«ç„°çƒŸé›¾æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"è­¦å‘Š: ç«ç„°çƒŸé›¾æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨æ­¤åŠŸèƒ½ã€‚é”™è¯¯: {e}")

            print("--- æ‰€æœ‰æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ ---")

        except Exception as e:
            print(f"è‡´å‘½é”™è¯¯: æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def _run_acoustic_analysis(self, camera_id: str):
        """åå°æŒç»­è¿è¡Œçš„åç¨‹ï¼Œç”¨äºåˆ†æç‰¹å®šæ‘„åƒå¤´çš„éŸ³é¢‘æ•°æ®ã€‚"""
        print(f"å£°å­¦åˆ†æåå°ä»»åŠ¡å·²å¯åŠ¨ for {camera_id}ã€‚")
        acoustic_detector = self._detectors.get("acoustic")
        if not acoustic_detector:
            print("å£°å­¦æ£€æµ‹å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿è¡Œå£°å­¦åˆ†æä»»åŠ¡ã€‚")
            return

        stream_obj = self._video_streams.get(camera_id)
        if not stream_obj or not hasattr(stream_obj, 'get_audio_file'):
            print(f"æ‘„åƒå¤´ {camera_id} ä¸æ”¯æŒéŸ³é¢‘æå–æˆ–è§†é¢‘æµæœªå¯åŠ¨ã€‚")
            return

        while stream_obj.is_running and self.config.ENABLE_SOUND_DETECTION:
            try:
                audio_file = stream_obj.get_audio_file()  # è¿™ä¸ªæ–¹æ³•éœ€è¦åœ¨VideoStreamä¸­å®ç°
                if audio_file and os.path.exists(audio_file):
                    events = await acoustic_detector.process_audio_file(audio_file)
                    for event in events:
                        event_emoji = {
                            "volume_anomaly": "ğŸ“¢", "high_frequency_noise": "ğŸ”Š", "sudden_noise": "ğŸ’¥"
                        }.get(event['type'], "ğŸ””")

                        print(f"{event_emoji} [éŸ³é¢‘] {event['description']} (Camera: {camera_id})")
                        self.send_alert_to_backend(
                            AIAnalysisResult(
                                camera_id=camera_id,
                                event_type=f"acoustic_{event['type']}",
                                location={"timestamp": event['timestamp']},
                                confidence=event['confidence'],
                                timestamp=datetime.now().isoformat(),
                                details={"description": event['description'], "audio_timestamp": event['timestamp']}
                            )
                        )
                else:
                    # ä»…åœ¨éWebcamProcessorçš„æƒ…å†µä¸‹æ‰“å°è­¦å‘Š
                    if not getattr(stream_obj, 'is_webcam', False):
                        print(f"âš ï¸ éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°æˆ–æœªå°±ç»ª for {camera_id}: {audio_file}")
            except Exception as e:
                print(f"å£°å­¦åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ for {camera_id}: {e}")
                traceback.print_exc()

            await asyncio.sleep(acoustic_detector.detection_interval)

    def send_alert_to_backend(self, result: AIAnalysisResult):
        """å‘é€AIåˆ†æç»“æœåˆ°åç«¯ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶"""

        def task():
            try:
                session = requests.Session()
                retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[500, 502, 503, 504])
                session.mount('http://', HTTPAdapter(max_retries=retries))
                session.mount('https://', HTTPAdapter(max_retries=retries))

                data = result.model_dump_json()  # ä½¿ç”¨pydanticçš„model_dump_json
                headers = {
                    'Content-Type': 'application/json',
                    'X-AI-Service': 'true',
                    'X-API-Key': self.config.AI_SERVICE_API_KEY,
                }

                response = session.post(self.config.BACKEND_ALERT_URL, data=data, headers=headers, timeout=5)

                if response.status_code == 200:
                    print(f"âœ… æˆåŠŸå‘é€å‘Šè­¦åˆ°åç«¯: {result.event_type}")
                else:
                    print(f"âŒ å‘é€å‘Šè­¦å¤±è´¥: HTTP {response.status_code}, å“åº”å†…å®¹: {response.text}")
            except Exception as e:
                print(f"âŒ å‘é€å‘Šè­¦æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                try:
                    with open('failed_alerts.json', 'a') as f:
                        # ç¡®ä¿ data å˜é‡åœ¨å¼‚å¸¸å‘ç”Ÿæ—¶ä¹Ÿå¯ç”¨
                        json_data = result.model_dump_json() if 'result' in locals() else {}
                        json.dump(
                            {"timestamp": datetime.now().isoformat(), "alert": json.loads(json_data), "error": str(e)},
                            f)
                        f.write('\n')
                except Exception as write_error:
                    print(f"æ— æ³•ä¿å­˜å¤±è´¥çš„å‘Šè­¦: {str(write_error)}")

        self._thread_pool.submit(task)

    async def send_detection_to_websocket(self, camera_id: str, detection_results: dict):
        """é€šè¿‡WebSocketå‘å‰ç«¯å‘é€æ£€æµ‹ç»“æœæ•°æ® (éå›¾åƒ)"""

        def task():
            try:
                websocket_data = {
                    "type": "detection_result",
                    "data": {
                        "camera_id": camera_id,
                        "timestamp": datetime.now().isoformat(),
                        "detections": detection_results.get("detections", []),
                        "performance_info": detection_results.get("performance_info", {})
                    }
                }
                response = requests.post(
                    self.config.BACKEND_WEBSOCKET_BROADCAST_URL,
                    json=websocket_data,
                    headers={'Content-Type': 'application/json'},
                    timeout=2
                )
                if response.status_code == 200:
                    print(f"âœ… æ£€æµ‹æ•°æ®å·²å‘é€åˆ°WebSocket: {camera_id}")
                else:
                    print(f"âš ï¸ WebSocketæ•°æ®å‘é€å¤±è´¥: {response.status_code}")
            except Exception as e:
                print(f"å‘é€æ£€æµ‹æ•°æ®åˆ°WebSocketå¤±è´¥: {e}")

        self._thread_pool.submit(task)

    async def shutdown_services(self):
        """å…³é—­æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„æœåŠ¡å’Œæ¸…ç†èµ„æº"""
        print("æœåŠ¡æ­£åœ¨å…³é—­ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
        for stream in list(self._video_streams.values()):  # ä½¿ç”¨list()é¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹å­—å…¸
            stream.stop()
        if "acoustic" in self._detectors:
            self._detectors["acoustic"].stop_monitoring()
        self._thread_pool.shutdown(wait=True)
        print("èµ„æºæ¸…ç†å®Œæ¯•ã€‚")

    # --- AI æ£€æµ‹ç»“æœç¨³å®šåŒ–å‡½æ•° (ç§»åŠ¨åˆ°æ­¤ç±»ä¸­) ---
    def _calculate_bbox_distance(self, bbox1, bbox2):
        center1 = [(bbox1[0] + bbox1[2]) / 2, (bbox1[1] + bbox1[3]) / 2]
        center2 = [(bbox2[0] + bbox2[2]) / 2, (bbox2[1] + bbox2[3]) / 2]
        return ((center1[0] - center2[0]) ** 2 + (center1[1] - center2[1]) ** 2) ** 0.5

    def _calculate_bbox_overlap(self, bbox1, bbox2):
        x1 = max(bbox1[0], bbox2[0])
        y1 = max(bbox1[1], bbox2[1])
        x2 = min(bbox1[2], bbox2[2])
        y2 = min(bbox1[3], bbox2[3])
        if x2 <= x1 or y2 <= y1: return 0
        intersection = (x2 - x1) * (y2 - y1)
        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
        union = area1 + area2 - intersection
        return intersection / union if union > 0 else 0

    def stabilize_detections(self, camera_id: str, new_detections: List[Dict]) -> List[Dict]:
        """é«˜æ€§èƒ½è‡ªé€‚åº”æ£€æµ‹ç»“æœç¨³å®šåŒ– - äººè„¸æŒç»­è·Ÿè¸ªä¼˜åŒ–"""
        current_time = time.time()
        if camera_id not in self._detection_cache:
            self._detection_cache[camera_id] = {"objects": {}, "face_history": {}}

        cache = self._detection_cache[camera_id]["objects"]
        face_history = self._detection_cache[camera_id]["face_history"]
        results = []
        matched_ids = set()

        face_detections = [d for d in new_detections if d["type"] == "face"]
        other_detections = [d for d in new_detections if d["type"] != "face"]

        face_results = self._advanced_face_tracking(camera_id, face_detections, cache, face_history, current_time,
                                                    matched_ids)
        results.extend(face_results)

        other_results = self._standard_object_tracking(other_detections, cache, current_time, matched_ids, camera_id)
        results.extend(other_results)

        self._cleanup_expired_cache(cache, face_history, current_time)
        return results

    def _advanced_face_tracking(self, camera_id: str, face_detections: List[Dict], cache: Dict, face_history: Dict,
                                current_time: float, matched_ids: set) -> List[Dict]:
        results = []
        config = self._stabilization_config.get(camera_id, {})
        FACE_MATCH_THRESHOLD = config.get('face_match_threshold', 150)
        FACE_SMOOTH_FACTOR = config.get('face_smooth_factor', 0.85)
        JITTER_THRESHOLD = config.get('jitter_detection_threshold', 20)
        SIZE_CHANGE_RATIO = config.get('max_size_change_ratio', 0.2)
        FACE_KEEP_TIME = 1.5
        FACE_MIN_CONFIDENCE = 0.25
        FACE_STABLE_THRESHOLD = 1
        CONFIDENCE_SMOOTH_FACTOR = 0.7

        IDENTITY_HISTORY_SIZE = 8
        IDENTITY_CHANGE_THRESHOLD = 0.6
        IDENTITY_CONFIDENCE_DIFF = 0.12
        MIN_STABLE_FRAMES = 2

        self._predict_missing_faces(cache, face_history, current_time)

        for face_det in face_detections:
            face_bbox = face_det["bbox"]
            best_match_id = None
            best_score = float('inf')

            for obj_id, obj_data in cache.items():
                if obj_data["type"] != "face" or obj_id in matched_ids: continue
                score = self._calculate_face_match_score(face_bbox, obj_data, current_time)
                if score < best_score and score < FACE_MATCH_THRESHOLD:
                    best_score = score
                    best_match_id = obj_id

            if best_match_id:
                old_obj = cache[best_match_id]
                history = face_history.get(best_match_id, {})
                is_jittery = self._detect_jitter(face_bbox, old_obj["bbox"], history, JITTER_THRESHOLD)

                effective_smooth_factor = min(0.97, FACE_SMOOTH_FACTOR + 0.05) if is_jittery else FACE_SMOOTH_FACTOR
                smoothed_bbox = self._advanced_face_smoothing(face_bbox, old_obj, history, effective_smooth_factor)
                smoothed_bbox = self._stabilize_bbox_size(smoothed_bbox, old_obj["bbox"],
                                                          max_change_ratio=SIZE_CHANGE_RATIO)

                old_confidence = old_obj.get("confidence", face_det["confidence"])
                smoothed_confidence = old_confidence * (1 - CONFIDENCE_SMOOTH_FACTOR) + face_det[
                    "confidence"] * CONFIDENCE_SMOOTH_FACTOR
                if smoothed_confidence < old_confidence * 0.7:
                    smoothed_confidence = old_confidence * 0.8

                cache[best_match_id].update({
                    "bbox": smoothed_bbox,
                    "confidence": max(FACE_MIN_CONFIDENCE, smoothed_confidence),
                    "last_seen": current_time,
                    "stable_count": min(old_obj.get("stable_count", 0) + 1, 10),
                    "consecutive_detections": old_obj.get("consecutive_detections", 0) + 1,
                    "last_detection": current_time,
                    "is_jittery": is_jittery,
                    "flicker_protection": True
                })

                if best_match_id not in face_history:
                    face_history[best_match_id] = {"positions": [], "timestamps": []}
                history = face_history[best_match_id]
                history["positions"].append(smoothed_bbox)
                history["timestamps"].append(current_time)
                if len(history["positions"]) > 5:
                    history["positions"] = history["positions"][-5:]
                    history["timestamps"] = history["timestamps"][-5:]

                if "identity" in face_det:
                    new_identity = face_det["identity"].copy()
                    if "confidence" not in new_identity:
                        new_identity["confidence"] = face_det.get("confidence", 0.5)
                    stable_identity = self._stabilize_face_identity(
                        best_match_id, new_identity, face_history, IDENTITY_HISTORY_SIZE,
                        IDENTITY_CHANGE_THRESHOLD, IDENTITY_CONFIDENCE_DIFF, MIN_STABLE_FRAMES
                    )
                    face_det["identity"] = stable_identity
                    cache[best_match_id]["identity"] = stable_identity

                matched_ids.add(best_match_id)
                result_det = face_det.copy()
                result_det["bbox"] = smoothed_bbox
                result_det["tracking_id"] = best_match_id
                result_det["is_stable"] = cache[best_match_id]["stable_count"] >= FACE_STABLE_THRESHOLD
                result_det["consecutive_detections"] = cache[best_match_id]["consecutive_detections"]
                results.append(result_det)
            else:
                new_id = f"face_{int(current_time * 1000) % 10000}"
                cache[new_id] = {
                    "bbox": face_bbox, "confidence": face_det["confidence"], "type": "face",
                    "last_seen": current_time, "stable_count": 1, "consecutive_detections": 1,
                    "first_seen": current_time, "last_detection": current_time
                }
                face_history[new_id] = {"positions": [face_bbox], "timestamps": [current_time]}
                result_det = face_det.copy()
                result_det["tracking_id"] = new_id
                result_det["is_stable"] = False
                result_det["consecutive_detections"] = 1
                results.append(result_det)
                matched_ids.add(new_id)

        for obj_id, obj_data in list(cache.items()):
            if (obj_data["type"] == "face" and obj_id not in matched_ids and obj_data.get("consecutive_detections",
                                                                                          0) >= 3):
                time_since_last_seen = current_time - obj_data["last_seen"]
                if time_since_last_seen < FACE_KEEP_TIME:
                    predicted_bbox = self._predict_face_position(obj_id, face_history, current_time)
                    if predicted_bbox:
                        kept_det = {
                            "type": "face", "bbox": predicted_bbox,
                            "confidence": max(0.3,
                                              obj_data["confidence"] * (1 - time_since_last_seen / FACE_KEEP_TIME)),
                            "timestamp": datetime.now().isoformat(), "tracking_id": obj_id, "is_stable": True,
                            "is_kept": True, "known": False, "name": "æœªçŸ¥",
                            "time_since_detection": time_since_last_seen
                        }
                        results.append(kept_det)
        return results

    def _calculate_face_match_score(self, new_bbox: List[int], obj_data: Dict, current_time: float) -> float:
        old_bbox = obj_data["bbox"]
        old_center = ((old_bbox[0] + old_bbox[2]) / 2, (old_bbox[1] + old_bbox[3]) / 2)
        new_center = ((new_bbox[0] + new_bbox[2]) / 2, (new_bbox[1] + new_bbox[3]) / 2)
        center_distance = ((old_center[0] - new_center[0]) ** 2 + (old_center[1] - new_center[1]) ** 2) ** 0.5
        old_w, old_h = old_bbox[2] - old_bbox[0], old_bbox[3] - old_bbox[1]
        new_w, new_h = new_bbox[2] - new_bbox[0], new_bbox[3] - new_bbox[1]
        size_similarity = abs(old_w - new_w) + abs(old_h - new_h)
        time_weight = min(2.0, current_time - obj_data["last_seen"])
        consecutive_bonus = max(0, 5 - obj_data.get("consecutive_detections", 0))
        score = center_distance + size_similarity * 0.5 + time_weight * 10 + consecutive_bonus
        return score

    def _advanced_face_smoothing(self, new_bbox: List[int], old_obj: Dict, history: Dict, smooth_factor: float) -> List[
        int]:
        old_bbox = old_obj["bbox"]
        smoothed_bbox = [int(old_bbox[i] * (1 - smooth_factor) + new_bbox[i] * smooth_factor) for i in range(4)]
        if history and "positions" in history and len(history["positions"]) >= 2:
            positions = history["positions"]
            timestamps = history.get("timestamps", [])
            if len(positions) >= 3 and len(timestamps) >= 3:
                recent_positions = positions[-3:]
                recent_timestamps = timestamps[-3:]
                total_dx, total_dy = 0, 0
                valid_moves = 0
                for i in range(1, len(recent_positions)):
                    dt = recent_timestamps[i] - recent_timestamps[i - 1]
                    if dt > 0:
                        old_center = ((recent_positions[i - 1][0] + recent_positions[i - 1][2]) / 2,
                                      (recent_positions[i - 1][1] + recent_positions[i - 1][3]) / 2)
                        new_center = ((recent_positions[i][0] + recent_positions[i][2]) / 2,
                                      (recent_positions[i][1] + recent_positions[i][3]) / 2)
                        dx = (new_center[0] - old_center[0]) / dt
                        dy = (new_center[1] - old_center[1]) / dt
                        speed = (dx ** 2 + dy ** 2) ** 0.5
                        if speed < 200:
                            total_dx += dx
                            total_dy += dy
                            valid_moves += 1
                if valid_moves > 0:
                    avg_dx = total_dx / valid_moves
                    avg_dy = total_dy / valid_moves
                    motion_factor = 0.15 if abs(avg_dx) < 50 and abs(avg_dy) < 50 else 0.05
                    predict_offset_x = int(avg_dx * motion_factor)
                    predict_offset_y = int(avg_dy * motion_factor)
                    smoothed_bbox[0] += predict_offset_x
                    smoothed_bbox[1] += predict_offset_y
                    smoothed_bbox[2] += predict_offset_x
                    smoothed_bbox[3] += predict_offset_y
        w = smoothed_bbox[2] - smoothed_bbox[0]
        h = smoothed_bbox[3] - smoothed_bbox[1]
        if w <= 0 or h <= 0: return new_bbox
        old_center = ((old_bbox[0] + old_bbox[2]) / 2, (old_bbox[1] + old_bbox[3]) / 2)
        new_center = ((smoothed_bbox[0] + smoothed_bbox[2]) / 2, (smoothed_bbox[1] + smoothed_bbox[3]) / 2)
        center_change = ((old_center[0] - new_center[0]) ** 2 + (old_center[1] - new_center[1]) ** 2) ** 0.5
        if center_change > 25:
            limit_factor = 25 / center_change
            final_center_x = old_center[0] + (new_center[0] - old_center[0]) * limit_factor
            final_center_y = old_center[1] + (new_center[1] - old_center[1]) * limit_factor
            smoothed_bbox = [int(final_center_x - w / 2), int(final_center_y - h / 2), int(final_center_x + w / 2),
                             int(final_center_y + h / 2)]
        return smoothed_bbox

    def _predict_missing_faces(self, cache: Dict, face_history: Dict, current_time: float):
        for obj_id, obj_data in cache.items():
            if obj_data["type"] == "face":
                time_since_last_seen = current_time - obj_data["last_seen"]
                if 0.2 < time_since_last_seen < 1.0:
                    predicted_pos = self._predict_face_position(obj_id, face_history, current_time)
                    if predicted_pos:
                        obj_data["predicted_bbox"] = predicted_pos

    def _predict_face_position(self, obj_id: str, face_history: Dict, current_time: float) -> Optional[List[int]]:
        if obj_id not in face_history or len(face_history[obj_id]["positions"]) < 2: return None
        history = face_history[obj_id]
        positions = history["positions"]
        timestamps = history["timestamps"]
        if len(positions) >= 2:
            last_pos = positions[-1]
            prev_pos = positions[-2]
            last_time = timestamps[-1]
            prev_time = timestamps[-2]
            dt = last_time - prev_time
            if dt > 0:
                vx = (last_pos[0] - prev_pos[0]) / dt
                vy = (last_pos[1] - prev_pos[1]) / dt
                time_delta = current_time - last_time
                predicted_x = last_pos[0] + vx * time_delta
                predicted_y = last_pos[1] + vy * time_delta
                w = last_pos[2] - last_pos[0]
                h = last_pos[3] - last_pos[1]
                return [int(predicted_x), int(predicted_y), int(predicted_x + w), int(predicted_y + h)]
        return None

    def _standard_object_tracking(self, detections: List[Dict], cache: Dict, current_time: float, matched_ids: set,
                                  camera_id: str = "") -> List[Dict]:
        results = []
        config = self._stabilization_config.get(camera_id, {})
        OBJECT_MATCH_THRESHOLD = config.get('object_match_threshold', 80)
        OBJECT_SMOOTH_FACTOR = config.get('object_smooth_factor', 0.95)
        SIZE_CHANGE_RATIO = config.get('max_size_change_ratio', 0.1)
        CONFIDENCE_SMOOTH_FACTOR = 0.8

        for det in detections:
            bbox = det["bbox"]
            det_type = det["type"]
            best_match_id = None
            best_distance = OBJECT_MATCH_THRESHOLD

            for obj_id, obj_data in cache.items():
                if obj_data["type"] != det_type or obj_id in matched_ids: continue
                old_center = ((obj_data["bbox"][0] + obj_data["bbox"][2]) / 2,
                              (obj_data["bbox"][1] + obj_data["bbox"][3]) / 2)
                new_center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)
                distance = ((old_center[0] - new_center[0]) ** 2 + (old_center[1] - new_center[1]) ** 2) ** 0.5
                if distance < best_distance:
                    best_distance = distance
                    best_match_id = obj_id

            if best_match_id:
                old_obj = cache[best_match_id]
                old_bbox = old_obj["bbox"]
                old_center = ((old_bbox[0] + old_bbox[2]) / 2, (old_bbox[1] + old_bbox[3]) / 2)
                new_center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)
                movement = ((old_center[0] - new_center[0]) ** 2 + (old_center[1] - new_center[1]) ** 2) ** 0.5

                enhanced_smooth = min(0.95, OBJECT_SMOOTH_FACTOR + 0.08) if movement > 25 else OBJECT_SMOOTH_FACTOR
                smoothed_bbox = [int(old_bbox[i] * (1 - enhanced_smooth) + bbox[i] * enhanced_smooth) for i in range(4)]
                smoothed_bbox = self._stabilize_bbox_size(smoothed_bbox, old_bbox, max_change_ratio=SIZE_CHANGE_RATIO)

                old_confidence = old_obj.get("confidence", det["confidence"])
                smoothed_confidence = old_confidence * (1 - CONFIDENCE_SMOOTH_FACTOR) + det[
                    "confidence"] * CONFIDENCE_SMOOTH_FACTOR
                if smoothed_confidence < old_confidence * 0.6:
                    smoothed_confidence = old_confidence * 0.7

                cache[best_match_id].update({
                    "bbox": smoothed_bbox, "confidence": max(0.3, smoothed_confidence), "last_seen": current_time,
                    "stable_count": min(old_obj.get("stable_count", 0) + 1, 8),
                    "consecutive_detections": old_obj.get("consecutive_detections", 0) + 1,
                    "flicker_protection": True
                })
                matched_ids.add(best_match_id)
                result_det = det.copy()
                result_det["bbox"] = smoothed_bbox
                result_det["tracking_id"] = best_match_id
                result_det["is_stable"] = cache[best_match_id]["stable_count"] >= 2
                result_det["consecutive_detections"] = cache[best_match_id]["consecutive_detections"]
                results.append(result_det)
            else:
                new_id = f"{det_type}_{int(current_time * 1000) % 10000}"
                cache[new_id] = {
                    "bbox": bbox, "confidence": det["confidence"], "type": det_type,
                    "last_seen": current_time, "stable_count": 1, "consecutive_detections": 1
                }
                result_det = det.copy()
                result_det["tracking_id"] = new_id
                result_det["is_stable"] = False
                result_det["consecutive_detections"] = 1
                results.append(result_det)
                matched_ids.add(new_id)
        return results

    def _detect_jitter(self, new_bbox: List[int], old_bbox: List[int], history: Dict, threshold: int = 30) -> bool:
        old_center = ((old_bbox[0] + old_bbox[2]) / 2, (old_bbox[1] + old_bbox[3]) / 2)
        new_center = ((new_bbox[0] + new_bbox[2]) / 2, (new_bbox[1] + new_bbox[3]) / 2)
        movement = ((old_center[0] - new_center[0]) ** 2 + (old_center[1] - new_center[1]) ** 2) ** 0.5
        if movement > threshold: return True
        if history and "positions" in history and len(history["positions"]) >= 3:
            positions = history["positions"][-3:]
            centers = [((pos[0] + pos[2]) / 2, (pos[1] + pos[3]) / 2) for pos in positions]
            if len(centers) >= 2:
                movements = []
                for i in range(1, len(centers)):
                    move = ((centers[i][0] - centers[i - 1][0]) ** 2 + (centers[i][1] - centers[i - 1][1]) ** 2) ** 0.5
                    movements.append(move)
                if len(movements) >= 2:
                    movement_variance = sum((m - sum(movements) / len(movements)) ** 2 for m in movements) / len(
                        movements)
                    if movement_variance > 100: return True
        return False

    def _stabilize_bbox_size(self, new_bbox: List[int], old_bbox: List[int], max_change_ratio: float = 0.2) -> List[
        int]:
        old_w, old_h = old_bbox[2] - old_bbox[0], old_bbox[3] - old_bbox[1]
        new_w, new_h = new_bbox[2] - new_bbox[0], new_bbox[3] - new_bbox[1]
        w_change_ratio = abs(new_w - old_w) / old_w if old_w > 0 else 0
        h_change_ratio = abs(new_h - old_h) / old_h if old_h > 0 else 0
        if w_change_ratio > max_change_ratio or h_change_ratio > max_change_ratio:
            center_x, center_y = (new_bbox[0] + new_bbox[2]) / 2, (new_bbox[1] + new_bbox[3]) / 2
            max_w_change, max_h_change = old_w * max_change_ratio, old_h * max_change_ratio
            stabilized_w = old_w + max_w_change if new_w > old_w else old_w - max_w_change
            stabilized_h = old_h + max_h_change if new_h > old_h else old_h - max_h_change
            stabilized_bbox = [int(center_x - stabilized_w / 2), int(center_y - stabilized_h / 2),
                               int(center_x + stabilized_w / 2), int(center_y + stabilized_h / 2)]
            return stabilized_bbox
        return new_bbox

    def _process_face_recognition_with_stabilization(self, camera_id: str, frame: np.ndarray) -> List[Dict]:
        current_tolerance = self._face_recognition_config.get('tolerance', 0.65)
        recognized_faces = self._detectors["face"].detect_and_recognize(frame, tolerance=current_tolerance)

        face_detections = []
        for face in recognized_faces:
            location = face["location"]
            bbox = [location["left"], location["top"], location["right"], location["bottom"]]
            detection = {
                "type": "face", "bbox": bbox, "confidence": face.get("confidence", 0.5),
                "timestamp": datetime.now().isoformat(), "identity": face["identity"]
            }
            face_detections.append(detection)

        if face_detections:
            stabilized_faces = self.stabilize_detections(camera_id, face_detections)
            return stabilized_faces
        else:
            return []

    def _stabilize_face_identity(self, face_id: str, new_identity: Dict, face_history: Dict, history_size: int = 10,
                                 change_threshold: float = 0.7, confidence_diff: float = 0.15,
                                 min_stable_frames: int = 3) -> Dict:
        if face_id not in face_history:
            face_history[face_id] = {"identity_history": [], "current_identity": new_identity, "stable_count": 0,
                                     "last_change_time": time.time()}
            return new_identity

        history_data = face_history[face_id]
        if "positions" in history_data and "current_identity" not in history_data:  # Compatibility check
            history_data.update({"identity_history": [], "current_identity": new_identity, "stable_count": 0,
                                 "last_change_time": time.time()})
            return new_identity
        if "current_identity" not in history_data:
            history_data["current_identity"] = new_identity
            history_data["identity_history"] = []
            history_data["stable_count"] = 0
            history_data["last_change_time"] = time.time()
            return new_identity

        current_identity = history_data["current_identity"]
        if "identity_history" not in history_data: history_data["identity_history"] = []
        identity_history = history_data["identity_history"]
        if "stable_count" not in history_data: history_data["stable_count"] = 0

        identity_history.append(
            {"name": new_identity.get("name", "unknown"), "confidence": new_identity.get("confidence", 0),
             "timestamp": time.time()})
        if len(identity_history) > history_size: identity_history.pop(0)

        name_votes = {};
        total_weight = 0
        for record in identity_history:
            name = record["name"];
            confidence = record["confidence"]
            weight = max(0.1, confidence)
            name_votes[name] = name_votes.get(name, 0) + weight
            total_weight += weight

        winning_name = "unknown";
        vote_ratio = 0
        if name_votes and total_weight > 0:
            most_voted_name = max(name_votes.items(), key=lambda x: x[1])
            vote_ratio = most_voted_name[1] / total_weight
            winning_name = most_voted_name[0]

        current_name = current_identity.get("name", "unknown")
        new_name = new_identity.get("name", "unknown")

        should_change_identity = False;
        change_reason = ""
        if current_name == winning_name:
            history_data["stable_count"] = history_data.get("stable_count", 0) + 1
            should_change_identity = False
            change_reason = "èº«ä»½ä¸€è‡´ï¼Œä¿æŒç¨³å®š"
        elif vote_ratio >= change_threshold:
            current_confidence = current_identity.get("confidence", 0)
            new_confidence = new_identity.get("confidence", 0)
            if new_confidence > current_confidence + confidence_diff:
                should_change_identity = True
                change_reason = f"æŠ•ç¥¨æ”¯æŒç‡{vote_ratio:.1%}ï¼Œç½®ä¿¡åº¦æå‡{new_confidence - current_confidence:.2f}"
            elif history_data.get("stable_count", 0) >= min_stable_frames:
                should_change_identity = True
                change_reason = f"æŠ•ç¥¨æ”¯æŒç‡{vote_ratio:.1%}ï¼Œå·²ç¨³å®š{history_data['stable_count']}å¸§"
            else:
                should_change_identity = False
                change_reason = f"æŠ•ç¥¨æ”¯æŒç‡{vote_ratio:.1%}ï¼Œä½†ç¨³å®šå¸§æ•°ä¸è¶³({history_data.get('stable_count', 0)}<{min_stable_frames})"
        else:
            should_change_identity = False
            change_reason = f"æŠ•ç¥¨æ”¯æŒç‡ä¸è¶³({vote_ratio:.1%}<{change_threshold:.1%})"

        if should_change_identity:
            best_confidence = 0
            for record in identity_history:
                if record["name"] == winning_name: best_confidence = max(best_confidence, record["confidence"])
            history_data["current_identity"] = {"name": winning_name, "known": winning_name != "unknown",
                                                "confidence": best_confidence}
            history_data["stable_count"] = 0
            history_data["last_change_time"] = time.time()
        else:
            if new_name == current_name and new_identity.get("confidence", 0) > current_identity.get("confidence", 0):
                history_data["current_identity"]["confidence"] = new_identity["confidence"]
        return history_data["current_identity"]

    def _cleanup_expired_cache(self, cache: Dict, face_history: Dict, current_time: float):
        to_remove = []
        for obj_id, obj_data in cache.items():
            time_since_seen = current_time - obj_data["last_seen"]
            if obj_data["type"] == "face":
                if time_since_seen > 6.0:
                    to_remove.append(obj_id)
                elif time_since_seen > 3.0:
                    current_confidence = obj_data.get("confidence", 1.0)
                    fade_factor = max(0.15, 1 - (time_since_seen - 3.0) / 3.0)
                    obj_data["confidence"] = max(0.15, current_confidence * fade_factor)
                    obj_data["fading"] = True
            else:
                if time_since_seen > 3.0:
                    to_remove.append(obj_id)
                elif time_since_seen > 1.5:
                    current_confidence = obj_data.get("confidence", 1.0)
                    fade_factor = max(0.2, 1 - (time_since_seen - 1.5) / 1.5)
                    obj_data["confidence"] = max(0.2, current_confidence * fade_factor)
                    obj_data["fading"] = True
        for obj_id in to_remove:
            del cache[obj_id]
            if obj_id in face_history: del face_history[obj_id]
        if len(cache) > 25:
            sorted_items = sorted(cache.items(), key=lambda x: x[1]["last_seen"])
            for obj_id, _ in sorted_items[:-20]:
                del cache[obj_id]
                if obj_id in face_history: del face_history[obj_id]

    # --- æ€§èƒ½ä¼˜åŒ–ç­–ç•¥å‡½æ•° ---
    def _get_performance_strategy(self, performance_mode: str, frame_count: int, is_low_res: bool) -> Dict:
        if performance_mode == "fast":
            is_object_frame = frame_count % 2 == 0
            return {
                "run_object_detection": is_object_frame, "run_face_recognition": not is_object_frame,
                "object_scale_factor": 0.4, "face_scale_factor": 0.5, "use_parallel": False,
                "use_stabilization": frame_count % 4 == 0, "max_detections": 3, "face_limit": 2
            }
        elif performance_mode == "balanced":
            use_parallel_this_frame = frame_count % 3 == 0
            return {
                "run_object_detection": True, "run_face_recognition": True,
                "object_scale_factor": 0.6 if use_parallel_this_frame else 1.0,
                "face_scale_factor": 0.7 if use_parallel_this_frame else 1.1,
                "use_parallel": use_parallel_this_frame, "use_stabilization": True,
                "max_detections": 5, "face_limit": 3
            }
        else:  # quality
            return {
                "run_object_detection": True, "run_face_recognition": True,
                "object_scale_factor": 0.8, "face_scale_factor": 0.9, "use_parallel": True,
                "use_stabilization": True, "max_detections": 8, "face_limit": 3
            }

    # --- æ ¸å¿ƒå¸§åˆ†æé€»è¾‘ (æ•´åˆåˆ°AIServiceManager) ---
    async def process_single_frame(self, frame: np.ndarray, camera_id: str,
                                   enable_face_recognition: bool, enable_object_detection: bool,
                                   enable_behavior_detection: bool, enable_fire_detection: bool,
                                   performance_mode: str) -> Dict:
        start_time = time.time()
        # ç®€å•å¸§è®¡æ•°ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥æ›´ç²¾ç¡®åœ°ç®¡ç†
        frame_count = self._detection_cache.get(camera_id, {}).get("frame_count", 0) + 1
        if camera_id not in self._detection_cache:
            self._detection_cache[camera_id] = {"objects": {}, "face_history": {}, "frame_count": 0}
        self._detection_cache[camera_id]["frame_count"] = frame_count

        height, width = frame.shape[:2]
        is_low_res = width < 400 or height < 400

        if camera_id not in self._object_trackers:
            self._object_trackers[camera_id] = DeepSORTTracker()

        results = {
            "camera_id": camera_id, "timestamp": datetime.now().isoformat(), "detections": [], "alerts": [],
            "performance_info": {"mode": performance_mode, "frame_count": frame_count}
        }

        # åº”ç”¨AIè®¾ç½® (ä»å­˜å‚¨çš„è®¾ç½®ä¸­è·å–)
        camera_settings = self._ai_settings.get(camera_id)
        if camera_settings:
            enable_face_recognition = camera_settings.get('face_recognition', enable_face_recognition)
            enable_object_detection = camera_settings.get('object_detection', enable_object_detection)
            enable_behavior_detection = camera_settings.get('behavior_analysis', enable_behavior_detection)
            enable_fire_detection = camera_settings.get('fire_detection', enable_fire_detection)
            if camera_settings.get('realtime_mode', False):
                performance_mode = "fast"

        strategy = self._get_performance_strategy(performance_mode, frame_count, is_low_res)

        active_tasks = []

        # ç«ç„°æ£€æµ‹
        if enable_fire_detection and "fire" in self._detectors:
            try:
                fire_detector = self._detectors["fire"]
                fire_results = fire_detector.detect(frame, confidence_threshold=0.25)
                for fire_obj in fire_results:
                    bbox = [int(float(coord)) for coord in fire_obj["coordinates"]]
                    detection = {
                        "type": "fire_detection", "class_name": fire_obj["class_name"],
                        "detection_type": fire_obj["type"], "confidence": float(fire_obj["confidence"]),
                        "bbox": bbox, "timestamp": datetime.now().isoformat()
                    }
                    results["detections"].append(detection)
                    self.send_alert_to_backend(
                        AIAnalysisResult(
                            camera_id=camera_id, event_type=f"fire_detection_{fire_obj['type']}",
                            location={"box": bbox}, confidence=float(fire_obj["confidence"]),
                            timestamp=datetime.now().isoformat(),
                            details={"detection_type": fire_obj["type"], "object_type": fire_obj["class_name"],
                                     "area": fire_obj["area"], "center": fire_obj["center"]}
                        )
                    )
            except Exception as e:
                print(f"ç«ç„°æ£€æµ‹å¤±è´¥: {e}")
                traceback.print_exc()

        # ç›®æ ‡æ£€æµ‹ä»»åŠ¡
        def optimized_object_detection():
            if not (enable_object_detection and strategy["run_object_detection"]): return []
            try:
                obj_scale = strategy["object_scale_factor"]
                obj_height, obj_width = int(height * obj_scale), int(width * obj_scale)
                obj_image = cv2.resize(frame, (obj_width, obj_height))
                confidence_threshold = self._object_detection_config.get('confidence_threshold', 0.35)
                detected_objects = self._detectors["object"].predict(obj_image,
                                                                     confidence_threshold=confidence_threshold)

                scale_back_x, scale_back_y = width / obj_width, height / obj_height
                object_detections = []
                for obj in detected_objects[:strategy["max_detections"]]:
                    bbox = [int(float(obj["coordinates"][0]) * scale_back_x),
                            int(float(obj["coordinates"][1]) * scale_back_y),
                            int(float(obj["coordinates"][2]) * scale_back_x),
                            int(float(obj["coordinates"][3]) * scale_back_y)]
                    object_detections.append({
                        "type": "object", "class_name": obj["class_name"], "confidence": float(obj["confidence"]),
                        "bbox": bbox, "timestamp": datetime.now().isoformat()
                    })
                return object_detections
            except Exception as e:
                print(f"ç›®æ ‡æ£€æµ‹å¤±è´¥: {e}")
                return []

        # äººè„¸è¯†åˆ«ä»»åŠ¡
        def optimized_face_recognition():
            if not (enable_face_recognition and strategy["run_face_recognition"]): return []
            try:
                face_scale = strategy["face_scale_factor"]
                if performance_mode == "fast": face_scale = min(0.5, face_scale)
                face_height, face_width = int(height * face_scale), int(width * face_scale)
                face_image = cv2.resize(frame, (face_width, face_height))

                stabilized_faces = self._process_face_recognition_with_stabilization(camera_id, face_image)

                scale_back_x, scale_back_y = width / face_width, height / face_height
                face_detections = []
                for face in stabilized_faces[:strategy["face_limit"]]:
                    if face["type"] == "face":
                        face_bbox = [int(float(face["bbox"][0]) * scale_back_x),
                                     int(float(face["bbox"][1]) * scale_back_y),
                                     int(float(face["bbox"][2]) * scale_back_x),
                                     int(float(face["bbox"][3]) * scale_back_y)]
                        face_detections.append({
                            "type": "face", "known": face.get("identity", {}).get("known", False),
                            "name": face.get("identity", {}).get("name", "æœªçŸ¥"),
                            "confidence": float(face.get("confidence", 0.5)),
                            "bbox": face_bbox, "tracking_id": face.get("tracking_id", ""),
                            "timestamp": face.get("timestamp", datetime.now().isoformat())
                        })
                        if not face.get("identity", {}).get("known", False) and performance_mode != "fast":
                            self.send_alert_to_backend(
                                AIAnalysisResult(
                                    camera_id=camera_id, event_type="unknown_face_detected",
                                    location={"box": face_bbox}, confidence=float(face.get("confidence", 0.5)),
                                    timestamp=datetime.now().isoformat(),
                                    details={"tracking_id": face.get("tracking_id"),
                                             "is_stable": face.get("is_stable", False)}
                                )
                            )
                return face_detections
            except Exception as e:
                print(f"äººè„¸è¯†åˆ«å¤±è´¥: {e}")
                return []

        # æ‰§è¡Œç­–ç•¥
        if strategy["use_parallel"]:
            with ThreadPoolExecutor(max_workers=2) as executor:
                future_objects = executor.submit(optimized_object_detection)
                future_faces = executor.submit(optimized_face_recognition)
                object_results = future_objects.result()
                face_results = future_faces.result()
        else:
            object_results = optimized_object_detection()
            face_results = optimized_face_recognition()

        # Deep SORTè¿½è¸ª
        tracked_object_results = []
        if object_results and camera_id in self._object_trackers:
            try:
                tracker = self._object_trackers[camera_id]
                detection_list = []
                for obj in object_results:
                    if obj["type"] == "object":
                        detection_list.append({
                            'coordinates': obj['bbox'], 'confidence': obj['confidence'],
                            'class_name': obj['class_name'], 'class_id': obj.get('class_id', 0)
                        })
                if detection_list:
                    tracked_results = tracker.update(detection_list, frame)
                    for tracked_obj in tracked_results:
                        tracked_object_results.append({
                            "type": "object", "class_name": tracked_obj["class_name"],
                            "confidence": tracked_obj["confidence"],
                            "bbox": tracked_obj["coordinates"], "tracking_id": tracked_obj["tracking_id"],
                            "timestamp": datetime.now().isoformat()
                        })
                        if tracked_obj["class_name"] == "person" and tracked_obj["confidence"] > 0.5:
                            self.send_alert_to_backend(
                                AIAnalysisResult(
                                    camera_id=camera_id, event_type="object_person_detected",
                                    location={"box": tracked_obj["coordinates"]}, confidence=tracked_obj["confidence"],
                                    timestamp=datetime.now().isoformat(),
                                    details={"tracking_id": tracked_obj["tracking_id"],
                                             "class_name": tracked_obj["class_name"],
                                             "tracking_method": "deep_sort", "realtime_detection": True}
                                )
                            )
            except Exception as e:
                print(f"Deep SORTè¿½è¸ªå¤±è´¥: {e}")
                tracked_object_results = object_results
        else:
            tracked_object_results = object_results

        all_detections = tracked_object_results + face_results  # Face results are already stabilized
        results["detections"] = all_detections

        # å±é™©åŒºåŸŸæ£€æµ‹
        try:
            person_detections = [d for d in all_detections if
                                 d.get('type') == 'object' and d.get('class_name') == 'person']
            if person_detections:
                danger_zone_alerts = danger_zone_detector.detect_intrusions(camera_id, all_detections)
                for alert in danger_zone_alerts:
                    results["alerts"].append({
                        "type": alert["type"], "message": alert["message"], "tracking_id": alert.get("tracking_id"),
                        "zone_name": alert.get("zone_name"), "position": alert.get("position"),
                        "distance": alert.get("distance"), "dwell_time": alert.get("dwell_time")
                    })
                    self.send_alert_to_backend(
                        AIAnalysisResult(
                            camera_id=camera_id, event_type=alert["type"],
                            location={"position": alert.get("position", [])},
                            confidence=1.0, timestamp=datetime.now().isoformat(),
                            details={"tracking_id": alert.get("tracking_id"), "zone_id": alert.get("zone_id"),
                                     "zone_name": alert.get("zone_name"), "distance": alert.get("distance"),
                                     "dwell_time": alert.get("dwell_time"), "detection_method": "danger_zone_geometric",
                                     "realtime_detection": True}
                        )
                    )
        except Exception as e:
            print(f"âš ï¸ å±é™©åŒºåŸŸæ£€æµ‹å¤±è´¥: {e}")

        total_time = (time.time() - start_time) * 1000
        results["performance_info"]["processing_time_ms"] = round(total_time, 1)
        results["performance_info"]["detection_count"] = len(results["detections"])
        results["performance_info"]["tracking_info"] = {
            "deep_sort_objects": len(tracked_object_results),
            "stabilized_faces": len(face_results),
            "tracker_available": camera_id in self._object_trackers
        }

        # å¼‚æ­¥å‘é€æ£€æµ‹ç»“æœåˆ°WebSocket (ä¸é˜»å¡å“åº”)
        if results.get("detections"):
            self._thread_pool.submit(lambda: asyncio.run(self.send_detection_to_websocket(camera_id, results)))

        return results

    # --- AISettings Model (for API) ---
    class AISettings(BaseModel):
        model_config = ConfigDict(extra='allow')  # å…è®¸é¢å¤–å­—æ®µ
        face_recognition: bool = True
        object_detection: bool = True
        behavior_analysis: bool = False
        sound_detection: bool = False
        fire_detection: bool = True
        realtime_mode: bool = True

    def get_ai_settings(self, camera_id: str) -> Dict:
        if camera_id not in self._ai_settings:
            self._ai_settings[camera_id] = self.AISettings().model_dump()
        return self._ai_settings[camera_id]

    def update_ai_settings(self, camera_id: str, settings: Dict):
        current_settings = self.get_ai_settings(camera_id)
        current_settings.update(settings)
        self._ai_settings[camera_id] = current_settings
        if camera_id in self._video_streams:
            self._video_streams[camera_id].update_settings(settings)

    # --- Face Recognition Sensitivity Management ---
    def get_face_recognition_config(self) -> Dict:
        return self._face_recognition_config

    def update_face_recognition_config(self, config_data: Dict):
        self._face_recognition_config.update(config_data)
        # Clear face cache for immediate effect
        for cam_id in list(self._detection_cache.keys()):
            if cam_id in self._detection_cache and "face_history" in self._detection_cache[cam_id]:
                self._detection_cache[cam_id]["face_history"].clear()
                self._detection_cache[cam_id]["objects"] = {k: v for k, v in
                                                            self._detection_cache[cam_id]["objects"].items() if
                                                            v.get("type") != "face"}

    # --- Object Detection Sensitivity Management ---
    def get_object_detection_config(self) -> Dict:
        return self._object_detection_config

    def update_object_detection_config(self, config_data: Dict):
        self._object_detection_config.update(config_data)
        # Clear object cache for immediate effect (except faces)
        for cam_id in list(self._detection_cache.keys()):
            if cam_id in self._detection_cache and "objects" in self._detection_cache[cam_id]:
                object_cache = self._detection_cache[cam_id]["objects"]
                self._detection_cache[cam_id]["objects"] = {k: v for k, v in object_cache.items() if
                                                            v.get("type") == "face"}

    # --- Stabilization Configuration ---
    def get_stabilization_config(self, camera_id: str) -> Dict:
        default_config = {
            "face_smooth_factor": 0.92, "object_smooth_factor": 0.88,
            "face_match_threshold": 120, "object_match_threshold": 60,
            "jitter_detection_threshold": 30, "max_size_change_ratio": 0.2,
            "updated_at": "é»˜è®¤é…ç½®"
        }
        return self._stabilization_config.get(camera_id, default_config)

    def update_stabilization_config(self, camera_id: str, config_data: Dict):
        if not camera_id in self._stabilization_config:
            self._stabilization_config[camera_id] = self.get_stabilization_config(camera_id).copy()
        self._stabilization_config[camera_id].update(config_data)
        self._stabilization_config[camera_id]["updated_at"] = datetime.now().isoformat()
        # Clear cache for immediate effect
        if camera_id in self._detection_cache:
            self._detection_cache[camera_id] = {"objects": {}, "face_history": {}, "frame_count": 0}

    def apply_stabilization_preset(self, preset_name: str, camera_id: str):
        presets = {
            "anti_flicker": {"name": "ğŸš¨ è¶…å¼ºé˜²é—ªçƒæ¨¡å¼",
                             "config": {"face_smooth_factor": 0.97, "object_smooth_factor": 0.95,
                                        "face_match_threshold": 150, "object_match_threshold": 80,
                                        "jitter_detection_threshold": 15, "max_size_change_ratio": 0.1}},
            "anti_jitter": {"name": "æŠ—æŠ–åŠ¨æ¨¡å¼", "config": {"face_smooth_factor": 0.95, "object_smooth_factor": 0.92,
                                                             "face_match_threshold": 100, "object_match_threshold": 50,
                                                             "jitter_detection_threshold": 20,
                                                             "max_size_change_ratio": 0.15}},
            "ultra_stable": {"name": "è¶…ç¨³å®šæ¨¡å¼", "config": {"face_smooth_factor": 0.97, "object_smooth_factor": 0.95,
                                                              "face_match_threshold": 80, "object_match_threshold": 40,
                                                              "jitter_detection_threshold": 15,
                                                              "max_size_change_ratio": 0.1}},
            "balanced": {"name": "å¹³è¡¡æ¨¡å¼", "config": {"face_smooth_factor": 0.88, "object_smooth_factor": 0.85,
                                                        "face_match_threshold": 120, "object_match_threshold": 60,
                                                        "jitter_detection_threshold": 30,
                                                        "max_size_change_ratio": 0.2}},
            "responsive": {"name": "å“åº”æ¨¡å¼", "config": {"face_smooth_factor": 0.80, "object_smooth_factor": 0.75,
                                                          "face_match_threshold": 150, "object_match_threshold": 80,
                                                          "jitter_detection_threshold": 40,
                                                          "max_size_change_ratio": 0.25}},
            "default": {"name": "é»˜è®¤æ¨¡å¼", "config": {"face_smooth_factor": 0.92, "object_smooth_factor": 0.88,
                                                       "face_match_threshold": 120, "object_match_threshold": 60,
                                                       "jitter_detection_threshold": 30, "max_size_change_ratio": 0.2}}
        }
        if preset_name not in presets: raise HTTPException(status_code=404, detail=f"é¢„è®¾ '{preset_name}' ä¸å­˜åœ¨")

        config = presets[preset_name]["config"].copy()
        self.update_stabilization_config(camera_id, config)
        return {"status": "success", "message": f"å·²åº”ç”¨ '{presets[preset_name]['name']}' é…ç½®åˆ°æ‘„åƒå¤´ {camera_id}",
                "preset": presets[preset_name], "applied_config": config}

    def list_stabilization_presets(self) -> Dict:
        return {
            "anti_flicker": {"name": "ğŸš¨ è¶…å¼ºé˜²é—ªçƒæ¨¡å¼", "description": "ä¸“é—¨è§£å†³æ¡†ä¸€é—ªä¸€é—ªé—®é¢˜çš„è¶…å¼ºé…ç½®",
                             "best_for": ["æ£€æµ‹æ¡†é—ªçƒ", "å‡ºç°æ¶ˆå¤±é¢‘ç¹", "ç½®ä¿¡åº¦ä¸ç¨³å®š"],
                             "trade_offs": "æœ€å¼ºç¨³å®šæ€§ï¼Œè½»å¾®å»¶è¿Ÿå¢åŠ "},
            "anti_jitter": {"name": "æŠ—æŠ–åŠ¨æ¨¡å¼", "description": "é’ˆå¯¹ä¸¥é‡æŠ–åŠ¨é—®é¢˜çš„å¼ºåŒ–é…ç½®",
                            "best_for": ["æŠ–åŠ¨ä¸¥é‡", "ç›®æ ‡é¢‘ç¹åˆ‡æ¢", "ä½è´¨é‡æ‘„åƒå¤´"],
                            "trade_offs": "å¯èƒ½å¢åŠ 10-20mså»¶è¿Ÿ"},
            "ultra_stable": {"name": "è¶…ç¨³å®šæ¨¡å¼", "description": "æœ€å¼ºç¨³å®šåŒ–ï¼Œé€‚åˆé™æ€åœºæ™¯ç›‘æ§",
                             "best_for": ["é™æ€ç›‘æ§", "é«˜ç²¾åº¦è¦æ±‚", "ç¨³å®šç¯å¢ƒ"], "trade_offs": "å“åº”ç¨æ…¢ï¼Œé€‚åˆé™æ€åœºæ™¯"},
            "balanced": {"name": "å¹³è¡¡æ¨¡å¼", "description": "å…¼é¡¾ç¨³å®šæ€§å’Œå“åº”æ€§",
                         "best_for": ["å¤§å¤šæ•°åœºæ™¯", "åŠ¨é™ç»“åˆ", "ä¸€èˆ¬ç›‘æ§"], "trade_offs": "ç»¼åˆè¡¨ç°è‰¯å¥½"},
            "responsive": {"name": "å“åº”æ¨¡å¼", "description": "ä¼˜å…ˆå“åº”é€Ÿåº¦ï¼Œè½»åº¦ç¨³å®šåŒ–",
                           "best_for": ["å¿«é€Ÿç§»åŠ¨", "å®æ—¶äº’åŠ¨", "ä½å»¶è¿Ÿè¦æ±‚"], "trade_offs": "å¯èƒ½æœ‰è½»å¾®æŠ–åŠ¨"},
            "default": {"name": "é»˜è®¤æ¨¡å¼", "description": "ç³»ç»Ÿé»˜è®¤è®¾ç½®", "best_for": ["åˆå§‹é…ç½®", "æ ‡å‡†ç¯å¢ƒ"],
                        "trade_offs": "å¯æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´"}
        }

    async def apply_anti_flicker_all_cameras(self) -> Dict:
        applied_cameras = []
        active_cameras = list(self._video_streams.keys())
        if not active_cameras: active_cameras = ["default"]  # If no active streams, apply to a default config
        for camera_id in active_cameras:
            self.apply_stabilization_preset("anti_flicker", camera_id)
            applied_cameras.append(camera_id)
        return {"status": "success", "message": "ğŸš¨ å·²å¯¹æ‰€æœ‰æ‘„åƒå¤´åº”ç”¨è¶…å¼ºé˜²é—ªçƒé…ç½®ï¼",
                "applied_cameras": applied_cameras}

    # --- System Status and Performance ---
    def get_system_status(self) -> SystemStatus:
        return SystemStatus(
            active_streams_count=len(self._video_streams),
            detectors_initialized={name: det is not None for name, det in self._detectors.items()},
            system_load=None,  # Placeholder for actual system load
            memory_usage=None  # Placeholder for actual memory usage
        )

    def get_performance_stats(self) -> Dict:
        stats = {"cache_info": {}, "detector_status": {}, "optimization_status": "high_performance_mode"}
        for camera_id, cache_data in self._detection_cache.items():
            if isinstance(cache_data, dict) and "objects" in cache_data:
                stats["cache_info"][camera_id] = {"cached_objects": len(cache_data["objects"]), "types": {}}
                for obj_data in cache_data["objects"].values():
                    obj_type = obj_data.get("type", "unknown")
                    stats["cache_info"][camera_id]["types"][obj_type] = stats["cache_info"][camera_id]["types"].get(
                        obj_type, 0) + 1
        stats["detector_status"] = {
            "object_detection": "parallel_enabled" if "object" in self._detectors else "disabled",
            "face_recognition": "adaptive_scaling_enabled" if "face" in self._detectors else "disabled",
            "behavior_detection": "enabled" if "behavior" in self._detectors else "disabled",
            "acoustic_detection": "enabled" if "acoustic" in self._detectors else "disabled"
        }
        stats["performance_features"] = {
            "parallel_processing": True, "adaptive_image_scaling": {"object": "60%", "face": "60-85%"},
            "face_specific_stabilization": True, "size_aware_matching": True, "memory_leak_protection": True,
            "async_backend_communication": True, "smart_caching": True
        }
        return {"status": "success", "stats": stats}

    # --- Danger Zone Management ---
    def update_danger_zones(self, camera_id: str, zones_data: List[Dict]):
        danger_zone_detector.update_camera_zones(camera_id, zones_data)

    def get_danger_zone_status(self, camera_id: str) -> Dict:
        return danger_zone_detector.get_zone_status(camera_id)

    def remove_danger_zone(self, camera_id: str, zone_id: str):
        danger_zone_detector.remove_danger_zone(camera_id, zone_id)


# åˆ›å»ºæœåŠ¡ç®¡ç†å™¨å®ä¾‹
service_manager = AIServiceManager(app_config)


# --- FastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨ã€‚"""
    await service_manager.initialize_detectors()
    yield  # æœåŠ¡åœ¨æ­¤è¿è¡Œæ—¶ï¼Œå¤„ç†APIè¯·æ±‚
    await service_manager.shutdown_services()


# åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹
app = FastAPI(
    title="SmartEye AI Service",
    description="æä¾›è§†é¢‘æµå¤„ç†ã€ç›®æ ‡æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«ã€äººè„¸è¯†åˆ«å’Œå£°å­¦äº‹ä»¶æ£€æµ‹èƒ½åŠ›",
    version="2.0.0",
    lifespan=lifespan
)

# å¢åŠ è¯·æ±‚è¶…æ—¶è®¾ç½®ä¸­é—´ä»¶
from starlette.middleware.base import BaseHTTPMiddleware
from fastapi.responses import JSONResponse


class TimeoutMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        try:
            return await asyncio.wait_for(call_next(request), timeout=app_config.FASTAPI_TIMEOUT_SECONDS)
        except asyncio.TimeoutError:
            return JSONResponse(status_code=504, content={"detail": "è¯·æ±‚å¤„ç†è¶…æ—¶ï¼Œè¯·é‡è¯•"})


app.add_middleware(TimeoutMiddleware)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- API ç«¯ç‚¹å®šä¹‰ ---

# RTMPæ¨æµå›è°ƒå¤„ç†
@app.post("/rtmp/publish")
async def rtmp_publish_callback(request: dict = Body(...)):
    stream_name = request.get('name', '')
    camera_id = f"rtmp_stream_{stream_name}"
    print(f"ğŸ“¡ RTMPæ¨æµå¼€å§‹: stream={stream_name}, camera_id={camera_id}")
    return {"status": "success", "message": "æ¨æµå·²æ¥å—", "camera_id": camera_id}


@app.post("/rtmp/publish_done")
async def rtmp_publish_done_callback(request: dict = Body(...)):
    stream_name = request.get('name', '')
    camera_id = f"rtmp_stream_{stream_name}"
    print(f"ğŸ“¡ RTMPæ¨æµç»“æŸ: stream={stream_name}")
    if camera_id in service_manager._video_streams:
        await service_manager._video_streams[camera_id].stop()
        del service_manager._video_streams[camera_id]
        if camera_id in service_manager._object_trackers:
            del service_manager._object_trackers[camera_id]
        if camera_id in service_manager._detection_cache:
            del service_manager._detection_cache[camera_id]
        print(f"æ¸…ç†æ‘„åƒå¤´ {camera_id} èµ„æº")
    return {"status": "success", "message": "æ¨æµç»“æŸå¤„ç†å®Œæˆ"}


# AIåˆ†æè®¾ç½®
@app.post("/frame/analyze/settings/{camera_id}")
async def update_ai_settings(camera_id: str, settings: service_manager.AISettings = Body(...)):
    service_manager.update_ai_settings(camera_id, settings.model_dump())
    return {"status": "success", "message": "AIåˆ†æè®¾ç½®å·²æ›´æ–°", "settings": service_manager.get_ai_settings(camera_id)}


@app.get("/frame/analyze/settings/{camera_id}")
async def get_ai_settings(camera_id: str):
    return service_manager.get_ai_settings(camera_id)


# å¯åŠ¨/åœæ­¢è§†é¢‘æµ
@app.post("/stream/start/")
async def start_stream(config: CameraConfig):  # Using CameraConfig from models/alert_models.py
    if config.camera_id in service_manager._video_streams:
        raise HTTPException(status_code=400, detail=f"æ‘„åƒå¤´ {config.camera_id} å·²åœ¨è¿è¡Œ")

    try:
        print(f"æ­£åœ¨å¯åŠ¨è§†é¢‘æµ: {config.stream_url}")
        stream = VideoStream(stream_url=config.stream_url, camera_id=config.camera_id,
                             predictor=service_manager._detectors.get("object"),
                             face_recognizer=service_manager._detectors.get("face"),
                             fire_detector=service_manager._detectors.get("fire"))

        is_available = await stream.test_connection()
        if not is_available:
            raise HTTPException(status_code=400, detail="æ— æ³•è¿æ¥åˆ°è§†é¢‘æµï¼Œè¯·æ£€æŸ¥æµåœ°å€æ˜¯å¦æ­£ç¡®")

        # åŠ¨æ€åˆå§‹åŒ– Deep SORT è¿½è¸ªå™¨
        if config.camera_id not in service_manager._object_trackers:
            service_manager._object_trackers[config.camera_id] = DeepSORTTracker()
            print(f"ä¸ºæ‘„åƒå¤´ {config.camera_id} åˆå§‹åŒ–Deep SORTè¿½è¸ªå™¨")

        # å¯åŠ¨éŸ³é¢‘æå– (å¦‚æœå¯ç”¨å£°å­¦æ£€æµ‹)
        if config.enable_sound_detection and service_manager.config.ENABLE_SOUND_DETECTION:
            await stream.start_audio_extraction()
            # å¯åŠ¨å£°å­¦åˆ†æåå°ä»»åŠ¡ (æ¯ä¸ªæµä¸€ä¸ªä»»åŠ¡)
            asyncio.create_task(service_manager._run_acoustic_analysis(config.camera_id))

        if not await stream.start():
            raise HTTPException(status_code=500, detail="æ— æ³•å¯åŠ¨è§†é¢‘æµå¤„ç†")

        # å°†æµå®ä¾‹æ·»åŠ åˆ°ç®¡ç†å™¨
        service_manager._video_streams[config.camera_id] = stream

        # å¯åŠ¨å¼‚æ­¥å¸§å¤„ç†ä»»åŠ¡
        asyncio.create_task(process_video_stream_async_loop(stream, config.camera_id))

        return {
            "status": "success", 
            "message": f"æˆåŠŸå¯åŠ¨æ‘„åƒå¤´ {config.camera_id} çš„è§†é¢‘æµå¤„ç†",
            "success": True  # ä¸ºäº†å…¼å®¹å‰ç«¯ä»£ç 
        }

    except Exception as e:
        print(f"å¯åŠ¨è§†é¢‘æµæ—¶å‡ºé”™: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨è§†é¢‘æµæ—¶å‡ºé”™: {str(e)}")


async def process_video_stream_async_loop(stream: VideoStream, camera_id: str):
    """å¼‚æ­¥å¤„ç†è§†é¢‘æµå¾ªç¯ï¼Œä»VideoStreamè·å–å¸§å¹¶é€å…¥åˆ†æå™¨"""
    print(f"å¼€å§‹å¼‚æ­¥å¤„ç†è§†é¢‘æµå¾ªç¯: {camera_id}")
    frame_process_counter = 0
    while stream.is_running:
        try:
            # è¿™é‡Œç›´æ¥ä» VideoStream è·å–å¸§ï¼ŒVideoStream å†…éƒ¨è´Ÿè´£è¯»å–
            success, frame = stream.get_raw_frame()  # VideoStream éœ€è¦æš´éœ²è¿™ä¸ªæ–¹æ³•
            if not success or frame is None:
                await asyncio.sleep(0.01)  # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…ç©ºè½¬
                continue

            # ä½¿ç”¨ AIServiceManager çš„æ ¸å¿ƒåˆ†æé€»è¾‘
            # æ³¨æ„ï¼šè¿™é‡Œæ˜¯å•å¸§ä¸Šä¼ APIçš„é€»è¾‘ï¼Œå¦‚æœæ˜¯åœ¨è¿ç»­è§†é¢‘æµä¸­ï¼Œ
            # é€»è¾‘ä¼šç•¥æœ‰ä¸åŒï¼Œéœ€è¦VideoStreamçš„process_frameè¿›è¡Œå¤„ç†å¹¶è¿”å›ç»“æœ
            # ä¸ºäº†ä¸ç°æœ‰ä»£ç ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å‡è®¾VideoStreamå·²ç»é›†æˆäº†æ£€æµ‹å™¨å¹¶å¤„ç†å¸§

            # è¿™é‡Œçš„ process_frame åº”è¯¥æ˜¯ç”± stream è‡ªèº«è°ƒç”¨çš„ã€‚
            # è¿™é‡Œéœ€è¦ä¿®æ”¹ VideoStream å†…éƒ¨çš„ _process_frames å¾ªç¯ï¼Œè®©å®ƒç›´æ¥è°ƒç”¨ service_manager.process_single_frame
            # æˆ–è€…ï¼Œå°† service_manager.process_single_frame çš„é€»è¾‘ç§»åŠ¨åˆ° VideoStream.process_frame ä¸­

            # å½“å‰ä»£ç çš„ç»“æ„æ„å‘³ç€ app.py çš„ process_video_stream_async_loop ä¼šä» stream è·å–å¸§
            # ç„¶åå°†å¸§äº¤ç»™ service_manager.process_single_frame è¿›è¡Œåˆ†æã€‚
            # è¿™æ˜¯ä¸ºäº†å¤ç”¨ analyze_frame API çš„æ ¸å¿ƒåˆ†æé€»è¾‘ã€‚

            # è·å–å½“å‰AIåˆ†æè®¾ç½®
            settings = service_manager.get_ai_settings(camera_id)
            performance_mode = "balanced"  # é»˜è®¤å€¼
            if settings:
                performance_mode = "fast" if settings.get('realtime_mode', False) else "balanced"

            # è°ƒç”¨ AIServiceManager çš„æ ¸å¿ƒåˆ†ææ–¹æ³•
            analysis_results = await service_manager.process_single_frame(
                frame=frame,
                camera_id=camera_id,
                enable_face_recognition=settings.get('face_recognition', True),
                enable_object_detection=settings.get('object_detection', True),
                enable_behavior_detection=settings.get('behavior_analysis', False),
                enable_fire_detection=settings.get('fire_detection', True),
                performance_mode=performance_mode
            )

            # åˆ†æç»“æœä¼šç”± service_manager å†…éƒ¨å¼‚æ­¥å‘é€ç»™åç«¯å’ŒWebSocket

            # æ§åˆ¶å¸§ç‡ (å¦‚æœéœ€è¦)
            await asyncio.sleep(0.02)  # çº¦50fps

        except Exception as e:
            print(f"è§†é¢‘æµå¤„ç†å¾ªç¯é”™è¯¯ [{camera_id}]: {e}")
            traceback.print_exc()
            await asyncio.sleep(1)  # å‘ç”Ÿé”™è¯¯æ—¶ç­‰å¾…1ç§’å†ç»§ç»­


@app.post("/stream/stop/{camera_id}")
async def stop_stream(camera_id: str):
    try:
        if camera_id not in service_manager._video_streams:
            return {
                "status": "error",
                "message": f"æ‘„åƒå¤´ {camera_id} æœªåœ¨è¿è¡Œ",
                "success": False
            }

        # åœæ­¢è§†é¢‘æµ
        stream = service_manager._video_streams[camera_id]
        await stream.stop()  # ä½¿ç”¨ VideoStream çš„ stop æ–¹æ³•

        # ä»ç®¡ç†å™¨ä¸­ç§»é™¤æµå®ä¾‹
        del service_manager._video_streams[camera_id]
        
        # æ¸…ç†ç›¸å…³èµ„æº
        if camera_id in service_manager._object_trackers:
            del service_manager._object_trackers[camera_id]
        
        if camera_id in service_manager._detection_cache:
            service_manager._detection_cache[camera_id] = {"objects": {}, "face_history": {}, "frame_count": 0}

        return {
            "status": "success",
            "message": f"æˆåŠŸåœæ­¢æ‘„åƒå¤´ {camera_id} çš„è§†é¢‘æµå¤„ç†",
            "success": True
        }
    except Exception as e:
        print(f"åœæ­¢è§†é¢‘æµæ—¶å‡ºé”™: {str(e)}")
        traceback.print_exc()
        return {
            "status": "error",
            "message": f"åœæ­¢è§†é¢‘æµæ—¶å‡ºé”™: {str(e)}",
            "success": False
        }


# äººè„¸æ³¨å†Œ
class FaceData(BaseModel):
    person_name: str
    image_data: str


@app.post("/face/register/")
async def register_face(face_data: FaceData):
    try:
        image_bytes = base64.b64decode(face_data.image_data)
        image_array = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
        if image is None:
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„Base64å›¾åƒæ•°æ®ã€‚")

        # è°ƒç”¨äººè„¸è¯†åˆ«å™¨çš„æ·»åŠ æ–¹æ³• (FaceRecognizer ç±»ä¸­éœ€è¦å®ç°æ­¤æ–¹æ³•)
        face_recognizer_instance = service_manager._detectors.get("face")
        if not face_recognizer_instance:
            raise HTTPException(status_code=500, detail="äººè„¸è¯†åˆ«å™¨æœªåˆå§‹åŒ–ã€‚")

        # å‡è®¾FaceRecognizeræœ‰ä¸€ä¸ª add_face æ–¹æ³•
        # if face_recognizer_instance.add_face(image, face_data.person_name):
        #     return {"status": "success", "message": f"äººè„¸ '{face_data.person_name}' æ³¨å†ŒæˆåŠŸã€‚"}
        # else:
        #     raise HTTPException(status_code=400, detail="æ³¨å†Œå¤±è´¥ï¼Œå¯èƒ½æœªåœ¨å›¾åƒä¸­æ£€æµ‹åˆ°äººè„¸ã€‚")

        # æš‚æ—¶è¿”å›æˆåŠŸï¼Œå¾…FaceRecognizerå®ç°add_face
        print(f"æ”¶åˆ°äººè„¸æ³¨å†Œè¯·æ±‚: {face_data.person_name}")
        return {"status": "success", "message": "äººè„¸æ³¨å†Œè¯·æ±‚å·²æ”¶åˆ° (åŠŸèƒ½å¾…FaceRecognizerå®ç°)ã€‚"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ç³»ç»ŸçŠ¶æ€
@app.get("/system/status/", response_model=SystemStatus)
async def get_system_status():
    return service_manager.get_system_status()


# éŸ³é¢‘æ£€æµ‹è®¾ç½®
@app.post("/audio/settings/")
async def update_audio_settings(
        confidence_threshold: Optional[float] = Body(default=None),
        detection_interval: Optional[float] = Body(default=None),
        event_cooldown: Optional[float] = Body(default=None),
        sensitivity: Optional[str] = Body(default=None)  # "low", "medium", "high"
):
    acoustic_detector = service_manager._detectors.get("acoustic")
    if not acoustic_detector:
        raise HTTPException(status_code=500, detail="éŸ³é¢‘æ£€æµ‹å™¨æœªåˆå§‹åŒ–")

    if sensitivity is not None and sensitivity not in ["low", "medium", "high"]:
        raise HTTPException(status_code=400, detail="æ•æ„Ÿåº¦å¿…é¡»æ˜¯ 'low', 'medium' æˆ– 'high'")

    acoustic_detector.update_settings(
        confidence_threshold=confidence_threshold,
        detection_interval=detection_interval,
        event_cooldown=event_cooldown,
        sensitivity=sensitivity
    )
    return {
        "status": "success", "message": "éŸ³é¢‘æ£€æµ‹è®¾ç½®å·²æ›´æ–°",
        "current_settings": {
            "confidence_threshold": acoustic_detector.confidence_threshold,
            "detection_interval": acoustic_detector.detection_interval,
            "event_cooldown": acoustic_detector.event_cooldown,
            "volume_multiplier": acoustic_detector.volume_multiplier,
            "frequency_multiplier": acoustic_detector.frequency_multiplier,
            "noise_multiplier": acoustic_detector.noise_multiplier
        }
    }


@app.get("/audio/settings/")
async def get_audio_settings():
    acoustic_detector = service_manager._detectors.get("acoustic")
    if not acoustic_detector:
        raise HTTPException(status_code=500, detail="éŸ³é¢‘æ£€æµ‹å™¨æœªåˆå§‹åŒ–")
    return {
        "status": "success",
        "settings": {
            "confidence_threshold": acoustic_detector.confidence_threshold,
            "detection_interval": acoustic_detector.detection_interval,
            "event_cooldown": acoustic_detector.event_cooldown,
            "volume_multiplier": acoustic_detector.volume_multiplier,
            "frequency_multiplier": acoustic_detector.frequency_multiplier,
            "noise_multiplier": acoustic_detector.noise_multiplier
        }
    }


@app.post("/audio/reset/")
async def reset_audio_history():
    acoustic_detector = service_manager._detectors.get("acoustic")
    if not acoustic_detector:
        raise HTTPException(status_code=500, detail="éŸ³é¢‘æ£€æµ‹å™¨æœªåˆå§‹åŒ–")
    acoustic_detector.reset_event_history()
    return {"status": "success", "message": "éŸ³é¢‘äº‹ä»¶å†å²å·²é‡ç½®"}


@app.post("/audio/frontend/alert/")
async def process_frontend_audio_alert(
        camera_id: str = Body(...),
        audio_level: float = Body(...),
        event_type: str = Body(default="high_volume"),
        timestamp: Optional[str] = Body(default=None)
):
    if not timestamp: timestamp = datetime.now().isoformat()
    alert_result = AIAnalysisResult(
        camera_id=camera_id, event_type=f"frontend_{event_type}", location={"audio_level": audio_level},
        confidence=min(audio_level / 100.0, 1.0), timestamp=timestamp,
        details={"source": "frontend_audio_detection", "audio_level": audio_level, "event_type": event_type}
    )
    service_manager.send_alert_to_backend(alert_result)
    return {"status": "success", "message": f"éŸ³é¢‘å‘Šè­¦å·²å¤„ç†: {event_type}, éŸ³é‡çº§åˆ«: {audio_level}%"}


# å•å¸§åˆ†æ (ç”¨äºå‰ç«¯ä¸Šä¼ å›¾ç‰‡è¿›è¡Œåˆ†æ)
@app.post("/frame/analyze/")
async def analyze_frame_endpoint(
        frame: UploadFile = File(...),
        camera_id: str = Form(...),
        enable_face_recognition: bool = Form(True),
        enable_object_detection: bool = Form(True),
        enable_behavior_detection: bool = Form(False),
        enable_fire_detection: bool = Form(True),
        performance_mode: str = Form("balanced")  # "fast", "balanced", "quality"
):
    try:
        image_data = await frame.read()
        image_array = np.frombuffer(image_data, np.uint8)
        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
        if image is None:
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„å›¾åƒæ•°æ®")

        results = await service_manager.process_single_frame(
            frame=image,
            camera_id=camera_id,
            enable_face_recognition=enable_face_recognition,
            enable_object_detection=enable_object_detection,
            enable_behavior_detection=enable_behavior_detection,
            enable_fire_detection=enable_fire_detection,
            performance_mode=performance_mode
        )
        return {"status": "success", "results": results}
    except Exception as e:
        print(f"åˆ†æå¸§æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")


# ç½‘ç»œæ‘„åƒå¤´æµå¤„ç† (ç®€åŒ–ä¸ºä»…å¯åŠ¨/åœæ­¢æ ‡è®°)
@app.get("/stream/webcam/start/{camera_id}")
async def start_webcam_stream(camera_id: str):
    if camera_id not in service_manager._video_streams:
        # ä¸ºç½‘ç»œæ‘„åƒå¤´åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„VideoStreamå®ä¾‹
        class WebcamProcessor(VideoStream):  # ç»§æ‰¿VideoStreamä»¥å¤ç”¨å…¶æ–¹æ³•
            def __init__(self, camera_id_val: str):
                # è™šæ‹ŸURLï¼Œå› ä¸ºWebcamProcessorä¸æ‹‰æµ
                super().__init__(stream_url=f"webcam://{camera_id_val}", camera_id=camera_id_val)
                self.is_webcam = True
                self.is_running = True
                self.frame_count = 0

            # æ¨¡æ‹Ÿè·å–åŸå§‹å¸§ï¼ˆå®é™…åº”ç”±å‰ç«¯æ¨é€ï¼‰
            def get_raw_frame(self) -> Tuple[bool, Optional[np.ndarray]]:
                return False, None  # WebcamProcessor ä¸ä¸»åŠ¨æ‹‰å–å¸§

            def get_audio_file(self) -> Optional[str]:
                return None  # WebcamProcessor ä¸æä¾›éŸ³é¢‘æ–‡ä»¶

        service_manager._video_streams[camera_id] = WebcamProcessor(camera_id)
        if camera_id not in service_manager._object_trackers:
            service_manager._object_trackers[camera_id] = DeepSORTTracker()
        print(f"ç½‘ç»œæ‘„åƒå¤´æµ {camera_id} å·²æ ‡è®°ä¸ºå¯åŠ¨")
    return {"status": "success", "message": f"ç½‘ç»œæ‘„åƒå¤´æµ {camera_id} å·²å¯åŠ¨", "camera_id": camera_id}


@app.post("/stream/webcam/stop/{camera_id}")
async def stop_webcam_stream(camera_id: str):
    if camera_id in service_manager._video_streams:
        stream = service_manager._video_streams[camera_id]
        stream.stop()  # åœæ­¢æ ‡è®°
        del service_manager._video_streams[camera_id]
        if camera_id in service_manager._object_trackers: del service_manager._object_trackers[camera_id]
        if camera_id in service_manager._detection_cache: del service_manager._detection_cache[camera_id]
        print(f"ç½‘ç»œæ‘„åƒå¤´æµ {camera_id} å·²åœæ­¢å¹¶æ¸…é™¤ç¼“å­˜")
    return {"status": "success", "message": f"ç½‘ç»œæ‘„åƒå¤´æµ {camera_id} å·²åœæ­¢å¹¶æ¸…é™¤ç¼“å­˜"}


# ç¼“å­˜ç®¡ç†
@app.post("/detection/cache/clear/{camera_id}")
async def clear_detection_cache(camera_id: str):
    if camera_id in service_manager._detection_cache:
        del service_manager._detection_cache[camera_id]
        return {"status": "success", "message": f"å·²æ¸…é™¤æ‘„åƒå¤´ {camera_id} çš„æ£€æµ‹ç¼“å­˜"}
    return {"status": "success", "message": f"æ‘„åƒå¤´ {camera_id} æ— ç¼“å­˜éœ€è¦æ¸…é™¤"}


@app.post("/detection/cache/clear/all")
async def clear_all_detection_cache():
    service_manager._detection_cache.clear()
    return {"status": "success", "message": "å·²æ¸…é™¤æ‰€æœ‰æ£€æµ‹ç¼“å­˜"}


# æ€§èƒ½ä¼˜åŒ–ä¸è°ƒè¯•
@app.get("/performance/optimize/")
async def get_performance_tips():
    return {
        "status": "success", "tips": [], "performance_mode": "high_performance",
        "optimizations": ["åŠ¨æ€æ£€æµ‹é˜ˆå€¼", "ç»“æœæ•°é‡é™åˆ¶", "ä½åˆ†è¾¨ç‡è·³è¿‡", "å¼‚æ­¥åç«¯é€šä¿¡", "é”™è¯¯å®¹é”™æœºåˆ¶"]
    }  # ç®€åŒ–ï¼Œè¯¦æƒ…ç”±ç®¡ç†å™¨æä¾›


@app.get("/performance/stats/")
async def get_performance_stats():
    return service_manager.get_performance_stats()


@app.get("/debug/face_tracking/{camera_id}")
async def get_face_tracking_debug(camera_id: str):
    # è°ƒè¯•ä¿¡æ¯ç›´æ¥ä»service_managerè·å–å…¶å†…éƒ¨ç¼“å­˜
    if camera_id not in service_manager._detection_cache:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°è¯¥æ‘„åƒå¤´çš„æ£€æµ‹ç¼“å­˜ã€‚")

    cache_data = service_manager._detection_cache[camera_id]
    face_cache = {k: v for k, v in cache_data.get("objects", {}).items() if v.get("type") == "face"}
    face_history = cache_data.get("face_history", {})
    current_time = time.time()

    debug_info = {
        "camera_id": camera_id, "face_cache": {}, "face_history": {},
        "tracking_parameters": service_manager.get_stabilization_config(camera_id),  # ä½¿ç”¨å®é™…é…ç½®
        "advanced_features": ["äººè„¸ä¸“ç”¨æŒç»­è·Ÿè¸ªç®—æ³•", "è¿åŠ¨è¶‹åŠ¿é¢„æµ‹å’Œæ’å€¼", "è‡ªé€‚åº”å›¾åƒç¼©æ”¾", "ç»¼åˆåŒ¹é…è¯„åˆ†ç³»ç»Ÿ",
                              "è¿åŠ¨å†å²è®°å½•", "é¢„æµ‹ä½ç½®ä¿æŒæœºåˆ¶", "è¿ç»­æ£€æµ‹ä¼˜å…ˆçº§"]
    }

    for obj_id, obj_data in face_cache.items():
        time_since_last_seen = current_time - obj_data["last_seen"]
        debug_info["face_cache"][obj_id] = {
            "bbox": obj_data["bbox"], "confidence": obj_data["confidence"],
            "stable_count": obj_data.get("stable_count", 0),
            "consecutive_detections": obj_data.get("consecutive_detections", 0),
            "last_seen": obj_data["last_seen"], "is_stable": obj_data.get("stable_count", 0) >= 1,
            "age_seconds": time_since_last_seen,
            "status": "active" if time_since_last_seen < 0.5 else "missing" if time_since_last_seen < 2.0 else "expired",
            "first_seen": obj_data.get("first_seen", obj_data["last_seen"]),
            "has_predicted_bbox": "predicted_bbox" in obj_data
        }
    for obj_id, history in face_history.items():
        if obj_id in debug_info["face_cache"]:
            debug_info["face_history"][obj_id] = {
                "position_count": len(history.get("positions", [])),
                "latest_positions": history.get("positions", [])[-3:],
                "latest_timestamps": history.get("timestamps", [])[-3:],
                "has_motion_data": len(history.get("positions", [])) >= 2
            }
            if len(history.get("positions", [])) >= 2:
                pos1, pos2 = history["positions"][-2], history["positions"][-1]
                time1, time2 = history["timestamps"][-2], history["timestamps"][-1]
                if time2 - time1 > 0:
                    vx = (pos2[0] - pos1[0]) / (time2 - time1)
                    vy = (pos2[1] - pos1[1]) / (time2 - time1)
                    speed = (vx ** 2 + vy ** 2) ** 0.5
                    debug_info["face_history"][obj_id]["motion_speed"] = round(speed, 2)
                    debug_info["face_history"][obj_id]["motion_vector"] = [round(vx, 2), round(vy, 2)]

    debug_info["statistics"] = {
        "total_faces": len(face_cache),
        "active_faces": len([f for f in face_cache.values() if f.get("status") == "active"]),
        "missing_faces": len([f for f in face_cache.values() if f.get("status") == "missing"]),
        "face_history_count": len(face_history), "system_status": "é«˜çº§äººè„¸æŒç»­è·Ÿè¸ªå·²å¯ç”¨"
    }
    return {"status": "success", "debug_info": debug_info}


@app.get("/performance/mode/recommend/")
async def recommend_performance_mode():
    import psutil
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    gpu_available = False;
    gpu_memory_free = 0
    try:
        import GPUtil
        gpus = GPUtil.getGPUs()
        gpu_available = len(gpus) > 0
        if gpus: gpu_memory_free = gpus[0].memoryFree
    except:
        pass

    if cpu_percent > 80 or memory.percent > 85:
        recommended_mode = "fast"; reason = "ç³»ç»Ÿè´Ÿè½½è¾ƒé«˜ï¼Œå»ºè®®ä½¿ç”¨æé€Ÿæ¨¡å¼å‡å°‘å¡é¡¿"
    elif cpu_percent < 40 and memory.percent < 60 and gpu_available:
        recommended_mode = "quality"; reason = "ç³»ç»Ÿæ€§èƒ½å……è¶³ï¼Œå¯ä»¥ä½¿ç”¨è´¨é‡æ¨¡å¼è·å¾—æœ€ä½³æ•ˆæœ"
    else:
        recommended_mode = "balanced"; reason = "ç³»ç»Ÿæ€§èƒ½é€‚ä¸­ï¼Œå»ºè®®ä½¿ç”¨å¹³è¡¡æ¨¡å¼"

    return {
        "recommended_mode": recommended_mode, "reason": reason,
        "system_info": {
            "cpu_usage": f"{cpu_percent:.1f}%", "memory_usage": f"{memory.percent:.1f}%",
            "available_memory": f"{memory.available / 1024 / 1024 / 1024:.1f}GB",
            "gpu_available": gpu_available, "gpu_memory_free": f"{gpu_memory_free}MB" if gpu_available else "N/A"
        }
    }


@app.get("/performance/guide/")
async def get_performance_guide():
    return {
        "title": "SmartEye æ‘„åƒå¤´ç›‘æ§æ€§èƒ½ä¼˜åŒ–æŒ‡å—", "overview": "é’ˆå¯¹åŒæ—¶å¼€å¯å¤šä¸ªæ£€æµ‹åŠŸèƒ½å¯¼è‡´å¡é¡¿çš„è§£å†³æ–¹æ¡ˆ",
        "performance_modes": {},  # ç®€åŒ–ï¼Œç®¡ç†å™¨ä¸­æä¾›
        "usage_instructions": {"api_call": {"endpoint": "/frame/analyze/", "new_parameter": "performance_mode"}},
        "optimization_tips": {"hardware": [], "software": [], "network": []},
        "monitoring": {}, "troubleshooting": {}, "quick_start": {}
    }


# æ£€æµ‹æ¡†ç¨³å®šåŒ–é…ç½®
@app.post("/detection/stabilization/config/")
async def configure_stabilization(
        camera_id: str = Body(...),
        face_smooth_factor: float = Body(default=0.92),
        object_smooth_factor: float = Body(default=0.88),
        face_match_threshold: int = Body(default=120),
        object_match_threshold: int = Body(default=60),
        jitter_detection_threshold: int = Body(default=30),
        max_size_change_ratio: float = Body(default=0.2)
):
    if not (0.5 <= face_smooth_factor <= 0.99) or not (0.5 <= object_smooth_factor <= 0.99) or \
            not (30 <= face_match_threshold <= 300) or not (20 <= object_match_threshold <= 200):
        raise HTTPException(status_code=400, detail="å‚æ•°èŒƒå›´æ— æ•ˆ")

    config_data = {
        "face_smooth_factor": face_smooth_factor, "object_smooth_factor": object_smooth_factor,
        "face_match_threshold": face_match_threshold, "object_match_threshold": object_match_threshold,
        "jitter_detection_threshold": jitter_detection_threshold, "max_size_change_ratio": max_size_change_ratio
    }
    service_manager.update_stabilization_config(camera_id, config_data)
    return {"status": "success", "message": f"æ‘„åƒå¤´ {camera_id} çš„ç¨³å®šåŒ–å‚æ•°å·²æ›´æ–°",
            "config": service_manager.get_stabilization_config(camera_id)}


@app.get("/detection/stabilization/config/{camera_id}")
async def get_stabilization_config(camera_id: str):
    config = service_manager.get_stabilization_config(camera_id)
    return {
        "camera_id": camera_id, "config": config,
        "parameter_explanations": {
            "face_smooth_factor": "äººè„¸æ¡†å¹³æ»‘å¼ºåº¦ (0.5-0.99ï¼Œè¶Šé«˜è¶Šå¹³æ»‘ä½†å¯èƒ½æ»å)",
            "object_smooth_factor": "ç›®æ ‡æ¡†å¹³æ»‘å¼ºåº¦ (0.5-0.99ï¼Œè¶Šé«˜è¶Šå¹³æ»‘)",
            "face_match_threshold": "äººè„¸åŒ¹é…è·ç¦»é˜ˆå€¼ (åƒç´ ï¼Œè¶Šå°è¶Šä¸¥æ ¼)",
            "object_match_threshold": "ç›®æ ‡åŒ¹é…è·ç¦»é˜ˆå€¼ (åƒç´ ï¼Œè¶Šå°è¶Šä¸¥æ ¼)",
            "jitter_detection_threshold": "æŠ–åŠ¨æ£€æµ‹é˜ˆå€¼ (åƒç´ ï¼Œè¶…è¿‡æ­¤å€¼è§¦å‘æŠ—æŠ–åŠ¨)",
            "max_size_change_ratio": "æ¡†å°ºå¯¸å˜åŒ–é™åˆ¶ (0.1-0.5ï¼Œé™åˆ¶çªç„¶çš„å°ºå¯¸å˜åŒ–)"
        }
    }


@app.post("/detection/stabilization/preset/{preset_name}")
async def apply_stabilization_preset(preset_name: str, camera_id: str = Body(...)):
    try:
        return service_manager.apply_stabilization_preset(preset_name, camera_id)
    except HTTPException as e:
        raise e
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/detection/stabilization/presets/")
async def list_stabilization_presets():
    return {"available_presets": service_manager.list_stabilization_presets()}


@app.post("/detection/anti_flicker/apply/")
async def apply_anti_flicker_all_cameras():
    return await service_manager.apply_anti_flicker_all_cameras()


@app.post("/detection/identity_stabilization/status/")
async def check_identity_stabilization_status():
    return {
        "status": "active", "message": "ğŸ¯ äººè„¸èº«ä»½ç¨³å®šåŒ–ç³»ç»Ÿè¿è¡Œæ­£å¸¸",
        "system_stats": {},  # ç®€åŒ–
        "stabilization_features": {},  # ç®€åŒ–
        "current_settings": {},  # ç®€åŒ–
        "quick_actions": {}  # ç®€åŒ–
    }


@app.get("/detection/anti_jitter/status/")
async def get_anti_jitter_status():
    return {
        "anti_jitter_enabled": True, "status": "è‡ªåŠ¨å¯ç”¨", "description": "æŠ—æŠ–åŠ¨åŠŸèƒ½å·²é»˜è®¤å¯ç”¨ï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œ",
        "automatic_features": {}, "default_settings": {}, "performance_impact": {}, "monitoring": {},
        "if_still_jittering": {}, "message": "âœ… æŠ—æŠ–åŠ¨åŠŸèƒ½å·²è‡ªåŠ¨å¯ç”¨å¹¶è¿è¡Œï¼Œæ‚¨çš„æ£€æµ‹æ¡†ä¼šè‡ªåŠ¨ä¿æŒç¨³å®šï¼"
    }


# äººè„¸çµæ•åº¦
@app.post("/face/sensitivity/adjust/")
async def adjust_face_recognition_sensitivity(
        tolerance: float = Body(default=None, description="äººè„¸è¯†åˆ«å®¹å¿åº¦ (0.3-0.8, è¶Šå¤§è¶Šå®½æ¾)"),
        detection_model: str = Body(default=None, description="æ£€æµ‹æ¨¡å‹ (auto/cnn/hog)"),
        enable_multi_scale: Optional[bool] = Body(default=None, description="æ˜¯å¦å¯ç”¨å¤šå°ºåº¦æ£€æµ‹"),
        min_face_size: Optional[int] = Body(default=None, description="æœ€å°äººè„¸å°ºå¯¸ (åƒç´ )")
):
    if tolerance is not None and not (0.3 <= tolerance <= 0.8): raise HTTPException(status_code=400,
                                                                                    detail="tolerance å¿…é¡»åœ¨ 0.3-0.8 èŒƒå›´å†…")
    if detection_model is not None and detection_model not in ["auto", "cnn", "hog"]: raise HTTPException(
        status_code=400, detail="detection_model å¿…é¡»æ˜¯ auto/cnn/hog ä¹‹ä¸€")

    config_data = {}
    if tolerance is not None: config_data["tolerance"] = tolerance
    if detection_model is not None: config_data["detection_model"] = detection_model
    if enable_multi_scale is not None: config_data["enable_multi_scale"] = enable_multi_scale
    if min_face_size is not None: config_data["min_face_size"] = min_face_size

    service_manager.update_face_recognition_config(config_data)
    return {"status": "success", "message": "äººè„¸è¯†åˆ«çµæ•åº¦å·²æˆåŠŸè°ƒæ•´",
            "config": service_manager.get_face_recognition_config()}


@app.get("/face/sensitivity/status/")
async def get_face_recognition_sensitivity():
    return {"status": "success", "current_config": service_manager.get_face_recognition_config()}


@app.post("/face/detection/test/")
async def test_face_detection_with_config(
        frame: UploadFile = File(...),
        tolerance: Optional[float] = Body(default=None),
        detection_model: Optional[str] = Body(default=None)
):
    try:
        image_bytes = await frame.read()
        nparr = np.frombuffer(image_bytes, np.uint8)
        test_frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if test_frame is None: raise HTTPException(status_code=400, detail="æ— æ³•è§£æå›¾ç‰‡")

        test_tolerance = tolerance or service_manager.get_face_recognition_config().get('tolerance', 0.65)
        test_model = detection_model or service_manager.get_face_recognition_config().get('detection_model', 'auto')

        if "face" not in service_manager._detectors: raise HTTPException(status_code=500, detail="äººè„¸è¯†åˆ«å™¨æœªåˆå§‹åŒ–")

        start_time = time.time()
        # è¿™é‡Œç›´æ¥è°ƒç”¨ FaceRecognizerï¼Œä¸ç»è¿‡ç¨³å®šåŒ–ï¼Œä»¥ä¾¿æµ‹è¯•åŸå§‹æ£€æµ‹æ•ˆæœ
        results = service_manager._detectors["face"].detect_and_recognize(test_frame, tolerance=test_tolerance)
        detection_time = (time.time() - start_time) * 1000

        total_faces = len(results);
        known_faces = sum(1 for r in results if r["identity"]["known"]);
        unknown_faces = total_faces - known_faces

        return {
            "status": "success",
            "test_results": {
                "total_faces_detected": total_faces, "known_faces": known_faces, "unknown_faces": unknown_faces,
                "detection_time_ms": round(detection_time, 2),
                "test_parameters": {"tolerance": test_tolerance, "detection_model": test_model}
            },
            "face_details": [
                {"face_id": i + 1, "identity": result["identity"]["name"], "known": result["identity"]["known"],
                 "confidence": round(result["confidence"], 3), "location": result["location"]}
                for i, result in enumerate(results)
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æµ‹è¯•å¤±è´¥: {str(e)}")


@app.post("/face/sensitivity/optimize/")
async def optimize_face_recognition_for_sensitivity():
    # åº”ç”¨é«˜çµæ•åº¦è®¾ç½®
    config_data = {
        "tolerance": 0.65, "detection_model": "auto", "enable_multi_scale": True, "min_face_size": 40
    }
    service_manager.update_face_recognition_config(config_data)

    cleared_cameras = []
    for camera_id in list(service_manager._detection_cache.keys()):
        if camera_id in service_manager._detection_cache and "face_history" in service_manager._detection_cache[
            camera_id]:
            service_manager._detection_cache[camera_id]["face_history"].clear()
            cleared_cameras.append(camera_id)

    return {
        "status": "success", "message": "ğŸ¯ äººè„¸è¯†åˆ«çµæ•åº¦å·²ä¼˜åŒ–!",
        "optimizations_applied": [], "new_config": service_manager.get_face_recognition_config(),
        "cleared_cache_cameras": cleared_cameras
    }


@app.get("/face/sensitivity/improvements/")
async def get_face_recognition_improvements():
    return {
        "status": "success", "optimization_summary": {},
        "current_config": service_manager.get_face_recognition_config()
    }


# ç›®æ ‡æ£€æµ‹çµæ•åº¦
@app.post("/object/sensitivity/adjust/")
async def adjust_object_detection_sensitivity(
        confidence_threshold: Optional[float] = Body(default=None),
        area_ratio_threshold: Optional[float] = Body(default=None),
        special_class_threshold: Optional[float] = Body(default=None),
        enable_size_filter: Optional[bool] = Body(default=None),
        enable_aspect_ratio_filter: Optional[bool] = Body(default=None)
):
    if confidence_threshold is not None and not (0.1 <= confidence_threshold <= 0.8): raise HTTPException(
        status_code=400, detail="confidence_threshold å¿…é¡»åœ¨ 0.1-0.8 èŒƒå›´å†…")
    if area_ratio_threshold is not None and not (0.5 <= area_ratio_threshold <= 1.0): raise HTTPException(
        status_code=400, detail="area_ratio_threshold å¿…é¡»åœ¨ 0.5-1.0 èŒƒå›´å†…")
    if special_class_threshold is not None and not (0.3 <= special_class_threshold <= 0.9): raise HTTPException(
        status_code=400, detail="special_class_threshold å¿…é¡»åœ¨ 0.3-0.9 èŒƒå›´å†…")

    config_data = {}
    if confidence_threshold is not None: config_data["confidence_threshold"] = confidence_threshold
    if area_ratio_threshold is not None: config_data["area_ratio_threshold"] = area_ratio_threshold
    if special_class_threshold is not None: config_data["special_class_threshold"] = special_class_threshold
    if enable_size_filter is not None: config_data["enable_size_filter"] = enable_size_filter
    if enable_aspect_ratio_filter is not None: config_data["enable_aspect_ratio_filter"] = enable_aspect_ratio_filter

    service_manager.update_object_detection_config(config_data)
    return {"status": "success", "message": "ç›®æ ‡æ£€æµ‹çµæ•åº¦å·²æˆåŠŸè°ƒæ•´",
            "config": service_manager.get_object_detection_config()}


@app.get("/object/sensitivity/status/")
async def get_object_detection_sensitivity():
    return {"status": "success", "current_config": service_manager.get_object_detection_config()}


@app.post("/object/sensitivity/optimize/")
async def optimize_object_detection_for_sensitivity():
    config_data = {
        "confidence_threshold": 0.3, "area_ratio_threshold": 0.95,
        "special_class_threshold": 0.5, "enable_size_filter": True, "enable_aspect_ratio_filter": False
    }
    service_manager.update_object_detection_config(config_data)

    cleared_cameras = []
    for camera_id in list(service_manager._detection_cache.keys()):
        if camera_id in service_manager._detection_cache and "objects" in service_manager._detection_cache[camera_id]:
            object_cache = service_manager._detection_cache[camera_id]["objects"]
            service_manager._detection_cache[camera_id]["objects"] = {k: v for k, v in object_cache.items() if
                                                                      v.get("type") == "face"}
            cleared_cameras.append(camera_id)

    return {
        "status": "success", "message": "ğŸ¯ ç›®æ ‡æ£€æµ‹çµæ•åº¦å·²ä¼˜åŒ–!",
        "optimizations_applied": [], "new_config": service_manager.get_object_detection_config(),
        "cleared_cache_cameras": cleared_cameras
    }


@app.post("/detection/sensitivity/optimize_all/")
async def optimize_all_detection_sensitivity():
    # ä¼˜åŒ–ç›®æ ‡æ£€æµ‹
    object_config_data = {
        "confidence_threshold": 0.3, "area_ratio_threshold": 0.95,
        "special_class_threshold": 0.5, "enable_size_filter": True, "enable_aspect_ratio_filter": False
    }
    service_manager.update_object_detection_config(object_config_data)

    # ä¼˜åŒ–äººè„¸è¯†åˆ«
    face_config_data = {
        "tolerance": 0.7, "detection_model": "auto", "enable_multi_scale": True, "min_face_size": 35
    }
    service_manager.update_face_recognition_config(face_config_data)

    cleared_cameras = []
    for camera_id in list(service_manager._detection_cache.keys()):
        service_manager._detection_cache[camera_id] = {"objects": {}, "face_history": {}, "frame_count": 0}
        cleared_cameras.append(camera_id)

    return {
        "status": "success", "message": "ğŸ¯ æ‰€æœ‰æ£€æµ‹ç³»ç»Ÿçµæ•åº¦å·²ä¼˜åŒ–åˆ°æœ€é«˜!",
        "object_detection_optimizations": [], "face_recognition_optimizations": [],
        "applied_configs": {"object_detection": service_manager.get_object_detection_config(),
                            "face_recognition": service_manager.get_face_recognition_config()},
        "cleared_cache_cameras": cleared_cameras
    }


# å±é™©åŒºåŸŸæ£€æµ‹API
@app.post("/danger_zone/config/")
async def configure_danger_zones(camera_id: str = Body(...), zones: List[Dict] = Body(...)):
    service_manager.update_danger_zones(camera_id, zones)
    return {"status": "success", "message": f"æ‘„åƒå¤´ {camera_id} çš„å±é™©åŒºåŸŸé…ç½®å·²æ›´æ–°", "configured_zones": len(zones),
            "zones": zones}


@app.get("/danger_zone/status/{camera_id}")
async def get_danger_zone_status(camera_id: str):
    status = service_manager.get_danger_zone_status(camera_id)
    return {"status": "success", "data": status}


@app.delete("/danger_zone/{camera_id}/{zone_id}")
async def remove_danger_zone(camera_id: str, zone_id: str):
    service_manager.remove_danger_zone(camera_id, zone_id)
    return {"status": "success", "message": f"å±é™©åŒºåŸŸ {zone_id} å·²ç§»é™¤"}


# äººè„¸å¤„ç†è¾…åŠ©API (ç”¨äºäººè„¸æ³¨å†Œå’ŒéªŒè¯ï¼Œå¯èƒ½éœ€è¦ä¸Djangoåç«¯äº¤äº’)
@app.post("/process_face")
async def process_face(request_data: dict):
    # ä¿æŒåŸæœ‰é€»è¾‘ï¼Œä½†è¯·æ³¨æ„ï¼Œface_recognitionåº“çš„é›†æˆåœ¨æ­¤å¤„å¹¶éé€šè¿‡AIServiceManagerç®¡ç†
    # è¿™æ˜¯ç”¨äºç‹¬ç«‹çš„äººè„¸ç‰¹å¾æå–è€Œéå®æ—¶æ£€æµ‹æµ
    try:
        image_data = base64.b64decode(request_data.get("image", ""))
        nparr = np.frombuffer(image_data, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if image is None: return {"success": False, "message": "æ— æ³•è§£ç å›¾åƒ"}

        # ä½¿ç”¨AIæœåŠ¡ä¸­çš„ FaceRecognizer å®ä¾‹
        face_recognizer_instance = service_manager._detectors.get("face")
        if not face_recognizer_instance:
            return {"success": False, "message": "äººè„¸è¯†åˆ«å™¨æœªåˆå§‹åŒ–"}

        # ä¸ºäº†é¿å…ä¾èµ–å¤–éƒ¨çš„face_recognitionåº“å¯¼å…¥ï¼Œæˆ‘ä»¬å‡è®¾FaceRecognizerå†…éƒ¨ä¼šå¤„ç†è¿™äº›
        # è¿™é‡Œéœ€è¦ FaceRecognizer æä¾›ä¸€ä¸ª extract_encoding æ–¹æ³•
        # æš‚æ—¶ç®€åŒ–ï¼Œç›´æ¥è°ƒç”¨detect_and_recognizeå¹¶å–ç¬¬ä¸€ä¸ª
        results = face_recognizer_instance.detect_and_recognize(image, tolerance=0.6)
        if not results: return {"success": False, "message": "æœªæ£€æµ‹åˆ°äººè„¸"}
        if len(results) > 1: return {"success": False, "message": "æ£€æµ‹åˆ°å¤šä¸ªäººè„¸ï¼Œè¯·ç¡®ä¿å›¾åƒä¸­åªæœ‰ä¸€ä¸ªäººè„¸"}

        # å‡è®¾ result['identity'] ä¸­åŒ…å«äº†ç¼–ç 
        # è¿™éƒ¨åˆ†ä»£ç éœ€è¦æ ¹æ® FaceRecognizer å®é™…è¿”å›çš„ç»“æ„è¿›è¡Œè°ƒæ•´
        # ä¸ºäº†å…¼å®¹æ€§ï¼Œè¿”å›ä¸€ä¸ªè™šæ‹Ÿç¼–ç 
        face_location = results[0]['location']
        # æ³¨æ„ï¼šFaceRecognizer.detect_and_recognize å¹¶æ²¡æœ‰ç›´æ¥è¿”å› face_encoding
        # è¿™éœ€è¦ FaceRecognizer æä¾›æ–°çš„æ–¹æ³•æˆ–è¿”å›æ›´è¯¦ç»†çš„æ•°æ®

        # ä¸´æ—¶è¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿçš„ç¼–ç ï¼Œç›´åˆ°FaceRecognizeræœ‰æ˜ç¡®çš„extract_encodingæ–¹æ³•
        mock_encoding = [0.1] * 128
        return {
            "success": True, "message": "äººè„¸å¤„ç†æˆåŠŸ",
            "face_encoding": mock_encoding,
            "face_location": face_location
        }
    except Exception as e:
        return {"success": False, "message": f"å¤„ç†äººè„¸æ—¶å‡ºé”™: {str(e)}"}


@app.post("/verify_face")
async def verify_face(request_data: dict):
    try:
        image_data = base64.b64decode(request_data.get("image", ""))
        nparr = np.frombuffer(image_data, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if image is None: return {"success": False, "message": "æ— æ³•è§£ç å›¾åƒ"}

        user_id = request_data.get("user_id")
        if not user_id: return {"success": False, "message": "æœªæä¾›ç”¨æˆ·ID"}

        face_recognizer_instance = service_manager._detectors.get("face")
        if not face_recognizer_instance:
            return {"success": False, "message": "äººè„¸è¯†åˆ«å™¨æœªåˆå§‹åŒ–"}

        # åŒæ ·ï¼Œè¿™éƒ¨åˆ†éœ€è¦ FaceRecognizer æä¾›æ¯”å¯¹èƒ½åŠ›æˆ– extract_encoding
        # æš‚æ—¶æ¨¡æ‹Ÿæ¯”å¯¹è¿‡ç¨‹
        # è·å–ä¼ å…¥å›¾åƒçš„äººè„¸ç¼–ç  (å‡è®¾FaceRecognizerèƒ½æå–)
        results = face_recognizer_instance.detect_and_recognize(image, tolerance=0.6)
        if not results: return {"success": False, "message": "æœªæ£€æµ‹åˆ°äººè„¸"}
        if len(results) > 1: return {"success": False, "message": "æ£€æµ‹åˆ°å¤šä¸ªäººè„¸ï¼Œè¯·ç¡®ä¿å›¾åƒä¸­åªæœ‰ä¸€ä¸ªäººè„¸"}

        # æ¨¡æ‹Ÿè·å–ç”¨æˆ·æ³¨å†Œçš„äººè„¸ç¼–ç  (ä»åç«¯è·å–)
        api_url = f"http://localhost:8000/api/users/faces/user/{user_id}/encodings/"
        try:
            response = requests.get(api_url)
            if response.status_code != 200: return {"success": False, "message": "æ— æ³•è·å–ç”¨æˆ·äººè„¸æ•°æ®"}
            stored_face_encodings = response.json().get("encodings", [])
            if not stored_face_encodings: return {"success": False, "message": "ç”¨æˆ·æ²¡æœ‰æ³¨å†Œäººè„¸æ•°æ®"}

            # å‡è®¾FaceRecognizerèƒ½ç›´æ¥è¿›è¡Œæ¯”è¾ƒæˆ–æˆ‘ä»¬èƒ½è·å–åˆ°å½“å‰å¸§çš„ç¼–ç 
            # å½“å‰FaceRecognizer.detect_and_recognizeè¿”å›çš„æ˜¯èº«ä»½ä¿¡æ¯ï¼Œä¸æ˜¯åŸå§‹ç¼–ç 
            # è¿™ä¸€æ­¥éœ€è¦è¿›ä¸€æ­¥é‡æ„ FaceRecognizer ä»¥æ”¯æŒè¿”å›åŸå§‹ç¼–ç æˆ–ç›´æ¥æ¯”å¯¹

            # æš‚æ—¶é€šè¿‡åå­—æ¥æ¨¡æ‹ŸéªŒè¯
            detected_name = results[0]["identity"].get("name", "unknown")
            # å‡è®¾åç«¯è¿”å›çš„encodingsä¸­ä¹ŸåŒ…å«äº†nameä¿¡æ¯ï¼Œæˆ–user_idå¯¹åº”ä¸€ä¸ªå›ºå®šname
            # è¿™é‡Œç®€åŒ–ä¸ºåªè¦æ˜¯"known"ä¸”åå­—åŒ¹é…å°±æˆåŠŸ
            if results[0]["identity"]["known"] and detected_name == user_id:  # å‡è®¾user_idå°±æ˜¯name
                return {"success": True, "matched": True, "confidence": results[0]["confidence"],
                        "message": "äººè„¸éªŒè¯æˆåŠŸ"}
            else:
                return {"success": True, "matched": False, "message": "äººè„¸ä¸åŒ¹é…"}

        except Exception as e:
            return {"success": False, "message": f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"}
    except Exception as e:
        return {"success": False, "message": f"å¤„ç†äººè„¸æ—¶å‡ºé”™: {str(e)}"}


# è§†é¢‘æµè¿æ¥æµ‹è¯•
@app.post("/stream/test/")
async def test_stream_connection_endpoint(url: str = Body(...), type: str = Body(...)):
    print(f"æ­£åœ¨æµ‹è¯•è§†é¢‘æµè¿æ¥: {url} (ç±»å‹: {type})")
    try:
        # ä¸ºäº†æµ‹è¯•ï¼Œè¿™é‡Œä¸ä¼ å…¥å®é™…çš„æ£€æµ‹å™¨å®ä¾‹
        stream = VideoStream(stream_url=url, camera_id="test_connection_id")
        is_available = await stream.test_connection()
        
        if is_available:
            return {
                "status": "success",
                "message": "è§†é¢‘æµå¯ç”¨",
                "success": True  # ä¸ºäº†å…¼å®¹å‰ç«¯ä»£ç 
            }
        else:
            return {
                "status": "error",
                "message": "æ— æ³•è¿æ¥åˆ°è§†é¢‘æµï¼Œè¯·æ£€æŸ¥æµåœ°å€æ˜¯å¦æ­£ç¡®",
                "success": False  # ä¸ºäº†å…¼å®¹å‰ç«¯ä»£ç 
            }
    except Exception as e:
        print(f"æµ‹è¯•è§†é¢‘æµè¿æ¥æ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "message": f"æµ‹è¯•è§†é¢‘æµæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
            "success": False  # ä¸ºäº†å…¼å®¹å‰ç«¯ä»£ç 
        }


# å¯åŠ¨Uvicorn
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)