import datetime
import os
import base64
import time
import asyncio
import traceback
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager
from typing import Dict, List, Optional

import torch
import uvicorn
import cv2
import numpy as np
import requests
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Body, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# å¯¼å…¥æˆ‘ä»¬è‡ªå®šä¹‰çš„æ‰€æœ‰æ ¸å¿ƒAIæ¨¡å—
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.video_stream import VideoStream
from core.object_detection import GenericPredictor
from core.behavior_detection import BehaviorDetector
from core.face_recognition import FaceRecognizer
from core.acoustic_detection import AcousticEventDetector  # æ›´æ–°ä¸ºæ–°çš„ç±»å
from models.alert_models import AIAnalysisResult  # ç¡®ä¿è¿™ä¸ªæ–‡ä»¶å­˜åœ¨

# åœ¨åº”ç”¨å¯åŠ¨æ—¶ï¼Œä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡
current_dir = os.path.dirname(os.path.abspath(__file__))
# æ„å»º .env æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
dotenv_path = os.path.join(current_dir, '.env')
# ä»æŒ‡å®šè·¯å¾„åŠ è½½ç¯å¢ƒå˜é‡
if os.path.exists(dotenv_path):
    print(f"--- æ­£åœ¨ä» '{dotenv_path}' åŠ è½½ç¯å¢ƒå˜é‡ ---")
    load_dotenv(dotenv_path=dotenv_path)
else:
    print(f"--- è­¦å‘Š: æœªæ‰¾åˆ° .env æ–‡ä»¶ at '{dotenv_path}'ï¼Œå°†ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡ ---")
load_dotenv()

# --- å…¨å±€å˜é‡ ---
video_streams: Dict[str, VideoStream] = {}
detectors: Dict[str, object] = {}
thread_pool = ThreadPoolExecutor(max_workers=os.cpu_count() or 4)


# --- FastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† ---

def init_detectors():
    """åˆå§‹åŒ–æ‰€æœ‰AIæ£€æµ‹å™¨æ¨¡å‹ã€‚è¿™ä¸ªå‡½æ•°åœ¨æœåŠ¡å¯åŠ¨æ—¶åªæ‰§è¡Œä¸€æ¬¡ã€‚"""
    try:
        print("--- æ­£åœ¨åˆå§‹åŒ–æ‰€æœ‰æ£€æµ‹å™¨ ---")


        # æˆ‘ä»¬ä¸å†ç¡¬ç¼–ç ä»»ä½•è·¯å¾„ï¼Œè€Œæ˜¯ä» .env æ–‡ä»¶ä¸­è¯»å–ç»Ÿä¸€çš„æ ¹è·¯å¾„ã€‚
        ASSET_BASE_PATH = os.getenv("G_DRIVE_ASSET_PATH")
        if not ASSET_BASE_PATH or not os.path.isdir(ASSET_BASE_PATH):
            raise FileNotFoundError(
                f"è‡´å‘½é”™è¯¯: ç¯å¢ƒå˜é‡ 'G_DRIVE_ASSET_PATH' æŒ‡å‘çš„èµ„æºç›®å½• '{ASSET_BASE_PATH}' ä¸å­˜åœ¨æˆ–æœªé…ç½®ã€‚è¯·æ£€æŸ¥ .env æ–‡ä»¶å’ŒGç›˜ç›®å½•ã€‚")

        print(f"--- ä½¿ç”¨èµ„æºæ ¹ç›®å½•: {ASSET_BASE_PATH} ---")

        # æ‰€æœ‰èµ„æºçš„è·¯å¾„éƒ½å°†åŸºäº ASSET_BASE_PATH æ„å»ºï¼Œå®ç°é›†ä¸­ç®¡ç†ã€‚
        model_weights_path = os.path.join(ASSET_BASE_PATH, "models", "torch",
                                          "fasterrcnn_resnet50_fpn_coco-258fb6c6.pth")
        class_names_path = os.path.join(ASSET_BASE_PATH, "models", "coco.names")
        known_faces_dir = os.path.join(ASSET_BASE_PATH, "known_faces")

        # 1. åˆå§‹åŒ–é€šç”¨ç›®æ ‡æ£€æµ‹å™¨
        class_names = []
        try:
            with open(class_names_path, 'r', encoding='utf-8') as f:
                class_names = [line.strip() for line in f.readlines()]
            print(f"æˆåŠŸä» '{class_names_path}' åŠ è½½ {len(class_names)} ä¸ªç±»åˆ«åç§°ã€‚")
        except FileNotFoundError:
            print(f"è­¦å‘Š: æ‰¾ä¸åˆ°ç±»åˆ«åç§°æ–‡ä»¶ at '{class_names_path}'ã€‚")
            # å³ä½¿æ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œä¹Ÿæä¾›ä¸€ä¸ªåŸºç¡€çš„é»˜è®¤å€¼ä»¥ç»´æŒè¿è¡Œ
            class_names = ["background", "person"]

        # ä½¿ç”¨YOLOv8æ¨¡å‹
        model_weights_path = os.path.join(ASSET_BASE_PATH, "models", "torch", "yolov8n.pt")
        print(f"æ­£åœ¨åŠ è½½YOLOv8æ¨¡å‹æƒé‡: {model_weights_path}")

        detectors["object"] = GenericPredictor(
            model_weights_path=model_weights_path,
            num_classes=len(class_names),
            class_names=class_names
        )

        # 2. åˆå§‹åŒ–è¡Œä¸ºæ£€æµ‹å™¨
        detectors["behavior"] = BehaviorDetector()

        # 3. åˆå§‹åŒ–äººè„¸è¯†åˆ«å™¨
        print(f"æ­£åœ¨ä»ç›®å½• '{known_faces_dir}' åŠ è½½å·²çŸ¥äººè„¸ã€‚")
        detectors["face"] = FaceRecognizer(
            known_faces_dir=known_faces_dir  # ä½¿ç”¨æˆ‘ä»¬æ–°æ„å»ºçš„è·¯å¾„
        )

        # 4. åˆå§‹åŒ–å£°å­¦äº‹ä»¶æ£€æµ‹å™¨ (å¦‚æœå¯ç”¨äº†)
        if os.getenv("ENABLE_SOUND_DETECTION", "false").lower() == "true":
            try:
                acoustic_detector = AcousticEventDetector()  # ä½¿ç”¨æ–°çš„ç±»å
                detectors["acoustic"] = acoustic_detector
            except Exception as e:
                print(f"è­¦å‘Š: å£°å­¦æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨æ­¤åŠŸèƒ½ã€‚é”™è¯¯: {e}")

        print("--- æ‰€æœ‰æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ ---")

    except Exception as e:
        print(f"è‡´å‘½é”™è¯¯: æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        raise


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨ã€‚"""
    # å¯åŠ¨ä»»åŠ¡
    init_detectors()
    # ç§»é™¤å¯¹ start_listening çš„è°ƒç”¨ï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨ä½¿ç”¨ process_audio_file æ–¹æ³•
    if "acoustic" in detectors:
        asyncio.create_task(run_acoustic_analysis())

    yield  # æœåŠ¡åœ¨æ­¤è¿è¡Œæ—¶ï¼Œå¤„ç†APIè¯·æ±‚

    # å…³é—­ä»»åŠ¡
    print("æœåŠ¡æ­£åœ¨å…³é—­ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
    for stream in video_streams.values():
        stream.stop()
    if "acoustic" in detectors:
        detectors["acoustic"].stop_monitoring()  # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å
    thread_pool.shutdown(wait=True)
    print("èµ„æºæ¸…ç†å®Œæ¯•ã€‚")


# åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹
app = FastAPI(
    title="AI æ™ºèƒ½åˆ†ææœåŠ¡ (æœ€ç»ˆç‰ˆ)",
    description="æä¾›è§†é¢‘æµå¤„ç†ã€ç›®æ ‡æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«ã€äººè„¸è¯†åˆ«å’Œå£°å­¦äº‹ä»¶æ£€æµ‹èƒ½åŠ›",
    version="2.0.0",
    lifespan=lifespan
)

# æ·»åŠ CORSä¸­é—´ä»¶ä»¥æ”¯æŒå‰ç«¯è·¨åŸŸè¯·æ±‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173"],  # å…è®¸å‰ç«¯åŸŸå
    allow_credentials=True,
    allow_methods=["*"],  # å…è®¸æ‰€æœ‰HTTPæ–¹æ³•
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰å¤´éƒ¨
)


# --- æ•°æ®æ¨¡å‹ (ä¿æŒä¸å˜) ---

class StreamConfig(BaseModel):
    camera_id: str
    stream_url: str
    enable_face_recognition: bool = True
    enable_behavior_detection: bool = True


class FaceData(BaseModel):
    person_name: str
    image_data: str


# --- æ ¸å¿ƒå‡½æ•° (ä¿æŒä¸å˜) ---

def send_result_to_backend(result: AIAnalysisResult):
    """å°†åˆ†æç»“æœå¼‚æ­¥å‘é€åˆ°åç«¯DjangoæœåŠ¡ã€‚"""

    def task():
        backend_url = os.getenv("DJANGO_BACKEND_URL", "http://127.0.0.1:8000/api/alerts/ai-results/")
        try:
            response = requests.post(backend_url, json=result.model_dump(), timeout=10)
            if 200 <= response.status_code < 300:
                print(f"âœ… [ç»“æœä¸ŠæŠ¥] æˆåŠŸå‘é€äº‹ä»¶ '{result.event_type}' åˆ°åç«¯ã€‚")
            else:
                print(f"âŒ [ç»“æœä¸ŠæŠ¥] å‘é€å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, åç«¯å“åº”: {response.text}")
        except requests.exceptions.RequestException as e:
            print(f"âŒ [è¯·æ±‚å¼‚å¸¸] æ— æ³•è¿æ¥åˆ°åç«¯æœåŠ¡: {e}")

    thread_pool.submit(task)


async def run_acoustic_analysis():
    """åœ¨åå°æŒç»­è¿è¡Œçš„åç¨‹ï¼Œç”¨äºåˆ†æéŸ³é¢‘æ•°æ®ã€‚"""
    print("å£°å­¦åˆ†æåå°ä»»åŠ¡å·²å¯åŠ¨ã€‚")
    acoustic_detector = detectors.get("acoustic")
    if not acoustic_detector: 
        return

    while True:  # æŒç»­è¿è¡Œï¼Œç›´åˆ°æœåŠ¡åœæ­¢
        try:
            for stream in video_streams.values():
                # æ£€æŸ¥æµå¯¹è±¡æ˜¯å¦æ”¯æŒéŸ³é¢‘æ–‡ä»¶å¤„ç†
                if not hasattr(stream, 'get_audio_file'):
                    # è·³è¿‡ä¸æ”¯æŒéŸ³é¢‘çš„æµï¼ˆå¦‚WebcamProcessorï¼‰
                    continue
                    
                audio_file = stream.get_audio_file()
                if audio_file and os.path.exists(audio_file):
                    events = await acoustic_detector.process_audio_file(audio_file)
                    for event in events:
                        # æ ¹æ®äº‹ä»¶ç±»å‹é€‰æ‹©ä¸åŒçš„emoji
                        event_emoji = {
                            "volume_anomaly": "ğŸ“¢",
                            "high_frequency_noise": "ğŸ”Š",
                            "sudden_noise": "ğŸ’¥"
                        }.get(event['type'], "ğŸ””")
                        
                        print(f"{event_emoji} [éŸ³é¢‘] {event['description']}")
                        print(f"   - ç±»å‹: {event['type']}")
                        print(f"   - æ—¶é—´: {datetime.datetime.fromtimestamp(event['timestamp']).strftime('%H:%M:%S')}")
                        print(f"   - ç½®ä¿¡åº¦: {event['confidence']:.2f}")
                        
                        alert = AIAnalysisResult(
                            camera_id=stream.camera_id,
                            event_type=f"acoustic_{event['type']}",
                            location={"timestamp": event['timestamp']},
                            confidence=event['confidence'],
                            timestamp=datetime.datetime.now().isoformat(),
                            details={
                                "description": event['description'],
                                "audio_timestamp": event['timestamp']
                            }
                        )
                        send_result_to_backend(alert)
                else:
                    # åªå¯¹çœŸæ­£çš„è§†é¢‘æµæ˜¾ç¤ºéŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°çš„è­¦å‘Š
                    print(f"âš ï¸ æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶: {audio_file}")
        except Exception as e:
            print(f"å£°å­¦åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            traceback.print_exc()
        
        await asyncio.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡ï¼Œå‡å°‘æ£€æµ‹é¢‘ç‡


def create_master_processor(camera_id: str, config: StreamConfig):
    """
    åˆ›å»ºä¸€ä¸ªä¸»AIå¤„ç†å™¨ï¼Œå®ƒè´Ÿè´£åè°ƒè°ƒç”¨æ‰€æœ‰è§†è§‰ç›¸å…³çš„AIæ£€æµ‹å™¨ã€‚
    """

    def master_processor(frame: np.ndarray):
        try:
            # ç›®æ ‡æ£€æµ‹
            detected_objects = detectors["object"].predict(frame, confidence_threshold=0.6)

            # æ‰“å°æ£€æµ‹åˆ°çš„ç›®æ ‡
            for obj in detected_objects:
                print(f"ğŸ¯ æ£€æµ‹åˆ° {obj['class_name']}: ç½®ä¿¡åº¦={obj['confidence']:.2f}, ä½ç½®={obj['coordinates']}")

            # è¡Œä¸ºæ£€æµ‹
            if config.enable_behavior_detection:
                person_boxes = [obj["coordinates"] for obj in detected_objects if obj["class_name"] == "person"]
                if person_boxes:
                    behaviors = detectors["behavior"].detect_behavior(frame, person_boxes, time.time())
                    for behavior in behaviors:
                        if behavior["is_abnormal"] and behavior["need_alert"]:
                            print(f"ğŸš¨ [{camera_id}] æ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸º: {behavior['behavior']}!")
                            alert = AIAnalysisResult(
                                camera_id=camera_id,
                                event_type=f"abnormal_behavior_{behavior['behavior']}",
                                location={"box": behavior["box"]},
                                confidence=behavior["confidence"],
                                timestamp=datetime.datetime.now().isoformat(),
                            )
                            send_result_to_backend(alert)

            if config.enable_face_recognition:
                recognized_faces = detectors["face"].detect_and_recognize(frame)
                for face in recognized_faces:
                    if not face["identity"]["known"]:  # ä¿®æ”¹è¿™é‡Œï¼Œä½¿ç”¨æ–°çš„åˆ¤æ–­é€»è¾‘
                        print(f"ğŸš¨ [{camera_id}] æ£€æµ‹åˆ°æœªçŸ¥äººå‘˜!")
                        alert = AIAnalysisResult(
                            camera_id=camera_id,
                            event_type="unknown_face_detected",
                            location={
                                "box": [
                                    face["location"]["left"],
                                    face["location"]["top"],
                                    face["location"]["right"],
                                    face["location"]["bottom"]
                                ]
                            },
                            confidence=face.get("confidence", 0.9),
                            timestamp=datetime.datetime.now().isoformat(),
                            details={
                                "face_location": face["location"],
                                "best_match": face.get("best_match")
                            }
                        )
                        send_result_to_backend(alert)

        except Exception as e:
            print(f"å¤„ç†å¸§æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯ [{camera_id}]: {e}")

    return master_processor


# --- API ç«¯ç‚¹ (ä¿æŒä¸å˜) ---

@app.post("/stream/start/")
async def start_stream(
    camera_id: str = Body(...),
    stream_url: str = Body(...),
    enable_face_recognition: bool = Body(default=True),
    enable_behavior_detection: bool = Body(default=True),
    enable_sound_detection: bool = Body(default=True)  # é»˜è®¤å¯ç”¨å£°éŸ³æ£€æµ‹
):
    """å¯åŠ¨è§†é¢‘æµå¤„ç†ã€‚"""
    if camera_id in video_streams:
        return {"status": "error", "message": f"æ‘„åƒå¤´ {camera_id} å·²åœ¨è¿è¡Œ"}

    def master_processor(frame: np.ndarray):
        try:
            # ç›®æ ‡æ£€æµ‹ (æé«˜ç½®ä¿¡åº¦é˜ˆå€¼åˆ° 0.85)
            detected_objects = detectors["object"].predict(frame, confidence_threshold=0.85)
            
            # è¿‡æ»¤æ‰ä¸€äº›å¯èƒ½çš„è¯¯æŠ¥
            filtered_objects = []
            for obj in detected_objects:
                # 1. æ£€æŸ¥ç›®æ ‡å¤§å°æ˜¯å¦åˆç†
                box = obj["coordinates"]
                width = box[2] - box[0]
                height = box[3] - box[1]
                area_ratio = (width * height) / (frame.shape[0] * frame.shape[1])
                
                # å¦‚æœç›®æ ‡å æ®äº†è¶…è¿‡80%çš„ç”»é¢ï¼Œå¯èƒ½æ˜¯è¯¯æŠ¥
                if area_ratio > 0.8:
                    continue
                    
                # 2. å¯¹ç‰¹å®šç±»åˆ«åº”ç”¨æ›´ä¸¥æ ¼çš„ç½®ä¿¡åº¦è¦æ±‚
                if obj["class_name"] in ["bicycle", "sports ball", "bird", "traffic light"]:
                    if obj["confidence"] < 0.9:  # å¯¹è¿™äº›å®¹æ˜“è¯¯æŠ¥çš„ç±»åˆ«è¦æ±‚æ›´é«˜çš„ç½®ä¿¡åº¦
                        continue
                
                filtered_objects.append(obj)
            
            # äººè„¸è¯†åˆ«ï¼ˆæå‰è¿›è¡Œï¼Œä»¥ä¾¿ä¸äººç‰©æ£€æµ‹ç»“æœå…³è”ï¼‰
            recognized_faces = []
            if enable_face_recognition:
                recognized_faces = detectors["face"].detect_and_recognize(frame)

            # å¤„ç†æ£€æµ‹åˆ°çš„ç›®æ ‡
            for obj in filtered_objects:
                if obj["class_name"] == "person":
                    # å¯¹äººç‰©è¿›è¡Œèº«ä»½è¯†åˆ«
                    person_box = obj["coordinates"]
                    person_identity = "æœªçŸ¥äººå‘˜"
                    
                    for face in recognized_faces:
                        face_box = face["location"]
                        # è®¡ç®—äººè„¸æ¡†çš„ä¸­å¿ƒç‚¹ï¼ˆæ³¨æ„ï¼šface_boxæ ¼å¼ä¸º{top, right, bottom, left}ï¼‰
                        face_center_x = (face_box["left"] + face_box["right"]) / 2
                        face_center_y = (face_box["top"] + face_box["bottom"]) / 2
                        
                        # æ£€æŸ¥äººè„¸ä¸­å¿ƒç‚¹æ˜¯å¦åœ¨äººç‰©æ¡†å†…ï¼Œæ·»åŠ ä¸€äº›å®¹å·®
                        # æœ‰æ—¶YOLOçš„äººç‰©æ¡†å¯èƒ½æ¯”å®é™…ç•¥å°ï¼Œæ‰€ä»¥æˆ‘ä»¬æ‰©å¤§æ£€æŸ¥èŒƒå›´
                        box_width = person_box[2] - person_box[0]
                        box_height = person_box[3] - person_box[1]
                        tolerance_x = box_width * 0.1  # 10%çš„å®¹å·®
                        tolerance_y = box_height * 0.1
                        
                        if (face_center_x >= (person_box[0] - tolerance_x) and 
                            face_center_x <= (person_box[2] + tolerance_x) and
                            face_center_y >= (person_box[1] - tolerance_y) and 
                            face_center_y <= (person_box[3] + tolerance_y)):
                            if face["identity"]["known"]:
                                person_identity = face["identity"]["name"]
                                print(f"âœ… æˆåŠŸåŒ¹é…äººè„¸åˆ°äººç‰©æ¡†: {person_identity}")
                                print(f"   äººè„¸ä½ç½®: ({face_center_x:.0f}, {face_center_y:.0f})")
                                print(f"   äººç‰©æ¡†: {person_box}")
                            break
                    
                    print(f"ğŸ¯ æ£€æµ‹åˆ°äººå‘˜ [{person_identity}]: ç½®ä¿¡åº¦={obj['confidence']:.2f}, ä½ç½®={obj['coordinates']}")
                    
                    # å¦‚æœæ˜¯æœªçŸ¥äººå‘˜ï¼Œå‘é€å‘Šè­¦
                    if person_identity == "æœªçŸ¥äººå‘˜":
                        alert = AIAnalysisResult(
                            camera_id=camera_id,
                            event_type="unknown_person_detected",
                            location={"box": person_box},
                            confidence=obj["confidence"],
                            timestamp=datetime.datetime.now().isoformat(),
                        )
                        send_result_to_backend(alert)
                else:
                    # å…¶ä»–ç‰©ä½“åªæ˜¾ç¤ºåŸºæœ¬æ£€æµ‹ä¿¡æ¯
                    print(f"ğŸ¯ æ£€æµ‹åˆ°ç‰©ä½“ [{obj['class_name']}]: ç½®ä¿¡åº¦={obj['confidence']:.2f}, ä½ç½®={obj['coordinates']}")

            # è¡Œä¸ºæ£€æµ‹
            if enable_behavior_detection:
                person_boxes = [obj["coordinates"] for obj in filtered_objects if obj["class_name"] == "person"]
                if person_boxes:
                    behaviors = detectors["behavior"].detect_behavior(frame, person_boxes)  # ç§»é™¤time.time()å‚æ•°
                    for behavior in behaviors:
                        if behavior["is_abnormal"] and behavior["need_alert"]:
                            print(f"ğŸš¨ [{camera_id}] æ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸º: {behavior['behavior']}!")
                            alert = AIAnalysisResult(
                                camera_id=camera_id,
                                event_type=f"abnormal_behavior_{behavior['behavior']}",
                                location={"box": behavior["box"]},
                                confidence=behavior["confidence"],
                                timestamp=datetime.datetime.now().isoformat(),
                            )
                            send_result_to_backend(alert)

            # äººè„¸è¯†åˆ«
            if enable_face_recognition:
                recognized_faces = detectors["face"].detect_and_recognize(frame)
                for face in recognized_faces:
                    if not face["identity"]["known"]:
                        print(f"ğŸš¨ [{camera_id}] æ£€æµ‹åˆ°æœªçŸ¥äººè„¸!")
                        alert = AIAnalysisResult(
                            camera_id=camera_id,
                            event_type="unknown_face_detected",
                            location=face["location"],
                            confidence=face.get("identity", {}).get("confidence", 0.5),  # ä½¿ç”¨ get æ–¹æ³•å®‰å…¨è·å–å€¼
                            timestamp=datetime.datetime.now().isoformat(),
                        )
                        send_result_to_backend(alert)

        except Exception as e:
            print(f"å¤„ç†å™¨æ‰§è¡Œé”™è¯¯: {e}")
            traceback.print_exc()  # ç°åœ¨å¯ä»¥æ­£å¸¸ä½¿ç”¨ traceback

    try:
        # åˆ›å»ºè§†é¢‘æµå®ä¾‹
        stream = VideoStream(stream_url=stream_url, camera_id=camera_id)
        
        # å¦‚æœå¯ç”¨äº†å£°éŸ³æ£€æµ‹ï¼Œå¯åŠ¨éŸ³é¢‘æå–
        if enable_sound_detection:
            await stream.start_audio_extraction()
        
        if not await stream.start():
            return {"status": "error", "message": "æ— æ³•å¯åŠ¨è§†é¢‘æµ"}

        # æ·»åŠ ä¸»å¤„ç†å™¨
        stream.add_processor(master_processor)
        
        # ä¿å­˜æµå®ä¾‹
        video_streams[camera_id] = stream
        
        # å¯åŠ¨å¼‚æ­¥å¤„ç†å¾ªç¯
        asyncio.create_task(process_video_stream_async(stream, camera_id))
        
        return {
            "status": "success",
            "message": f"æˆåŠŸå¯åŠ¨æ‘„åƒå¤´ {camera_id}",
            "stream_info": stream.get_stream_info()
        }
        
    except Exception as e:
        print(f"å¯åŠ¨è§†é¢‘æµæ—¶å‡ºé”™: {e}")
        return {"status": "error", "message": str(e)}

async def process_video_stream_async(stream: VideoStream, camera_id: str):
    """å¼‚æ­¥å¤„ç†è§†é¢‘æµ"""
    print(f"å¼€å§‹å¤„ç†è§†é¢‘æµ: {camera_id}")
    while stream.is_running:
        try:
            success, frame = await stream.read_frame()
            if not success:
                print(f"è¯»å–è§†é¢‘å¸§å¤±è´¥: {camera_id}")
                continue
                
            # å¤„ç†å¸§
            with stream.lock:
                for processor in stream.processors:
                    try:
                        processor(frame)
                    except Exception as e:
                        print(f"å¤„ç†å™¨æ‰§è¡Œé”™è¯¯: {e}")
            
            # æ§åˆ¶å¸§ç‡ (ä¼˜åŒ–ä¸ºæ›´é«˜é¢‘ç‡)
            await asyncio.sleep(0.02)  # çº¦50fps
            
        except Exception as e:
            print(f"è§†é¢‘æµå¤„ç†é”™è¯¯ [{camera_id}]: {e}")
            await asyncio.sleep(1)

@app.post("/stream/stop/{camera_id}")
async def stop_stream(camera_id: str):
    """åœæ­¢è§†é¢‘æµå¤„ç†ã€‚"""
    try:
        if camera_id in video_streams:
            # åœæ­¢è§†é¢‘æµå¤„ç†
            stream_processor = video_streams[camera_id]
            stream_processor.stop()
            del video_streams[camera_id]
            
            # å¦‚æœæ²¡æœ‰æ´»è·ƒçš„è§†é¢‘æµäº†ï¼Œåœæ­¢å£°éŸ³æ£€æµ‹
            if not video_streams and detectors.get("acoustic") and detectors["acoustic"].is_running:
                detectors["acoustic"].stop_listening()
                print("å£°éŸ³æ£€æµ‹å·²åœæ­¢")

            print(f"å·²åœæ­¢è§†é¢‘æµ: {stream_processor.stream_url}")
            return {"status": "success", "message": "è§†é¢‘æµå¤„ç†å·²åœæ­¢"}
        else:
            return {"status": "error", "message": f"æœªæ‰¾åˆ°æ‘„åƒå¤´ {camera_id} çš„è§†é¢‘æµ"}

    except Exception as e:
        print(f"åœæ­¢è§†é¢‘æµæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return {"status": "error", "message": str(e)}


@app.post("/face/register/")
async def register_face(face_data: FaceData):
    """æ³¨å†Œæ–°çš„äººè„¸åˆ°äººè„¸è¯†åˆ«å™¨ä¸­ã€‚"""
    try:
        image_bytes = base64.b64decode(face_data.image_data)
        image_array = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
        if image is None:
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„Base64å›¾åƒæ•°æ®ã€‚")

        # è°ƒç”¨äººè„¸è¯†åˆ«å™¨çš„æ·»åŠ æ–¹æ³• (éœ€åœ¨FaceRecognizerç±»ä¸­å®ç°æ­¤æ–¹æ³•)
        # if detectors["face"].add_face(image, face_data.person_name):
        #     return {"status": "success", "message": f"äººè„¸ '{face_data.person_name}' æ³¨å†ŒæˆåŠŸã€‚"}
        # else:
        #     raise HTTPException(status_code=400, detail="æ³¨å†Œå¤±è´¥ï¼Œå¯èƒ½æœªåœ¨å›¾åƒä¸­æ£€æµ‹åˆ°äººè„¸ã€‚")

        # æš‚æ—¶è¿”å›æˆåŠŸï¼Œå¾…FaceRecognizerå®ç°add_face
        print(f"æ”¶åˆ°äººè„¸æ³¨å†Œè¯·æ±‚: {face_data.person_name}")
        return {"status": "success", "message": "äººè„¸æ³¨å†Œè¯·æ±‚å·²æ”¶åˆ° (åŠŸèƒ½å¾…å®ç°)ã€‚"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/system/status/")
async def get_system_status():
    """è·å–æ•´ä¸ªAIæœåŠ¡çš„å½“å‰çŠ¶æ€ã€‚"""
    return {
        "active_streams_count": len(video_streams),
        "detectors_initialized": {name: det is not None for name, det in detectors.items()},
        "active_streams_details": {
            cam_id: stream.get_stream_info() for cam_id, stream in video_streams.items()
        }
    }


@app.post("/audio/settings/")
async def update_audio_settings(
    confidence_threshold: float = Body(default=None),
    detection_interval: float = Body(default=None), 
    event_cooldown: float = Body(default=None),
    sensitivity: str = Body(default=None)  # "low", "medium", "high"
):
    """æ›´æ–°éŸ³é¢‘æ£€æµ‹è®¾ç½®"""
    try:
        acoustic_detector = detectors.get("acoustic")
        if not acoustic_detector:
            return {"status": "error", "message": "éŸ³é¢‘æ£€æµ‹å™¨æœªåˆå§‹åŒ–"}
            
        # éªŒè¯æ•æ„Ÿåº¦å‚æ•°
        if sensitivity is not None and sensitivity not in ["low", "medium", "high"]:
            return {"status": "error", "message": "æ•æ„Ÿåº¦å¿…é¡»æ˜¯ 'low', 'medium' æˆ– 'high'"}
            
        # æ›´æ–°è®¾ç½®
        acoustic_detector.update_settings(
            confidence_threshold=confidence_threshold,
            detection_interval=detection_interval,
            event_cooldown=event_cooldown,
            sensitivity=sensitivity
        )
        
        return {
            "status": "success", 
            "message": "éŸ³é¢‘æ£€æµ‹è®¾ç½®å·²æ›´æ–°",
            "current_settings": {
                "confidence_threshold": acoustic_detector.confidence_threshold,
                "detection_interval": acoustic_detector.detection_interval,
                "event_cooldown": acoustic_detector.event_cooldown,
                "volume_multiplier": acoustic_detector.volume_multiplier,
                "frequency_multiplier": acoustic_detector.frequency_multiplier,
                "noise_multiplier": acoustic_detector.noise_multiplier
            }
        }
        
    except Exception as e:
        return {"status": "error", "message": f"æ›´æ–°è®¾ç½®å¤±è´¥: {str(e)}"}


@app.get("/audio/settings/")
async def get_audio_settings():
    """è·å–å½“å‰éŸ³é¢‘æ£€æµ‹è®¾ç½®"""
    try:
        acoustic_detector = detectors.get("acoustic")
        if not acoustic_detector:
            return {"status": "error", "message": "éŸ³é¢‘æ£€æµ‹å™¨æœªåˆå§‹åŒ–"}
            
        return {
            "status": "success",
            "settings": {
                "confidence_threshold": acoustic_detector.confidence_threshold,
                "detection_interval": acoustic_detector.detection_interval,
                "event_cooldown": acoustic_detector.event_cooldown,
                "volume_multiplier": acoustic_detector.volume_multiplier,
                "frequency_multiplier": acoustic_detector.frequency_multiplier,
                "noise_multiplier": acoustic_detector.noise_multiplier
            }
        }
        
    except Exception as e:
        return {"status": "error", "message": f"è·å–è®¾ç½®å¤±è´¥: {str(e)}"}


@app.post("/audio/reset/")
async def reset_audio_history():
    """é‡ç½®éŸ³é¢‘äº‹ä»¶å†å²"""
    try:
        acoustic_detector = detectors.get("acoustic")
        if not acoustic_detector:
            return {"status": "error", "message": "éŸ³é¢‘æ£€æµ‹å™¨æœªåˆå§‹åŒ–"}
            
        acoustic_detector.reset_event_history()
        return {"status": "success", "message": "éŸ³é¢‘äº‹ä»¶å†å²å·²é‡ç½®"}
        
    except Exception as e:
        return {"status": "error", "message": f"é‡ç½®å¤±è´¥: {str(e)}"}


@app.post("/audio/frontend/alert/")
async def process_frontend_audio_alert(
    camera_id: str = Body(...),
    audio_level: float = Body(...),
    event_type: str = Body(default="high_volume"),
    timestamp: str = Body(default=None)
):
    """å¤„ç†å‰ç«¯å‘é€çš„éŸ³é¢‘å‘Šè­¦äº‹ä»¶"""
    try:
        if not timestamp:
            timestamp = datetime.datetime.now().isoformat()
            
        # åˆ›å»ºéŸ³é¢‘å‘Šè­¦ç»“æœ
        alert_result = AIAnalysisResult(
            camera_id=camera_id,
            event_type=f"frontend_{event_type}",
            location={"audio_level": audio_level},
            confidence=min(audio_level / 100.0, 1.0),  # å°†éŸ³é‡çº§åˆ«è½¬æ¢ä¸ºç½®ä¿¡åº¦
            timestamp=timestamp,
            details={
                "source": "frontend_audio_detection",
                "audio_level": audio_level,
                "event_type": event_type
            }
        )
        
        # å‘é€åˆ°åç«¯
        send_result_to_backend(alert_result)
        
        return {
            "status": "success", 
            "message": f"éŸ³é¢‘å‘Šè­¦å·²å¤„ç†: {event_type}, éŸ³é‡çº§åˆ«: {audio_level}%"
        }
        
    except Exception as e:
        return {"status": "error", "message": f"å¤„ç†éŸ³é¢‘å‘Šè­¦å¤±è´¥: {str(e)}"}


@app.post("/frame/analyze/")
async def analyze_frame(
    frame: UploadFile = File(...),
    camera_id: str = Body(...),
    enable_face_recognition: bool = Body(default=True),
    enable_object_detection: bool = Body(default=True),
    enable_behavior_detection: bool = Body(default=False)
):
    """é«˜æ€§èƒ½å•å¸§å›¾åƒåˆ†æ"""
    try:
        # è¯»å–å›¾åƒæ•°æ®
        image_data = await frame.read()
        image_array = np.frombuffer(image_data, np.uint8)
        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
        
        if image is None:
            return {"status": "error", "message": "æ— æ•ˆçš„å›¾åƒæ•°æ®"}
        
        # è·å–å›¾åƒå°ºå¯¸ï¼Œå¦‚æœå›¾åƒå¤ªå°ï¼Œè·³è¿‡æŸäº›æ£€æµ‹ä»¥æé«˜æ€§èƒ½
        height, width = image.shape[:2]
        is_low_res = width < 300 or height < 300
        
        results = {
            "camera_id": camera_id,
            "timestamp": datetime.datetime.now().isoformat(),
            "detections": [],
            "alerts": []
        }
        
        # é«˜æ€§èƒ½ç›®æ ‡æ£€æµ‹
        if enable_object_detection:
            # æ ¹æ®å›¾åƒè´¨é‡è°ƒæ•´æ£€æµ‹ç­–ç•¥
            confidence_threshold = 0.8 if is_low_res else 0.7  # ä½åˆ†è¾¨ç‡æ—¶æé«˜é˜ˆå€¼å‡å°‘è¯¯æ£€
            
            try:
                detected_objects = detectors["object"].predict(image, confidence_threshold=confidence_threshold)
                
                # é™åˆ¶æ£€æµ‹ç»“æœæ•°é‡ä»¥æé«˜å¤„ç†é€Ÿåº¦
                detected_objects = detected_objects[:5] if is_low_res else detected_objects[:10]
                
                for obj in detected_objects:
                    # ç¡®ä¿åæ ‡è½¬æ¢ä¸ºPythonåŸç”Ÿintç±»å‹
                    bbox = [int(float(coord)) for coord in obj["coordinates"]]
                    detection = {
                        "type": "object",
                        "class_name": obj["class_name"],
                        "confidence": float(obj["confidence"]),
                        "bbox": bbox,
                        "timestamp": datetime.datetime.now().isoformat()
                    }
                    results["detections"].append(detection)
                    
                    # å¦‚æœæ£€æµ‹åˆ°äººå‘˜ï¼Œç”Ÿæˆå‘Šè­¦
                    if obj["class_name"] == "person" and obj["confidence"] > 0.8:
                        alert = {
                            "type": "person_detected",
                            "message": f"æ£€æµ‹åˆ°äººå‘˜ (ç½®ä¿¡åº¦: {obj['confidence']:.2f})",
                            "confidence": float(obj["confidence"]),
                            "location": bbox  # ä½¿ç”¨å·²è½¬æ¢çš„bbox
                        }
                        results["alerts"].append(alert)
            except Exception as e:
                print(f"ç›®æ ‡æ£€æµ‹å¤±è´¥: {e}")
                # ç›®æ ‡æ£€æµ‹å¤±è´¥æ—¶ä¸å½±å“å…¶ä»–åŠŸèƒ½ç»§ç»­è¿è¡Œ
        
        # ä¼˜åŒ–çš„äººè„¸è¯†åˆ«
        if enable_face_recognition and not is_low_res:  # ä½åˆ†è¾¨ç‡æ—¶è·³è¿‡äººè„¸è¯†åˆ«ä»¥æé«˜æ€§èƒ½
            try:
                recognized_faces = detectors["face"].detect_and_recognize(image)
                
                # é™åˆ¶äººè„¸æ£€æµ‹ç»“æœæ•°é‡
                recognized_faces = recognized_faces[:3]  # æœ€å¤šå¤„ç†3ä¸ªäººè„¸
                
                for face in recognized_faces:
                    # ç¡®ä¿äººè„¸åæ ‡è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
                    face_bbox = [
                        int(float(face["location"]["left"])),
                        int(float(face["location"]["top"])),
                        int(float(face["location"]["right"])),
                        int(float(face["location"]["bottom"]))
                    ]
                    # åˆ›å»ºä¸€ä¸ªå¹²å‡€çš„locationå­—å…¸
                    clean_location = {
                        "left": int(float(face["location"]["left"])),
                        "top": int(float(face["location"]["top"])),
                        "right": int(float(face["location"]["right"])),
                        "bottom": int(float(face["location"]["bottom"]))
                    }
                    
                    detection = {
                        "type": "face",
                        "known": face["identity"]["known"],
                        "name": face["identity"].get("name", "æœªçŸ¥"),
                        "confidence": float(face.get("confidence", 0.5)),
                        "bbox": face_bbox,
                        "timestamp": datetime.datetime.now().isoformat()
                    }
                    results["detections"].append(detection)
                    
                    # å¦‚æœæ˜¯æœªçŸ¥äººè„¸ï¼Œç”Ÿæˆå‘Šè­¦
                    if not face["identity"]["known"]:
                        alert = {
                            "type": "unknown_face",
                            "message": "æ£€æµ‹åˆ°æœªçŸ¥äººè„¸",
                            "confidence": float(face.get("confidence", 0.5)),
                            "location": clean_location  # ä½¿ç”¨å·²æ¸…ç†çš„location
                        }
                        results["alerts"].append(alert)
                        
                        # å¼‚æ­¥å‘é€åˆ°åç«¯ï¼ˆä¸é˜»å¡å½“å‰å¤„ç†ï¼‰
                        backend_alert = AIAnalysisResult(
                            camera_id=camera_id,
                            event_type="unknown_face_detected",
                            location=clean_location,  # ä½¿ç”¨å·²æ¸…ç†çš„location
                            confidence=float(face.get("confidence", 0.5)),
                            timestamp=datetime.datetime.now().isoformat(),
                            details={"realtime_detection": True}
                        )
                        # ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥å¤„ç†ï¼Œä¸é˜»å¡å“åº”
                        import threading
                        threading.Thread(target=lambda: send_result_to_backend(backend_alert), daemon=True).start()
            except Exception as e:
                print(f"äººè„¸è¯†åˆ«å¤±è´¥: {e}")
                # äººè„¸è¯†åˆ«å¤±è´¥æ—¶ä¸å½±å“å…¶ä»–åŠŸèƒ½ç»§ç»­è¿è¡Œ
        
        return {"status": "success", "results": results}
        
    except Exception as e:
        print(f"åˆ†æå¸§æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return {"status": "error", "message": f"åˆ†æå¤±è´¥: {str(e)}"}


@app.get("/stream/webcam/start/{camera_id}")
async def start_webcam_stream(camera_id: str):
    """å¯åŠ¨ç½‘ç»œæ‘„åƒå¤´æµå¤„ç†ï¼ˆç”¨äºå‰ç«¯æ‘„åƒå¤´ï¼‰"""
    try:
        # ä¸ºç½‘ç»œæ‘„åƒå¤´åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„è§†é¢‘æµå¤„ç†å™¨
        if camera_id not in video_streams:
            # åˆ›å»ºè™šæ‹Ÿæµå¤„ç†å™¨
            class WebcamProcessor:
                def __init__(self, camera_id):
                    self.camera_id = camera_id
                    self.is_running = True
                    self.frame_count = 0
                    
                def get_status(self):
                    return {
                        "camera_id": self.camera_id,
                        "status": "running" if self.is_running else "stopped",
                        "type": "webcam",
                        "frame_count": self.frame_count
                    }
                    
                def stop(self):
                    self.is_running = False
                    
                def process_frame(self):
                    self.frame_count += 1
            
            video_streams[camera_id] = WebcamProcessor(camera_id)
        
        return {
            "status": "success",
            "message": f"ç½‘ç»œæ‘„åƒå¤´æµ {camera_id} å·²å¯åŠ¨",
            "camera_id": camera_id
        }
        
    except Exception as e:
        return {"status": "error", "message": f"å¯åŠ¨å¤±è´¥: {str(e)}"}


@app.post("/stream/webcam/stop/{camera_id}")
async def stop_webcam_stream(camera_id: str):
    """åœæ­¢ç½‘ç»œæ‘„åƒå¤´æµå¤„ç†"""
    try:
        if camera_id in video_streams:
            video_streams[camera_id].stop()
            del video_streams[camera_id]
            return {"status": "success", "message": f"ç½‘ç»œæ‘„åƒå¤´æµ {camera_id} å·²åœæ­¢"}
        else:
            return {"status": "error", "message": f"æœªæ‰¾åˆ°æ‘„åƒå¤´æµ: {camera_id}"}
            
    except Exception as e:
        return {"status": "error", "message": f"åœæ­¢å¤±è´¥: {str(e)}"}


@app.get("/performance/optimize/")
async def get_performance_tips():
    """è·å–æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
    try:
        tips = []
        
        # æ£€æŸ¥æ£€æµ‹å™¨çŠ¶æ€
        if "object" in detectors:
            tips.append({
                "type": "info",
                "title": "ç›®æ ‡æ£€æµ‹ä¼˜åŒ–",
                "description": "å·²å¯ç”¨æ™ºèƒ½æ£€æµ‹é˜ˆå€¼è°ƒæ•´å’Œç»“æœæ•°é‡é™åˆ¶"
            })
        
        if "face" in detectors:
            tips.append({
                "type": "info", 
                "title": "äººè„¸è¯†åˆ«ä¼˜åŒ–",
                "description": "ä½åˆ†è¾¨ç‡å›¾åƒè‡ªåŠ¨è·³è¿‡äººè„¸è¯†åˆ«ï¼Œæœ€å¤šå¤„ç†3ä¸ªäººè„¸"
            })
            
        tips.extend([
            {
                "type": "success",
                "title": "æ€§èƒ½æ¨¡å¼å·²æ¿€æ´»",
                "description": "AIæœåŠ¡å·²å¯ç”¨é«˜æ€§èƒ½å¤„ç†æ¨¡å¼ï¼ŒåŒ…æ‹¬å¼‚æ­¥åç«¯é€šä¿¡å’Œé”™è¯¯å®¹é”™æœºåˆ¶"
            },
            {
                "type": "warning",
                "title": "ç½‘ç»œä¼˜åŒ–å»ºè®®",
                "description": "å‰ç«¯å·²å¯ç”¨å¸§å·®æ£€æµ‹ã€åŠ¨æ€ç”»è´¨è°ƒæ•´å’Œæ™ºèƒ½è·³å¸§æœºåˆ¶"
            }
        ])
        
        return {
            "status": "success",
            "tips": tips,
            "performance_mode": "high_performance",
            "optimizations": [
                "åŠ¨æ€æ£€æµ‹é˜ˆå€¼",
                "ç»“æœæ•°é‡é™åˆ¶", 
                "ä½åˆ†è¾¨ç‡è·³è¿‡",
                "å¼‚æ­¥åç«¯é€šä¿¡",
                "é”™è¯¯å®¹é”™æœºåˆ¶"
            ]
        }
        
    except Exception as e:
        return {"status": "error", "message": f"è·å–æ€§èƒ½ä¿¡æ¯å¤±è´¥: {str(e)}"}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)