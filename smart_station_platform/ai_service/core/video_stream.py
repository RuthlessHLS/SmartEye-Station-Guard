# æ–‡ä»¶: ai_service/core/video_stream.py
import cv2
import time
import threading
import numpy as np
from typing import List, Callable, Optional, Dict
from datetime import datetime
import traceback
from models.alert_models import AIAnalysisResult  # ä¿®æ”¹ä¸ºç»å¯¹å¯¼å…¥
import logging
from datetime import datetime
import ffmpeg
import tempfile
import os
import asyncio
from typing import Optional, Tuple, Dict

logger = logging.getLogger(__name__)


def process_video_stream(video_url: str):
    """
    è¿æ¥åˆ°è§†é¢‘æµå¹¶é€å¸§äº§ç”Ÿå›¾åƒã€‚
    è¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å‡½æ•°ï¼Œå¯ä»¥è¢«å¾ªç¯è°ƒç”¨ã€‚
    """
    cap = cv2.VideoCapture(video_url)
    if not cap.isOpened():
        print(f"é”™è¯¯: æ— æ³•æ‰“å¼€è§†é¢‘æµ {video_url}")
        return  # å¦‚æœæ‰“ä¸å¼€ï¼Œå°±ç»“æŸ

    print(f"æˆåŠŸè¿æ¥åˆ°è§†é¢‘æµ: {video_url}")
    while True:
        ret, frame = cap.read()
        # å¦‚æœè¯»å–å¤±è´¥ (ret is False)
        if not ret:
            print("è§†é¢‘æµç»“æŸæˆ–å‘ç”Ÿé”™è¯¯ï¼Œ5ç§’åå°è¯•é‡è¿...")
            time.sleep(5)
            cap.release()  # é‡Šæ”¾æ—§çš„è¿æ¥
            cap = cv2.VideoCapture(video_url)  # å°è¯•é‡æ–°è¿æ¥
            continue  # ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯

        # å¦‚æœè¯»å–æˆåŠŸï¼Œä½¿ç”¨ yield å°†è¿™ä¸€å¸§å›¾åƒ"ç”Ÿäº§"å‡ºå»
        yield frame

    # å¾ªç¯ç»“æŸåï¼ˆç†è®ºä¸Šå¯¹äºå®æ—¶æµä¸ä¼šç»“æŸï¼‰ï¼Œé‡Šæ”¾èµ„æº
    cap.release()


class VideoStream:
    """
    è§†é¢‘æµå¤„ç†ç±»ï¼Œè´Ÿè´£ç®¡ç†è§†é¢‘æµçš„æ•è·å’ŒAIå¤„ç†
    """
    
    def __init__(self, stream_url: str, camera_id: str):
        """
        åˆå§‹åŒ–è§†é¢‘æµ
        
        Args:
            stream_url (str): è§†é¢‘æµURL
            acoustic_detector: å£°éŸ³æ£€æµ‹å™¨å®ä¾‹
        """
        self.stream_url = stream_url
        self.camera_id = camera_id
        self.cap = None
        self.is_running = False
        self.temp_audio_file = None
        self.audio_process = None
        self.processors: List[Callable[[np.ndarray], None]] = []
        self.lock = threading.Lock()
        self.acoustic_detector = None # This attribute is no longer used for audio processing
    
    def add_processor(self, processor: Callable[[np.ndarray], None]):
        """
        æ·»åŠ å¸§å¤„ç†å™¨
        
        Args:
            processor: æ¥å—frameä½œä¸ºå‚æ•°çš„å¤„ç†å‡½æ•°
        """
        with self.lock:
            self.processors.append(processor)
    
    async def start(self) -> bool:
        """
        å¯åŠ¨è§†é¢‘æµå¤„ç†
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸå¯åŠ¨
        """
        if self.is_running:
            print(f"è§†é¢‘æµ {self.stream_url} å·²åœ¨è¿è¡Œä¸­")
            return True
        
        # å°è¯•è¿æ¥è§†é¢‘æµ
        self.cap = cv2.VideoCapture(self.stream_url)
        if not self.cap.isOpened():
            print(f"é”™è¯¯: æ— æ³•æ‰“å¼€è§†é¢‘æµ {self.stream_url}")
            return False
        
        # å¦‚æœæ˜¯æœ¬åœ°è§†é¢‘æ–‡ä»¶ä¸”æœ‰å£°éŸ³æ£€æµ‹å™¨ï¼Œå¯åŠ¨éŸ³é¢‘å¤„ç†
        # if self.acoustic_detector and self.stream_url.startswith(('G:/', 'C:/', 'D:/', 'E:/', 'F:/')):
        #     print("æ£€æµ‹åˆ°æœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼Œå¯åŠ¨éŸ³é¢‘å¤„ç†...")
        #     self.acoustic_detector.start_video_audio_processing(self.stream_url)
        
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        self.is_running = True
        # self.thread = threading.Thread(target=self._process_loop, daemon=True) # This line is removed
        # self.thread.start() # This line is removed
        
        print(f"æˆåŠŸå¯åŠ¨è§†é¢‘æµ: {self.stream_url}")
        return True
    
    async def start_audio_extraction(self):
        """å¯åŠ¨éŸ³é¢‘æå–è¿‡ç¨‹"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶æ¥å­˜å‚¨éŸ³é¢‘
            temp_dir = tempfile.gettempdir()
            self.temp_audio_file = os.path.join(temp_dir, f"audio_{self.camera_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°æ–‡ä»¶
            is_local_file = self.stream_url.startswith(('file:///', 'G:/', 'C:/', 'D:/', 'E:/', 'F:/'))
            if is_local_file:
                stream_url = self.stream_url.replace('file:///', '')
            else:
                stream_url = self.stream_url
            
            print(f"å¼€å§‹ä» {stream_url} æå–éŸ³é¢‘...")
            print(f"éŸ³é¢‘å°†ä¿å­˜åˆ°: {self.temp_audio_file}")
            
            # ä½¿ç”¨ffmpegæå–éŸ³é¢‘
            stream = ffmpeg.input(stream_url)
            audio = stream.audio
            stream = ffmpeg.output(audio, self.temp_audio_file, 
                                 acodec='pcm_s16le',  # 16ä½PCMæ ¼å¼
                                 ac=1,                # å•å£°é“
                                 ar='44100',          # 44.1kHzé‡‡æ ·ç‡
                                 loglevel='error')    # åªæ˜¾ç¤ºé”™è¯¯æ—¥å¿—
                                 
            # å¯åŠ¨ffmpegè¿›ç¨‹
            self.audio_process = await asyncio.create_subprocess_exec(
                *ffmpeg.compile(stream),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            print("âœ… éŸ³é¢‘æå–è¿›ç¨‹å·²å¯åŠ¨")
            
        except Exception as e:
            print(f"âŒ å¯åŠ¨éŸ³é¢‘æå–æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            traceback.print_exc()
            self.temp_audio_file = None
            
    def get_audio_file(self) -> Optional[str]:
        """è·å–å½“å‰æ­£åœ¨å†™å…¥çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„"""
        return self.temp_audio_file
            
    async def read_frame(self) -> Tuple[bool, Optional[np.ndarray]]:
        if not self.is_running or not self.cap:
            return False, None
            
        try:
            ret, frame = self.cap.read()
            if not ret:
                logger.warning(f"æ— æ³•è¯»å–è§†é¢‘å¸§: {self.stream_url}")
                return False, None
            return True, frame
        except Exception as e:
            logger.error(f"è¯»å–è§†é¢‘å¸§æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False, None
    
    def stop(self):
        """
        åœæ­¢è§†é¢‘æµå¤„ç†
        """
        self.is_running = False
        
        # åœæ­¢éŸ³é¢‘å¤„ç†
        if self.acoustic_detector:
            self.acoustic_detector.stop_video_audio_processing()
        
        # The following lines are removed as per the new_code:
        # if self.thread and self.thread.is_alive():
        #     self.thread.join(timeout=5)
        
        if self.cap:
            self.cap.release()
            self.cap = None
        
        print(f"å·²åœæ­¢è§†é¢‘æµ: {self.stream_url}")
    
    def _process_loop(self):
        """
        è§†é¢‘å¤„ç†ä¸»å¾ªç¯
        """
        retry_count = 0
        max_retries = 5
        
        while self.is_running:
            try:
                ret, frame = self.cap.read()
                
                if not ret:
                    retry_count += 1
                    print(f"è§†é¢‘æµè¯»å–å¤±è´¥ï¼Œé‡è¯• {retry_count}/{max_retries}")
                    
                    if retry_count >= max_retries:
                        print("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå°è¯•é‡æ–°è¿æ¥...")
                        self.cap.release()
                        self.cap = cv2.VideoCapture(self.stream_url)
                        retry_count = 0
                    
                    time.sleep(1)
                    continue
                
                # é‡ç½®é‡è¯•è®¡æ•°
                retry_count = 0
                
                # å¤„ç†å¸§
                with self.lock:
                    for processor in self.processors:
                        try:
                            processor(frame)
                        except Exception as e:
                            print(f"å¤„ç†å™¨æ‰§è¡Œé”™è¯¯: {e}")
                
                # æ§åˆ¶å¸§ç‡ï¼Œé¿å…è¿‡åº¦æ¶ˆè€—CPU
                time.sleep(0.033)  # çº¦30fps
                
            except Exception as e:
                print(f"è§†é¢‘æµå¤„ç†å¾ªç¯é”™è¯¯: {e}")
                time.sleep(1)
    
    def get_stream_info(self) -> dict:
        """
        è·å–è§†é¢‘æµä¿¡æ¯
        
        Returns:
            dict: åŒ…å«è§†é¢‘æµçŠ¶æ€ä¿¡æ¯çš„å­—å…¸
        """
        info = {
            "stream_url": self.stream_url,
            "is_running": self.is_running,
            "processors_count": len(self.processors),
            "has_audio_processing": self.acoustic_detector is not None
        }
        
        if self.cap and self.cap.isOpened():
            info.update({
                "width": int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
                "height": int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
                "fps": self.cap.get(cv2.CAP_PROP_FPS)
            })
        
        return info

    def process_frame(self, frame: np.ndarray) -> Dict:
        """å¤„ç†å•å¸§å›¾åƒã€‚"""
        results = {
            "faces": [],
            "objects": [],
            "behaviors": [],
            "alerts": []
        }
        
        try:
            # 1. äººè„¸æ£€æµ‹å’Œè¯†åˆ«
            if self.face_recognizer:
                face_results = self.face_recognizer.detect_and_recognize(frame)
                results["faces"] = face_results
                
                # æ£€æŸ¥æ¯ä¸ªæœªçŸ¥äººè„¸å¹¶ç”Ÿæˆå‘Šè­¦
                for face in face_results:
                    if face["alert_needed"]:
                        alert = AIAnalysisResult(
                            camera_id=self.camera_id,
                            event_type="unknown_face_detected",
                            timestamp=face["detection_time"],
                            location={
                                "box": [
                                    face["location"]["left"],
                                    face["location"]["top"],
                                    face["location"]["right"],
                                    face["location"]["bottom"]
                                ],
                                "description": "æ‘„åƒå¤´è§†é‡å†…"
                            },
                            confidence=face["confidence"],
                            details={
                                "best_match_info": face["best_match"] if face["best_match"] else None,
                                "face_location": face["location"]
                            }
                        )
                        
                        # å‘é€æŠ¥è­¦åˆ°åç«¯
                        try:
                            self.alert_sender.send_alert(alert)
                            print(f"ğŸš¨ æœªçŸ¥äººå‘˜æŠ¥è­¦å·²å‘é€! ä½ç½®: {face['location']}")
                        except Exception as e:
                            print(f"å‘é€æœªçŸ¥äººå‘˜æŠ¥è­¦å¤±è´¥: {str(e)}")
                            traceback.print_exc()
            
            # 2. è¡Œä¸ºæ£€æµ‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.behavior_detector:
                behavior_results = self.behavior_detector.detect(frame)
                results["behaviors"] = behavior_results
            
            # 3. ç›®æ ‡æ£€æµ‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.object_detector:
                object_results = self.object_detector.detect(frame)
                results["objects"] = object_results
                
        except Exception as e:
            print(f"å¤„ç†å¸§æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            traceback.print_exc()
        
        return results

# å…¨å±€è§†é¢‘æµç®¡ç†å™¨
active_streams: Dict[str, VideoStream] = {}