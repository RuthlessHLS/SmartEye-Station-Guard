# æ–‡ä»¶: ai_service/core/face_recognition.py
# æè¿°: æ™ºèƒ½äººè„¸è¯†åˆ«å™¨ï¼Œç”¨äºæ£€æµ‹ã€ç¼–ç å’Œè¯†åˆ«äººè„¸ï¼Œæ”¯æŒåŠ¨æ€çµæ•åº¦é…ç½®å’Œæ–°é¢å­”æ³¨å†Œã€‚

import face_recognition
import numpy as np
import os
import cv2
from typing import List, Dict, Any, Optional, Tuple
import logging
from datetime import datetime
import asyncio
# from concurrent.futures import ThreadPoolExecutor  # ã€ç§»é™¤ã€‘ä¸å†éœ€è¦çº¿ç¨‹æ± 

# è·å–loggerå®ä¾‹
logger = logging.getLogger(__name__)

# ã€ç§»é™¤ã€‘ä¸å†éœ€è¦å…¨å±€çº¿ç¨‹æ± 
# _face_loading_thread_pool = ThreadPoolExecutor(max_workers=os.cpu_count() or 4)

def _calculate_eye_aspect_ratio(eye_landmarks: List[Tuple[int, int]]) -> float:
    """
    è®¡ç®—å•åªçœ¼ç›çš„çºµæ¨ªæ¯” (EAR - Eye Aspect Ratio)ã€‚
    è¿™ä¸ªå€¼åœ¨çœ¼ç›çå¼€æ—¶è¾ƒå¤§ï¼Œé—­åˆæ—¶æ¥è¿‘äº0ã€‚
    """
    # å‚ç›´è·ç¦»
    v_dist1 = np.linalg.norm(np.array(eye_landmarks[1]) - np.array(eye_landmarks[5]))
    v_dist2 = np.linalg.norm(np.array(eye_landmarks[2]) - np.array(eye_landmarks[4]))
    # æ°´å¹³è·ç¦»
    h_dist = np.linalg.norm(np.array(eye_landmarks[0]) - np.array(eye_landmarks[3]))
    
    # é¿å…é™¤ä»¥é›¶
    if h_dist == 0:
        return 0.0
        
    ear = (v_dist1 + v_dist2) / (2.0 * h_dist)
    return ear

class FaceRecognizer:
    def __init__(self, known_faces_dir, detection_model="hog", tolerance=0.5, jitter=1):
        """
        åˆå§‹åŒ–äººè„¸è¯†åˆ«å™¨ã€‚

        Args:
            known_faces_dir (str): åŒ…å«å·²çŸ¥äººè„¸å›¾ç‰‡çš„ç›®å½•è·¯å¾„ã€‚
                                ç›®å½•ç»“æ„åº”ä¸ºï¼šknown_faces_dir/person_name/image_files
        """
        self.known_faces = {}  # å­˜å‚¨æ¯ä¸ªäººçš„äººè„¸ç¼–ç  {name: [encodings]}
        self.enabled = True  # æ§åˆ¶æ£€æµ‹å™¨æ˜¯å¦å¯ç”¨

        # å†…éƒ¨é»˜è®¤æ£€æµ‹å‚æ•° (å¯è¢«å¤–éƒ¨é…ç½®è¦†ç›–)
        self._default_config = {
            'tolerance': 0.65,  # é»˜è®¤äººè„¸æ¯”å¯¹å®¹å¿åº¦
            'detection_model': 'auto',  # é»˜è®¤äººè„¸æ£€æµ‹æ¨¡å‹: 'hog', 'cnn', 'auto'
            'number_of_times_to_upsample': 2,  # æ£€æµ‹å‰å¯¹å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·æ¬¡æ•°
            'min_face_size': 40,  # æœ€å°äººè„¸å°ºå¯¸ (åƒç´ )ï¼Œå°äºæ­¤å°ºå¯¸çš„äººè„¸å°†è¢«å¿½ç•¥
        }
        self.current_config = self._default_config.copy()  # å½“å‰ç”Ÿæ•ˆçš„é…ç½®

        self.known_faces_dir = known_faces_dir
        self.known_face_encodings = []
        self.known_face_names = []
        self.tolerance = tolerance
        self.jitter = jitter

        # æ´»ä½“æ£€æµ‹é˜ˆå€¼
        self.EAR_THRESHOLD = 0.21  # çœ¨çœ¼æ£€æµ‹çš„EARé˜ˆå€¼
        self.HEAD_TURN_RATIO_THRESHOLD = 1.5  # è½¬å¤´æ£€æµ‹çš„é¼»å­-çœ¼ç›è·ç¦»æ¯”ç‡é˜ˆå€¼

        self._load_known_faces()

    def _load_known_faces(self):
        """
        æ ¸å¿ƒçš„åŠ è½½å‡½æ•°ï¼šæ‰«æç›®å½•ï¼Œè®¡ç®—ç¼–ç ã€‚è¿™æ˜¯ä¸€ä¸ªè€—æ—¶æ“ä½œã€‚
        """
        logger.info("=== å¼€å§‹æ‰«æäººè„¸åº“å¹¶ç”Ÿæˆç¼–ç ... ===")
        logger.info(f"ç›®å½•è·¯å¾„: {self.known_faces_dir}")

        temp_encodings = []
        temp_names = []

        if not os.path.isdir(self.known_faces_dir):
            logger.warning(f"å·²çŸ¥äººè„¸ç›®å½• '{self.known_faces_dir}' ä¸å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½ã€‚")
            return

        for person_name in os.listdir(self.known_faces_dir):
            person_dir = os.path.join(self.known_faces_dir, person_name)
            if not os.path.isdir(person_dir):
                continue
            
            for image_file in os.listdir(person_dir):
                if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    try:
                        image_path = os.path.join(person_dir, image_file)
                        image = face_recognition.load_image_file(image_path)
                        encodings = face_recognition.face_encodings(image)
                        if encodings:
                            temp_encodings.append(encodings[0])
                            temp_names.append(person_name)
                        else:
                            logger.warning(f"åœ¨å›¾ç‰‡ '{image_file}' ä¸­æœªæ‰¾åˆ°äººè„¸ï¼Œå·²è·³è¿‡ã€‚")
                    except Exception as e:
                        logger.error(f"å¤„ç†å›¾ç‰‡ '{image_file}' æ—¶å‡ºé”™: {e}", exc_info=True)
        
        # åŸå­æ›¿æ¢ï¼Œç¡®ä¿æœåŠ¡ä¸ä¼šåœ¨åŠ è½½è¿‡ç¨‹ä¸­ä½¿ç”¨ä¸å®Œæ•´çš„æ•°æ®
        self.known_face_encodings = temp_encodings
        self.known_face_names = temp_names
        logger.info(f"âœ… äººè„¸åº“åŠ è½½å®Œæˆã€‚å…±åŠ è½½ {len(self.known_face_encodings)} ä¸ªå·²çŸ¥äººè„¸ã€‚")

    async def reload_known_faces(self):
        """
        å¼‚æ­¥æ¥å£ï¼Œåœ¨åå°çº¿ç¨‹ä¸­å®‰å…¨åœ°æ‰§è¡Œè€—æ—¶çš„åŠ è½½ä»»åŠ¡ã€‚
        """
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, self._load_known_faces)

    def update_config(self, new_config: Dict[str, Any]):
        """
        æ›´æ–°äººè„¸è¯†åˆ«å™¨çš„é…ç½®å‚æ•°ã€‚
        Args:
            new_config (Dict[str, Any]): åŒ…å«è¦æ›´æ–°çš„é…ç½®é¡¹çš„å­—å…¸ã€‚
        """
        for key, value in new_config.items():
            if key in self.current_config:
                self.current_config[key] = value
                logger.info(f"æ›´æ–°äººè„¸è¯†åˆ«é…ç½®: {key} = {value}")
            else:
                logger.warning(f"å°è¯•æ›´æ–°ä¸å­˜åœ¨çš„é…ç½®é¡¹: {key}")
        logger.info(f"äººè„¸è¯†åˆ«å™¨å½“å‰é…ç½®: {self.current_config}")

    def set_enabled(self, enabled: bool):
        """å¯ç”¨æˆ–ç¦ç”¨äººè„¸è¯†åˆ«å™¨"""
        self.enabled = enabled
        logger.info(f"äººè„¸è¯†åˆ«å™¨å·²{'å¯ç”¨' if enabled else 'ç¦ç”¨'}")

    def add_face(self, image: np.ndarray, person_name: str) -> bool:
        """
        åœ¨è¿è¡Œæ—¶æ·»åŠ æ–°çš„äººè„¸åˆ°å·²çŸ¥äººè„¸æ•°æ®åº“ã€‚
        Args:
            image (np.ndarray): åŒ…å«æ–°äººè„¸çš„å›¾åƒ (BGRæ ¼å¼)ã€‚
            person_name (str): æ–°äººè„¸çš„åç§°ã€‚
        Returns:
            bool: å¦‚æœæˆåŠŸæ·»åŠ äººè„¸è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
        """
        if not self.enabled:
            logger.warning("äººè„¸è¯†åˆ«å™¨å·²ç¦ç”¨ï¼Œæ— æ³•æ·»åŠ æ–°é¢å­”ã€‚")
            return False

        try:
            # è½¬æ¢ä¸ºRGBæ ¼å¼ï¼ˆface_recognitionåº“è¦æ±‚ï¼‰
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # ä½¿ç”¨å½“å‰é…ç½®æ£€æµ‹äººè„¸
            face_locations = face_recognition.face_locations(
                rgb_image,
                model=self.current_config['detection_model'] if self.current_config[
                                                                    'detection_model'] != 'auto' else 'hog',
                number_of_times_to_upsample=self.current_config['number_of_times_to_upsample']
            )

            if not face_locations:
                logger.warning(f"æœªåœ¨æä¾›çš„å›¾åƒä¸­æ£€æµ‹åˆ°äººè„¸ï¼Œæ— æ³•æ³¨å†Œ '{person_name}'ã€‚")
                return False

            # è¿‡æ»¤æ‰è¿‡å°çš„äººè„¸
            filtered_locations = [
                loc for loc in face_locations
                if (loc[2] - loc[0]) >= self.current_config['min_face_size'] and
                   (loc[1] - loc[3]) >= self.current_config['min_face_size']
            ]

            if not filtered_locations:
                logger.warning(
                    f"æ£€æµ‹åˆ°çš„äººè„¸å°ºå¯¸è¿‡å°ï¼Œæ— æ³•æ³¨å†Œ '{person_name}' (æœ€å°å°ºå¯¸è¦æ±‚: {self.current_config['min_face_size']}px)ã€‚")
                return False

            face_encodings = face_recognition.face_encodings(rgb_image, filtered_locations)

            if not face_encodings:
                logger.warning(f"æ— æ³•ä»å›¾åƒä¸­æå–äººè„¸ç‰¹å¾ï¼Œæ— æ³•æ³¨å†Œ '{person_name}'ã€‚")
                return False

            # å°†æ–°æå–çš„ç¼–ç åŠ¨æ€æ·»åŠ åˆ°å†…å­˜ä¸­çš„å·²çŸ¥äººè„¸åˆ—è¡¨
            self.known_face_encodings.extend(face_encodings)
            self.known_face_names.extend([person_name] * len(face_encodings))

            # å°†æ–°æ³¨å†Œçš„äººè„¸å›¾ç‰‡ä¿å­˜åˆ° known_faces_dir å¯¹åº”çš„å­ç›®å½•ä»¥å®ç°æŒä¹…åŒ–
            person_dir = os.path.join(self.known_faces_dir, person_name)
            os.makedirs(person_dir, exist_ok=True)
            # ä¿å­˜ä¸€ä¸ªå‰¯æœ¬
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_image_path = os.path.join(person_dir, f"reg_{timestamp}.jpg")
            cv2.imwrite(output_image_path, image)

            logger.info(f"âœ… æˆåŠŸæ³¨å†Œäººè„¸ '{person_name}'ï¼Œæå– {len(face_encodings)} ä¸ªç‰¹å¾ï¼Œä¿å­˜åˆ° {output_image_path} å¹¶å·²æ›´æ–°å†…å­˜æ•°æ®åº“ã€‚")
            return True

        except Exception as e:
            logger.error(f"æ³¨å†Œäººè„¸ '{person_name}' æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
            return False

    def verify_liveness_action(self, image: np.ndarray, action: str) -> bool:
        """
        éªŒè¯å•å¼ å›¾ç‰‡æ˜¯å¦ç¬¦åˆæŒ‡å®šçš„æ´»ä½“åŠ¨ä½œã€‚

        Args:
            image (np.ndarray): åŒ…å«äººè„¸çš„å›¾åƒ (BGRæ ¼å¼)ã€‚
            action (str): éœ€è¦éªŒè¯çš„åŠ¨ä½œæŒ‡ä»¤ (ä¾‹å¦‚: 'è¯·çœ¨çœ¨çœ¼ç›', 'è¯·å‘å·¦è½¬å¤´').

        Returns:
            bool: å¦‚æœåŠ¨ä½œéªŒè¯æˆåŠŸè¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
        """
        try:
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # é¦–å…ˆï¼Œè·å–é¢éƒ¨å…³é”®ç‚¹
            face_landmarks_list = face_recognition.face_landmarks(rgb_image)
            if not face_landmarks_list:
                logger.warning(f"æ´»ä½“æ£€æµ‹å¤±è´¥ï¼šåœ¨å›¾åƒä¸­æœªæ‰¾åˆ°é¢éƒ¨å…³é”®ç‚¹ã€‚åŠ¨ä½œ: {action}")
                return False
            
            # é€šå¸¸æˆ‘ä»¬åªå¤„ç†ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„äººè„¸
            landmarks = face_landmarks_list[0]

            if 'çœ¨çœ¼' in action:
                left_eye_ear = _calculate_eye_aspect_ratio(landmarks['left_eye'])
                right_eye_ear = _calculate_eye_aspect_ratio(landmarks['right_eye'])
                avg_ear = (left_eye_ear + right_eye_ear) / 2.0
                logger.debug(f"çœ¨çœ¼æ£€æµ‹: å¹³å‡EAR={avg_ear:.3f} (é˜ˆå€¼: <{self.EAR_THRESHOLD})")
                return avg_ear < self.EAR_THRESHOLD

            elif 'å‘å·¦è½¬å¤´' in action:
                nose_tip = landmarks['nose_bridge'][-1]
                left_eye_center = np.mean(landmarks['left_eye'], axis=0)
                right_eye_center = np.mean(landmarks['right_eye'], axis=0)
                
                dist_to_left = np.linalg.norm(np.array(nose_tip) - left_eye_center)
                dist_to_right = np.linalg.norm(np.array(nose_tip) - right_eye_center)
                
                if dist_to_left == 0: return False # é¿å…é™¤é›¶
                ratio = dist_to_right / dist_to_left
                logger.debug(f"å‘å·¦è½¬å¤´æ£€æµ‹: è·ç¦»æ¯”={ratio:.2f} (é˜ˆå€¼: >{self.HEAD_TURN_RATIO_THRESHOLD})")
                return ratio > self.HEAD_TURN_RATIO_THRESHOLD

            elif 'å‘å³è½¬å¤´' in action:
                nose_tip = landmarks['nose_bridge'][-1]
                left_eye_center = np.mean(landmarks['left_eye'], axis=0)
                right_eye_center = np.mean(landmarks['right_eye'], axis=0)
                
                dist_to_left = np.linalg.norm(np.array(nose_tip) - left_eye_center)
                dist_to_right = np.linalg.norm(np.array(nose_tip) - right_eye_center)

                if dist_to_right == 0: return False # é¿å…é™¤é›¶
                ratio = dist_to_left / dist_to_right
                logger.debug(f"å‘å³è½¬å¤´æ£€æµ‹: è·ç¦»æ¯”={ratio:.2f} (é˜ˆå€¼: >{self.HEAD_TURN_RATIO_THRESHOLD})")
                return ratio > self.HEAD_TURN_RATIO_THRESHOLD
            
            elif 'ç‚¹å¤´' in action or 'æ­£è§†' in action:
                # å¯¹äºç‚¹å¤´å’Œæ­£è§†ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°ç¡®è®¤æ£€æµ‹åˆ°äº†äººè„¸å³å¯
                # æ›´å¤æ‚çš„å¯ä»¥æ£€æŸ¥é¼»å­å’Œä¸‹å·´çš„å‚ç›´è·ç¦»ï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æš‚æ—¶è®¤ä¸ºåªè¦æœ‰è„¸å°±ç®—é€šè¿‡
                return True

            else:
                logger.warning(f"æœªçŸ¥çš„æ´»ä½“æ£€æµ‹åŠ¨ä½œ: {action}")
                return False

        except Exception as e:
            logger.error(f"æ´»ä½“æ£€æµ‹åŠ¨ä½œ '{action}' éªŒè¯æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return False

    def detect_and_recognize(self, frame: np.ndarray) -> List[Dict]:
        """
        åœ¨å•å¸§å›¾åƒä¸­æ£€æµ‹å¹¶è¯†åˆ«äººè„¸ã€‚
        Args:
            frame (np.ndarray): BGRæ ¼å¼çš„è§†é¢‘å¸§ã€‚
        Returns:
            List[Dict]: ä¸€ä¸ªåŒ…å«æ£€æµ‹åˆ°çš„æ‰€æœ‰äººè„¸ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ã€‚
        """
        if not self.enabled:
            return []

        results = []

        # è·å–å½“å‰é…ç½®
        tolerance = self.current_config['tolerance']
        detection_model = self.current_config['detection_model']
        upsample_times = self.current_config['number_of_times_to_upsample']
        min_face_size = self.current_config['min_face_size']

        # è½¬æ¢ä¸ºRGBæ ¼å¼ï¼ˆface_recognitionåº“è¦æ±‚ï¼‰
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        face_locations = []

        # ğŸ¯ ä¼˜åŒ–äººè„¸æ£€æµ‹ï¼šæ ¹æ®é…ç½®é€‰æ‹©æ¨¡å‹å’Œå‚æ•°
        try:
            if detection_model == "cnn":
                face_locations = face_recognition.face_locations(rgb_frame, model="cnn",
                                                                 number_of_times_to_upsample=upsample_times)
            elif detection_model == "hog":
                face_locations = face_recognition.face_locations(rgb_frame, model="hog",
                                                                 number_of_times_to_upsample=upsample_times)
            else:  # auto æˆ–å…¶ä»–æœªæŒ‡å®šæ¨¡å‹æ—¶ï¼Œå°è¯•å¤šæ¨¡å‹ç­–ç•¥
                # ä¼˜å…ˆä½¿ç”¨hogï¼Œå› ä¸ºå®ƒé€šå¸¸æ›´å¿«
                face_locations = face_recognition.face_locations(rgb_frame, model="hog",
                                                                 number_of_times_to_upsample=upsample_times)
                if not face_locations and upsample_times < 3:  # å¦‚æœhogæ²¡æ£€æµ‹åˆ°ä¸”ä¸Šé‡‡æ ·æ¬¡æ•°ä¸é«˜ï¼Œå°è¯•æ›´é«˜çš„ä¸Šé‡‡æ ·
                    face_locations = face_recognition.face_locations(rgb_frame, model="hog",
                                                                     number_of_times_to_upsample=upsample_times + 1)
                if not face_locations:  # å¦‚æœhogä»æ²¡æ£€æµ‹åˆ°ï¼Œå°è¯•cnn
                    face_locations = face_recognition.face_locations(rgb_frame, model="cnn",
                                                                     number_of_times_to_upsample=upsample_times)

            if face_locations:
                logger.debug(f"ğŸ¯ æ£€æµ‹æ¨¡å‹ '{detection_model}' æ£€æµ‹åˆ° {len(face_locations)} ä¸ªäººè„¸")
        except Exception as e:
            logger.error(f"äººè„¸æ£€æµ‹å¤±è´¥: {str(e)}", exc_info=True)
            return []  # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨

        # è¿‡æ»¤æ‰è¿‡å°çš„äººè„¸
        filtered_face_locations = []
        for (top, right, bottom, left) in face_locations:
            face_height = bottom - top
            face_width = right - left
            if face_height >= min_face_size and face_width >= min_face_size:
                filtered_face_locations.append((top, right, bottom, left))
            else:
                logger.debug(f"  è¿‡æ»¤æ‰è¿‡å°äººè„¸: H={face_height}, W={face_width} (Min size: {min_face_size}px)")

        if not filtered_face_locations:
            logger.debug("âš ï¸ æœªæ£€æµ‹åˆ°æ»¡è¶³å°ºå¯¸è¦æ±‚çš„äººè„¸")
            return results

        # æå–äººè„¸ç‰¹å¾
        face_encodings = face_recognition.face_encodings(rgb_frame, filtered_face_locations)

        # å¯¹æ¯ä¸ªæ£€æµ‹åˆ°çš„äººè„¸è¿›è¡Œè¯†åˆ«
        for face_idx, (face_location, face_encoding) in enumerate(zip(filtered_face_locations, face_encodings)):
            top, right, bottom, left = face_location

            # ã€ä¿®å¤ã€‘æ·»åŠ ç¼ºå¤±çš„äººè„¸æ¯”å¯¹æ ¸å¿ƒé€»è¾‘
            matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding, tolerance=tolerance)
            name = "unknown"
            confidence = 0.0
            is_known = False

            face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)
            if len(face_distances) > 0:
                best_match_index = np.argmin(face_distances)
                if matches[best_match_index]:
                    name = self.known_face_names[best_match_index]
                    # å°†è·ç¦»è½¬æ¢ä¸ºç½®ä¿¡åº¦ (0.0 -> 100%, 0.6 -> 0%)
                    confidence = 1.0 - (face_distances[best_match_index] / tolerance)
                    confidence = max(0, min(1, confidence)) # ç¡®ä¿åœ¨0-1ä¹‹é—´
                    is_known = True

            results.append({
                "type": "face",
                "bbox": [left, top, right, bottom],
                "identity": {
                    "name": name,
                    "is_known": is_known,
                    "confidence": confidence,
                    "face_id": f"face_{face_idx}" # æ·»åŠ ä¸€ä¸ªå”¯ä¸€çš„æ ‡è¯†ç¬¦
                }
            })

        return results